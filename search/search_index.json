{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Welcome to TalTech HPC User Guides!","text":"<p>The use of the resources of the TalTech HPC Centre requires an active Uni-ID account (an application form for non-employees/non-students can be found here). Further, the user needs to be added to the HPC-USERS group. Please ask hpcsupport@taltech.ee to activate HPC access from your TalTech email and provide your UniID (six letters taken from the user's full name). In the case of using licensed programs, the user must also be added to the appropriate group. More information about available programs and licenses can be found here.</p> <p>TalTech HPC Centre includes cluster, cloud and is also responsible for providing access to the resources of the LUMI supercomputer.</p> <p>The cloud provides users the ability to create virtual machines where the user has full admin rights and can install all the necessary software themselves. VMs can be connected from outside and can be used for providing web services. Accessible through the ETAIS website.</p> <p>The cluster has a Linux operating system (based on CentOS; Debian or Ubuntu on special-purpose nodes) and uses SLURM as a batch scheduler and resource manager. Linux is the dominant operating system used for scientific computing and is the only operating system present in the Top500 list (a list of the 500 most powerful computers in the world). Linux command-line knowledge is essential for using the cluster. Resources on learning Linux can be found in our guide, including introductory lectures in Moodle. However, some graphical interface is available for data visualization, copy, and transfer.</p> <p>LUMI supercomputer is the fastest supercomputer in Europe, the fifth fastest globally and the seventh greenest supercomputer on the planet. Specifications of LUMI can be found here.</p>"},{"location":"index.html#hardware-specification","title":"Hardware Specification","text":"<p>TalTech ETAIS Cloud:</p> <ul> <li>5-node OpenStack cloud</li> <li>5 compute (nova) nodes with 768 GB of RAM and 80 threads each</li> <li>65 TB CephFS storage (net capacity)</li> <li>accessible through the ETAIS website</li> </ul> <p>TalTech cluster base (base.hpc.taltech.ee):</p> <ul> <li>SLURM v23 scheduler, a live load diagram</li> <li>1.5 PB storage, with a 0.5 TB/user quota on $HOME and 2 TB/user quota on SMBHOME</li> <li>32 green nodes, 2 x Intel Xeon Gold 6148 20C 2.40 GHz (40 cores, 80 threads per node), 96 GB DDR4-2666 R ECC RAM (green[1-32]), 25 Gbit Ethernet, 18 of these FDR InfiniBand (green-ib partition)</li> <li>1 mem1tb large memory node, 1 TB RAM, 4x Intel Xeon CPU E5-4640 (together 32 cores, 64 threads)</li> <li>2 ada GPU nodes, 2xNvidia L40/48GB, 2x 32core AMD EPYC 9354 Zen4 (together 64 cores, 128 threads), 1.5 TB RAM</li> <li>amp GPU nodes (specific guide for amp and amp1):</li> <li>amp: 8xNvidia A100/40GB, 2x 64core AMD EPYC 7742 Zen (together 128 cores, 256 threads), 1 TB RAM</li> <li>amp2: 8xNvidia A100/80GB, 2x 64core AMD EPYC 7713 zen3 (together 128 cores, 256 threads), 2 TB RAM</li> <li>Visualization node viz (accessible within University network and FortiVPN, guide for viz): 2xNvidia Tesla K20Xm graphic cards (on displays :0.0 and :0.1), CPU Intel(R) Xeon(R) CPU E5-2630L v2@2.40GHz (24 threads), 64 GB RAM, HDD 2 TB storage.</li> </ul>"},{"location":"index.html#billing","title":"Billing","text":""},{"location":"index.html#virtual-server-hosting","title":"Virtual server hosting","text":"What Unit TalTech internal External CPU CPU*hour 0.002 EUR 0.003 EUR Memory RAM*hour 0.001 EUR 0.0013 EUR Storage TB*year 20 EUR 80 EUR"},{"location":"index.html#taltech-cluster","title":"TalTech cluster","text":"What Unit TalTech internal External CPUcore &amp; &lt; 6 GB RAM CPUcore*hour 0.006 EUR 0.012 EUR CPUcore &amp; &gt; 6 GB RAM 6 GB RAM*hour 0.006 EUR 0.012 EUR GPU GPU*hour 0.20 EUR 0.50 EUR Storage 1 TB*Year 20 EUR 80 EUR <p>More details on how to calculate computational costs for TalTech cluster can be found in the Monitoring resources part of the Quickstart page.</p>"},{"location":"index.html#lumi-cluster-for-users-from-estonia","title":"LUMI cluster for users from Estonia","text":"What Unit Price for TalTech CPUcore CPUcore*hour 0.008 EUR GPU GPU*hour 0.35 EUR User home directory 20 GB free Project storage (persistent and scratch) TB*hour 0.0106 EUR Flash based scratch storage TB*hour 10 x 0.0106 EUR <p>A more detailed guide on how to calculate computational costs for LUMI can be found in the LUMI billing policy.</p>"},{"location":"index.html#slurm-partitions","title":"SLURM partitions","text":"partition default time time limit default memory nodes short 10 min 4 hours 1 GB/thread green, ada, amp common 10 min 8 days 1 GB/thread green green-ib 10 min 8 days 1 GB/thread green long 10 min 22 days 1 GB/thread green gpu 10 min 5 days 1 GB/thread amp, ada bigmem 10 min 8 days 1 GB/thread ada, amp, mem1tb"},{"location":"acknowledgement.html","title":"Acknowledgement","text":"<p>not changed to rocky yet</p>"},{"location":"acknowledgement.html#acknowledgement","title":"Acknowledgement","text":"<p>When publishing results obtained by using the systems of the HPC Centre, an acknowledgement would be appreciated. It helps emphasize the usefulness and importance of our services and to acquire funding for new systems. Please include a sentence along the lines of:</p> <p>\"The simulations were carried out in the High Performance Computing Centre of TalTech.\" </p>"},{"location":"billing.html","title":"Billing","text":""},{"location":"billing.html#virtual-server-hosting-openstack-cloud","title":"Virtual server hosting (OpenStack Cloud)","text":"What Unit TalTech internal External CPU CPU*hour 0.002 EUR 0.003 EUR Memory RAM*hour 0.001 EUR 0.0013 EUR Storage TB*year 20 EUR 80 EUR"},{"location":"billing.html#taltech-cluster","title":"TalTech cluster","text":"What Unit TalTech internal External CPUcore &amp; &lt; 6 GB RAM CPUcore*hour 0.006 EUR 0.012 EUR CPUcore &amp; &gt; 6 GB RAM 6 GB RAM*hour 0.006 EUR 0.012 EUR GPU GPU*hour 0.20 EUR 0.50 EUR Storage 1 TB*Year 20 EUR 80 EUR <p>More details on how to calculate computational costs for TalTech cluster can be found in the Monitoring resources part of the Quickstart page.</p>"},{"location":"billing.html#lumi-cluster-for-users-from-estonia","title":"LUMI cluster for users from Estonia","text":"What Unit Price for TalTech CPUcore CPUcore*hour 0.008 EUR GPU GPU*hour 0.35 EUR User home directory 20 GB free Project storage (persistent and scratch) TB*hour 0.0106 EUR Flash based scratch storage TB*hour 10 x 0.0106 EUR <p>A more detailed guide on how to calculate computational costs for LUMI can be found in the LUMI billing policy.</p>"},{"location":"cfd.html","title":"Cfd","text":"<p>not changed to rocky yet</p>"},{"location":"cfd.html#cfd","title":"CFD","text":"<p>Computational Fluid Dynamics</p> <p>The following software is available:</p> <ul> <li>OpenFOAM</li> <li>STAR-CCM+ (commercial)</li> <li>code_saturne (upcoming)</li> </ul> <p>The following software is under consideration:</p> <ul> <li>CONVERGE (commercial, free academic license, free training) https://convergecfd.com/</li> </ul>"},{"location":"cfd.html#openfoam","title":"OpenFOAM","text":""},{"location":"cfd.html#example-use-of-openfoam-on-the-base-cluster","title":"Example use of OpenFOAM on the BASE cluster","text":"<p>For the example we will use one of the tutorial cases.</p> Text Only<pre><code>module load mpi/openmpi-x86_64\nsource /share/apps/HPC2/OpenFOAM/OpenFOAM-v1912/etc/bashrc\n</code></pre> <p>First time users need to create their <code>$WM_PROJECT_USER_DIR</code>:</p> Text Only<pre><code>mkdir $WM_PROJECT_USER_DIR --parent\n</code></pre> <p>copy the damBreak tutorial case into the <code>$WM_PROJECT_USER_DIR</code>:</p> Text Only<pre><code>cp -r /share/apps/HPC2/OpenFOAM/OpenFOAM-v1912/tutorials/multiphase/interFoam/laminar/damBreak/damBreak $WM_PROJECT_USER_DIR/\ncd $WM_PROJECT_USER_DIR/damBreak\npwd\n</code></pre> <p>Now we can run the OpenFOAM case step-by-step or as a batch job.</p> <p>NB: Do not use the <code>Allrun</code> script(s) of the tutorials, as these may try to launch parallel jobs without requesting resources.</p>"},{"location":"cfd.html#interactive-single-process","title":"Interactive single process","text":"<p>For a non-parallel run of the tutorial case, the <code>decomposeParDict</code> needs to be removed:</p> Text Only<pre><code>mv system/decomposeParDict system/decomposeParDict-save\n</code></pre> <p>Running the damBreak case step-by-step interactively:</p> Text Only<pre><code>srun --partition=common -t 2:10:00 -\u2212pty bash \nsource /share/apps/HPC2/OpenFOAM/OpenFOAM-v1912/etc/bashrc\nblockMesh\nsetFields\ninterFoam\n</code></pre>"},{"location":"cfd.html#batch-job-non-interactive-parallel-job","title":"Batch-job (non-interactive) parallel job","text":"<p>Alternatively, we can run the job in parallel as a batch job: (If you removed/renamed the <code>decomposeParDict</code> before, copy  it back: <code>cp system/decomposeParDict-save system/decomposeParDict</code>)</p> <p>The <code>openfoam.slurm</code> script:</p> Text Only<pre><code>#!/bin/bash -l\n\n#SBATCH -n 4\n#SBATCH -t 00:10:00  \n#SBATCH -J openfoam-damBreak\n# #SBATCH --partition=green-ib\n\n#the following 2 lines are only needed if not done manually in command-line\nmodule load mpi/openmpi-x86_64\nsource /share/apps/HPC2/OpenFOAM/OpenFOAM-v1912/etc/bashrc\n\nblockMesh\ndecomposePar\nsetFields\nmpirun interFoam -parallel\nreconstructPar\n</code></pre> <p>and then run in the command-line (<code>module</code> and <code>source</code> are only needed if not in sbatch script):</p> Text Only<pre><code>module load mpi/openmpi-x86_64\nsource /share/apps/HPC2/OpenFOAM/OpenFOAM-v1912/etc/bashrc\nsbatch openfoam.slurm\n</code></pre>"},{"location":"cfd.html#pre-processing-geometry-and-mesh-generation","title":"Pre-processing (geometry and mesh generation)","text":"<p>The geometry and mesh can be either hand-coded using blockMesh or with Gmsh, FreeCAD or Salome. When using Gmsh, be sure to save the mesh in v2 ASCII format (see separate page on CAD-mesh. This creates a volume mesh</p> <p>To convert a Gmsh volume .msh file for OpenFOAM, use</p> Text Only<pre><code>gmshToFoam meshfile.msh\n</code></pre> <p>Another possibility is to use CAD for a surface mesh and use the snappyHexMesh utility to adapt a blockMesh volume mesh to the surface (see OpenFOAM motorcycle tutorial).</p>"},{"location":"cfd.html#visualizing-the-results-post-processing","title":"Visualizing the results (post-processing)","text":"<p>Login to viz, change to the case directory, create an empty .foam file for the case</p> Text Only<pre><code>touch damBreak.foam\n</code></pre> <p>and then use the regular ParaView </p> Text Only<pre><code>paraview\n</code></pre> <p>and open the .foam file from the menu</p>"},{"location":"cfd.html#comparison-of-the-execution-time","title":"Comparison of the execution time","text":"<p>It is educational to check the runtime of the code using the <code>time</code> command, e.g. for the single-thread</p> Text Only<pre><code>time interFoam\n</code></pre> <p>and for the parallel run (in the <code>openfoam.slurm</code> script)</p> Text Only<pre><code>time mpirun -n $SLURM_NTASKS interFoam -parallel\n</code></pre> <p>As the damBreak case is quite small, it is likely that the parallel run is not faster than the sequential, due to the communication overhead.</p> <p>In a testrun, the resuls have been as follows:</p> time type sequential parallel real 0m8.319s 0m39.463s user 0m6.927s 1m1.755s sys 0m0.432s 0m2.922s <p>Lesson to be learned: Parallel computation is only useful for sufficiently large jobs.</p> <p>NOTE: Parallel does not (necessarily) mean faster!!! Parallel execution introduces overhead (starting threads, communication)! For optimal execution time and optimal use of resources one needs to test and find the sweet spot.</p> <p></p> <p>The division into the areas is a combined decision taking into account \"real\" (wall clock) and \"user\" (summed time of all threads) time (from the <code>time</code> command). \"Wall clock\" (real) time is the time one needs to wait till the job is finished, \"Summed thread time\" (user) is the sum of the times that all individual threads needed, it should be roughly user = numtreads x real. For parallel programs, one can expect that \"user\" time of the parallel run is larger than for the sequential, due to communication overhead, if it is smaller, that probably means the individual threads could make better use of cache.</p> area why explanation sweet spot minimal \"user\" time = minimal heat production, optimal use of resources good range linear speedup for \"real\", with constant or slightly increasing \"user\" OK range slightly less than linear speedup for \"real\", and slightly increasing \"user\" avoid ascending slope in the diagram for \"real\" and \"user\" one actually needs to wait longer compared to the case with fewer cores <p>Recommended in this case would be to request 8 threads <code>-n 8 --ntasks-per-node 8</code> but use <code>mpirun -n 4</code>. OpenFOAM does not seem to benefit from hyperthreading.</p>"},{"location":"cfd.html#star-ccm","title":"STAR-CCM+","text":"<p>Commercial license belonging to ...</p>"},{"location":"chapel.html","title":"Chapel","text":"<p>not changed to rocky yet</p>"},{"location":"chapel.html#chapel-language-for-hpc","title":"Chapel language for HPC","text":"<p>Special programming language developed for supercomputing, origially by the Cray Cascade project in DARPA's High Productivity Computing Systems (HPCS) program.</p>"},{"location":"chapel.html#modules","title":"modules","text":"<p>on amp</p> Text Only<pre><code>module load amp\nmodule load llvm/14.0.0\nmodule load chapel/1.28\nmodule load amp-spack\nmodule load cuda/11.3.1-gcc-9.3.0-e4ej\n</code></pre> <p>on green</p> Text Only<pre><code>module load green-spack/0.17.1\nmodule load gcc/10.3.0-gcc-10.3.0-qshu\nmodule load llvm/13.0.0-gcc-10.3.0-dhdd\nmodule load opempi/4.1.1-gcc-10.3.0\n</code></pre>"},{"location":"chapel.html#evironment-variables","title":"evironment variables","text":"<p>on amp</p> Text Only<pre><code>source /gpfs/mariana/software/amp/chapel/chapel-1.28.0/util/quickstart/setchplenv.bash\n</code></pre> <p>on green</p> Text Only<pre><code>source /gpfs/mariana/software/green/chapel/chapel-1.28.0/util/quickstart/setchplenv.bash\n</code></pre> <p>Check which environments are available and compare to current settings</p> Text Only<pre><code>$CHPL_HOME/util/chplenv/printchplbuilds.py\n</code></pre> <p>check current variables</p> Text Only<pre><code> $CHPL_HOME/util/printchplenv\n</code></pre> <p>set some environment variables</p> Text Only<pre><code>CHPL_TASKS=fifo\nCHPL_GMP=none\nCHPL_TARGET_CPU=native\nCHPL_COMM=gasnet\nCHPL_COMM_SUBSTRATE=ibv\nCHPL_TARGET_COMPILER=gnu\nCHPL_RE2=none\nCHPL_LLVM=none\nCHPL_LAUNCHER_PARTITION=green-ib\nCHPL_SANITIZE=\nCHPL_HOME=/gpfs/mariana/software/green/chapel/chapel-1.28.0\nCHPL_LAUNCHER=slurm-srun\nCHPL_LAUNCHER_NODE_ACCESS=unset\nCHPL_MEM=jemalloc\n\n\n\nCHPL_COMM_SUBSTRATE=ibv        # several locales (nodes) with IB\nCHPL_COMM_SUBSTRATE=smp        # several locales within a single node\nCHPL_COMM_SUBSTRATE=mpi        # several locales (nodes) using MPI\nCHPL_COMM_SUBSTRATE=udp        # several locales (nodes) using\n\n\nCHPL_LOCALE_MODEL=flat  #normal\nCHPL_LOCALE_MODEL=numa  #locale is subdevided into numa domains\nCHPL_LOCALE_MODEL=gpu   #for GPU\n\nCHPL_LAUNCHER_PARTITION=green-ib\nCHPL_LAUNCHER_PARTITION=gpu\n</code></pre>"},{"location":"chapel.html#example-chapel-programs","title":"example chapel programs","text":"<p>can be copied from <code>$CHPL_HOME/examples</code></p> <p>set environment variables as needed</p> <p>compile</p> Text Only<pre><code>chpl -o jacobi-cpu-green jacobi-cpu.chpl\n</code></pre> <p>run the chapel program with apropriate number of locales (typically a locale is a full node!)</p> Text Only<pre><code>./jacobi-cpu-green -nl 1\n</code></pre> <p>the program can be started directly (without srun or sbatch), it uses the specified launcher set in the environment variable <code>CHPL_LAUNCHER=slurm-srun</code></p> <p>less threads on a node can be specified using the <code>CHPL_RT_NUM_THREADS_PER_LOCALE</code> environment variable</p>"},{"location":"cloud.html","title":"Quickstart: Cloud","text":""},{"location":"cloud.html#access-to-openstack-cloud-short-introduction","title":"Access to OpenStack Cloud (short introduction)","text":"In order to gain access to HPC Centre Cloud, you should take following steps:  1.  Login to ETAIS  and authenticate with your TalTech UniID credentials via MyAccessID. 2. Upload your SSH Public Key to ETAIS Portal. 3. Contact us by email (hpcsupport@taltech.ee), in Teams (HPC Support Chat) or through [Helpdesk](https://taltech.atlassian.net/servicedesk/customer/portal/18) to be added to a Project. 4. Get familiar with ETAIS documentation [here](https://etais.ee/using/)."},{"location":"cloud.html#access-to-openstack-cloud-long-version","title":"Access to OpenStack Cloud (long version)","text":"HPC Centre runs OpenStack-based Cloud. In order to gain access to it, you should take following steps:  1. Login to ETAIS  and authenticate with your TalTech UniID credentials via MyAccessID      For this the following steps should be done:      1.1 Go to ETAIS  and chose \"sign in with MyAccessID\".             ![etais](lumi/etais.png)                1.2 Chose **ttu.ee** as your affiliation              ![etais-login-2 alt &gt;&lt;](lumi/MyAccessID1.png)             1.3 Login using **Uni-ID (six letters taken from the user\u2019s full name),** but for longtime employees it could be name.surname.              ![etais-login-3 alt &gt;&lt;](lumi/etais-2.png)            1.4 Confirm your data and press button to continue.             ![etais](lumi/etais-3-1.png)                 1.5 Fill in the required fields, agree with term of use and press submit button.      ![etais](lumi/MyAccessID-1.png)  2. Upload your SSH Public Key to ETAIS Portal. For this go to SSH page.      ![etais-login-6](pictures/etais-login-6.png)             Add your public key (`id_rsa.pub`) to the correcponging field.       ![etais-login-7](pictures/etais-login-7.png)      How to get SSH keys can be read [here](ssh.md).  3. Contact us by email (hpcsupport@taltech.ee), in Teams (HPC Support Chat) or through [Helpdesk](https://taltech.atlassian.net/servicedesk/customer/portal/18) to be added to a Project. 4. Get familiar with ETAIS documentation [here](https://etais.ee/using/)."},{"location":"define.html","title":"Define","text":"<p>not changed to rocky yet</p>"},{"location":"define.html#define","title":"Define","text":"<ol> Define contains four main parts: <li>    Geomery menu:  read the geometry of the molecules, set up the coordinates of the system, find out the point group symmetry</li> <li>    Atomic attributes menu: select the basis sets for the atoms</li> <li>    Initial guess menu: determaine the charge of the molecule and generate the initial guess for the molecular orbitals and their occupation</li> <li>    General menu: select the computational method and set up advanced options such as excited state calculations.</li> </ol>  Some general instructions for define: <li>    * (or q)     -     Closes the current menu and writes the data into control</li> <li>    &amp;            -     Returns to the previous menu</li> <li>   qq            -     Quits Define immediately (panic button).</li> <p>Usually Define offers a default choice for all questions. The default choice can be accepted simply by pressing <code>Enter</code>.</p> <p>NB! define is case-sensitive. </p>"},{"location":"define.html#starting-define","title":"Starting define","text":"Text Only<pre><code>***********************************************************\n*                                                         *\n*                       D E F I N E                       *\n*                                                         *\n*         TURBOMOLE`S  INTERACTIVE  INPUT  PROGRAM        *\n*                                                         *\n*  Quantum Chemistry Group       University of Karlsruhe  *\n*                                                         *\n***********************************************************\n\n\nDATA WILL BE WRITTEN TO THE NEW FILE control\n\nIF YOU WANT TO READ DEFAULT-DATA FROM ANOTHER control-TYPE FILE,\nTHEN ENTER ITS LOCATION/NAME OR OTHERWISE HIT &gt;return&lt;.\n</code></pre> <p><code>Enter</code></p> Text Only<pre><code>INPUT TITLE OR \nENTER &amp; TO REPEAT DEFINITION OF DEFAULT INPUT FILE\n</code></pre> <p><code>Enter</code></p> Text Only<pre><code>SPECIFICATION OF MOLECULAR GEOMETRY ( #ATOMS=0     SYMMETRY=c1  )\nYOU MAY USE ONE OF THE FOLLOWING COMMANDS : \nsy &lt;group&gt; &lt;eps&gt; : DEFINE MOLECULAR SYMMETRY (default for eps=3d-1)\ndesy &lt;eps&gt;       : DETERMINE MOLECULAR SYMMETRY AND ADJUST \n                 COORDINATES (default for eps=1d-6)\nsusy             : ADJUST COORDINATES FOR SUBGROUPS\nai               : ADD ATOMIC COORDINATES INTERACTIVELY\na &lt;file&gt;         : ADD ATOMIC COORDINATES FROM FILE &lt;file&gt;\naa &lt;file&gt;        : ADD ATOMIC COORDINATES IN ANGSTROEM UNITS FROM FILE &lt;file&gt;\nsub              : SUBSTITUTE AN ATOM BY A GROUP OF ATOMS\ni                : INTERNAL COORDINATE MENU \nired             : REDUNDANT INTERNAL COORDINATES\nred_info         : DISPLAY REDUNDANT INTERNAL COORDINATES\nff               : UFF-FORCEFIELD CALCULATION\nm                : MANIPULATE GEOMETRY\nfrag             : Define Fragments for BSSE calculation\nw &lt;file&gt;         : WRITE MOLECULAR COORDINATES TO FILE &lt;file&gt; \nr &lt;file&gt;         : RELOAD ATOMIC AND INTERNAL COORDINATES FROM FILE &lt;file&gt;\nname             : CHANGE ATOMIC IDENTIFIERS \ndel              : DELETE ATOMS \ndis              : DISPLAY MOLECULAR GEOMETRY \nbanal            : CARRY OUT BOND ANALYSIS \n*                : TERMINATE MOLECULAR GEOMETRY SPECIFICATION \n                    AND WRITE GEOMETRY DATA TO CONTROL FILE\n\nIF YOU APPEND A QUESTION MARK TO ANY COMMAND AN EXPLANATION\nOF THAT COMMAND MAY BE GIVEN\n</code></pre> <p><code>a start-coord</code></p> Text Only<pre><code>CARTESIAN COORDINATES FOR  12 ATOMS HAVE SUCCESSFULLY \nBEEN ADDED. \n........\nSPECIFICATION OF MOLECULAR GEOMETRY ( #ATOMS=12    SYMMETRY=c1  )\n</code></pre> <p><code>ired</code></p> Text Only<pre><code>GEOIRED: NBDIM      30  NDEGR:      30 ......\n</code></pre> <p><code>*</code></p> Text Only<pre><code>ATOMIC ATTRIBUTE DEFINITION MENU  ( #atoms=12    #bas=12    #ecp=4     )\n\nb    : ASSIGN ATOMIC BASIS SETS \nbb   : b RESTRICTED TO BASIS SET LIBRARY \nbl   : LIST ATOMIC BASIS SETS ASSIGNED\nbm   : MODIFY DEFINITION OF ATOMIC BASIS SET\nbp   : SWITCH BETWEEN 5d/7f AND 6d/10f\nlib  : SELECT BASIS SET LIBRARY\necp  : ASSIGN EFFECTIVE CORE POTENTIALS \necpb : ecp RESTRICTED TO BASIS SET LIBRARY \necpi : GENERAL INFORMATION ABOUT EFFECTIVE CORE POTENTIALS\necpl : LIST EFFECTIVE CORE POTENTIALS ASSIGNED\necprm: REMOVE EFFECTIVE CORE POTENTIAL(S)\nc    : ASSIGN NUCLEAR CHARGES (IF DIFFERENT FROM DEFAULTS) \ncem  : ASSIGN NUCLEAR CHARGES FOR EMBEDDING \nm    : ASSIGN ATOMIC MASSES (IF DIFFERENT FROM DEFAULTS) \ndis  : DISPLAY MOLECULAR GEOMETRY \ndat  : DISPLAY ATOMIC ATTRIBUTES YET ESTABLISHED \nh    : EXPLANATION OF ATTRIBUTE DEFINITION SYNTAX \n*    : TERMINATE THIS SECTION AND WRITE DATA OR DATA REFERENCES TO control\nGOBACK=&amp; (TO GEOMETRY MENU !)\n</code></pre> <p><code>*</code></p> Text Only<pre><code>OCCUPATION NUMBER &amp; MOLECULAR ORBITAL DEFINITION MENU\n\nCHOOSE COMMAND \ninfsao     : OUTPUT SAO INFORMATION \natb        : Switch for writing MOs in ASCII or binary format\neht        : PROVIDE MOS &amp;&amp; OCCUPATION NUMBERS FROM EXTENDED HUECKEL GUESS \nuse &lt;file&gt; : SUPPLY MO INFORMATION USING DATA FROM &lt;file&gt; \nman        : MANUAL SPECIFICATION OF OCCUPATION NUMBERS \nhcore      : HAMILTON CORE GUESS FOR MOS\nflip       : FLIP SPIN OF A SELECTED ATOM\n&amp;          : MOVE BACK TO THE ATOMIC ATTRIBUTES MENU\nTHE COMMANDS  use  OR  eht  OR  *  OR q(uit) TERMINATE THIS MENU !!! \nFOR EXPLANATIONS APPEND A QUESTION MARK (?) TO ANY COMMAND\n</code></pre> <p><code>eht</code> <code>Enter</code> <code>0</code> <code>Enter</code> </p> Text Only<pre><code>GENERAL MENU : SELECT YOUR TOPIC \nscf    : SELECT NON-DEFAULT SCF PARAMETER \nmp2    : OPTIONS AND DATA GROUPS FOR rimp2 and mpgrad\ncc     : OPTIONS AND DATA GROUPS FOR ricc2\npnocc  : OPTIONS AND DATA GROUPS FOR pnoccsd\nex     : EXCITED STATE AND RESPONSE OPTIONS\nprop   : SELECT TOOLS FOR SCF-ORBITAL ANALYSIS \ndrv    : SELECT NON-DEFAULT INPUT PARAMETER FOR EVALUATION\n      OF ANALYTICAL ENERGY DERIVATIVES \n      (GRADIENTS, FORCE CONSTANTS) \nrex    : SELECT OPTIONS FOR GEOMETRY UPDATES USING RELAX\nstp    : SELECT NON-DEFAULT STRUCTURE OPTIMIZATION PARAMETER\ne      : DEFINE EXTERNAL ELECTROSTATIC FIELD \ndft    : DFT Parameters \nri     : RI Parameters \nrijk   : RI-JK-HF Parameters \nrirpa  : RIRPA Parameters \nsenex  : seminumeric exchange parameters \nhybno  : hybrid Noga/Diag parameters\ndsp    : DFT dispersion correction\ntrunc  : USE TRUNCATED AUXBASIS DURING ITERATIONS \nmarij  : MULTIPOLE ACCELERATED RI-J \ndis    : DISPLAY MOLECULAR GEOMETRY \nlist   : LIST OF CONTROL FILE \n&amp;      : GO BACK TO OCCUPATION/ORBITAL ASSIGNMENT MENU\n\n* or q : END OF DEFINE SESSION\n</code></pre> <p><code>*</code></p>"},{"location":"define.html#examples-of-define-files","title":"Examples of define files","text":""},{"location":"define.html#dft-calculation-pb86-d3bjdef2-svp","title":"DFT calculation (PB86-D3BJ/def2-SV(P))","text":"<p>In this example, the BP86 functional (dft/on) and def2-SV(P) basis set (b/all def2-SV(P)) will be used for the calculation. This is the default level of theory for DFT calculations in TURBOMOLE. BP86 functional has a good and stable performance throughout the periodic system, and by default def2 basis sets include ECPs for atoms beyond Kr. Additionally, will be used Grimme's dispersion correction D3BJ (dsp/on/bj). The geometry will be read from the file start-coord. Will be used the redundant internal coordinates (ired) since they typically result in the smallest number of optimization steps. To speed up calculations resolution-of-the-identity (RI) and multipole-accelerated RI-J (MARIJ) approximations will be used (ri/on and marij/on).  The molecule's charge is -1. An initial guess of the molecular orbitals will be done by eht* and up to 100 iterations will be done during scf cycle (scf/iter/100**).</p> <p>start-coord</p> Text Only<pre><code>$coord\n    -1.95916500780981     -0.42159243893826      0.00000000000000      ir\n    -1.95916500780981      4.47279824523555      0.00000000000000       i\n    -6.85355569198362     -0.42159243893826      0.00000000000000       i\n    -1.95916500780981     -0.42159243893826     -4.89439068417382       i\n    -1.95916500780981     -0.42159243893826      3.83614404975786       c\n    -2.45256841929780     -2.26300137375688      4.51014577207379       h\n    -0.11775648873094      0.07181261661147      4.51014577207379       h\n    -3.30717015319520      0.92641151591967      4.51014673583412       h\n     1.87697904194805     -0.42159243893826      0.00000000000000       c\n    -1.95916500780981     -4.25773648869612      0.00000000000000       c\n     4.25501040757134     -0.42159243893826      0.00000000000000       o\n    -1.95916500780981     -6.63576785431941      0.00000000000000       o\n$end\n</code></pre> <p>define</p> Text Only<pre><code>define &lt;&lt;EOF\n\n\na start-coord  \nired  \n*\nb\nall def2-SV(P)\n*\neht  \nyes  \n-1   \nyes  \nscf  \niter  \n100\n\nri  \non\n\nmarij  \non\n\ndft  \non\n\ndsp  \non  \nbj\n\n*\nEOF\n</code></pre> <p>comands to run</p> Text Only<pre><code>ridft &gt; JOB.out 2&gt;JOB.err    # single point calculation using RI-approximation\njobex -ri -c 45 &gt; FINAL.out 2&gt;FINAL.err # geometry optimization using RI-approximation,\n                                          will be don up to 45 steps\n</code></pre>"},{"location":"define.html#dft-calculation-mo6-hybrid-functional-and-different-basis-sets-including-ecp-with-frozen-position-of-several-atoms","title":"DFT calculation (MO6 (hybrid functional) and different basis sets including ECP) with frozen position of several atoms","text":"<p>In this example, the M06 functional (dft/on/func/m06) and two different basis sets (_b/1 6-31G/ecp/\"i\" DZP_*) will be used for calculations. For the first atom 6-31G will bu applied and for all I atoms - def-SV(P) with ECP. The geometry will be read from the file start-coord and Cartesian coordinates will be used for further calculations. In addition, the position of the two first atoms will be frozen, that can be done only in Cartesian coordinates. The molecule's charge is 0. An initial guess of the molecular orbitals will be done by eht** and up to 30 default iterations will be done during scf cycle.</p> <p>start-coord</p> Text Only<pre><code>$coord\n    0.74398670919525      0.42159243893826      0.00000000000000       n    f\n    2.02272352743997      2.22995142429552      3.13219908774303       i    f\n    2.02265750040888     -3.19517438119679      0.00000000000000       i\n    2.02272352743997      2.22995142429552     -3.13219909530193       i\n$end\n</code></pre> <p>define</p> Text Only<pre><code>define &lt;&lt;EOF\n\n\na start-coord  \n*\nno\nb\n1 6-31G*\necp  \n\"i\" DZP\n\n*\neht  \nyes  \n0   \nyes\n\ndft  \non  \nfunc  \nm06\n\n*\nEOF\n</code></pre> <p>NB!  In coord file should appear the corresponding \"f\" marks.</p> Text Only<pre><code>$coord\n    0.74398670919525      0.42159243893826      0.00000000000000       n    f\n    2.02272352743997      2.22995142429552      3.13219908774303       i    f\n</code></pre> <p>comands to run</p> Text Only<pre><code>dft &gt; JOB.out 2&gt;JOB.err      # single point calculation \njobex -c 45 &gt; FINAL.out 2&gt;FINAL.err # geometry optimization, will be done up to 45 steps\n</code></pre>"},{"location":"define.html#hf-optimization-with-frozen-internal-coordinated","title":"HF &amp; optimization with frozen internal coordinated","text":"<p>In this example, HF and minix basis set will be used for calculations. Some internal coordinates will be frozen (i/idef/f tors 1 2 3 4/f bend 1 2 3/f stre 1 2) during geometry optimization. To speed up calculations RI-approximations will be used.</p> <p>coord file</p> Text Only<pre><code>$coord\n     1.27839972889714      0.80710203135546      0.00041573974923       c\n     1.42630859331810      2.88253155131977      0.00372276048178       h\n     3.06528696563114     -0.57632867600746     -0.00069919866917       o\n    -1.91446264796512     -0.31879679861781      0.00039684248791       o\n    -2.98773260513752      1.98632893279876     -0.00701088395301       h\n$end\n</code></pre> <p>define</p> Text Only<pre><code>define &lt;&lt;EOF\n\n\na start-coord  \ni\nidef\nf tors 1 2 3 4\nf bend 1 2 3 \nf stre 1 2\n\n\n\nired  \n*\nbb all minix\n\n*\neht  \nyes  \n0   \nyes\n\nri  \non\n\n*\nEOF\n</code></pre> <p>NB!  In coord file should appear a corresponding part with list of defined internal coordinates:</p> Text Only<pre><code>$intdef\n# definitions of internal coordinates\n1 f  1.0000000000000 tors    1    2    3    4 val=  -0.04664\n2 f  1.0000000000000 bend    1    2    3      val=  26.89863\n3 f  1.0000000000000 stre    1    2           val=   2.08070\n</code></pre> <p>comands to run</p> Text Only<pre><code>dscf &gt; JOB.out 2&gt;JOB.err              # single point calculation  \njobex -ri &gt; FINAL.out 2&gt;FINAL.err     # geometry optimization \n(RI-approximation will be used if it is specified in control file)\n</code></pre>"},{"location":"define.html#ri-mp2-calculation","title":"RI-MP2 calculation","text":"<p>In this example, calculations will be performed at the MP2/def2-TZVP level of theory (b/all def2-TZVP and cc/ricc2/mp2/geoopt model=mp2), inner core electrons will be freezed and conergence criteria  increaced (_mp2/freeze//cbas/b/all def2-TZVP//denconv/0.1E-07_*). The symmetry of the molecule is determined and will be utilized (desy). To speed up calculations RI-approximations will be used (ricc2). The molecule's charge is 0. An initial guess of the molecular orbitals will be done by eht* and up to 70 iterations will be done during scf cycle.</p> <p>coord file</p> Text Only<pre><code>$coord\n         0.00000000000000     -0.00000000000000      0.00000000000000  c\n        -1.18649579051912      1.18649579051912      1.18649579051912  h\n         1.18649579051912     -1.18649579051912      1.18649579051912  h\n        -1.18649579051912     -1.18649579051912     -1.18649579051912  h\n         1.18649579051912      1.18649579051912     -1.18649579051912  h\n$end\n</code></pre> <p>define</p> Text Only<pre><code>define &lt;&lt;EOF\n\n\na start-coord \ndesy\nired  \n*\nb\nall def2-TZVP\n*\neht  \nyes  \n0   \nyes\n\nscf\niter\n70\n\nmp2 \nfreeze\n*\ncbas\nb\nall def2-TZVP\n*   \ndenconv\n0.1E-07\n*\ncc\nricc2\nmp2 \ngeoopt model=mp2\n*\n*\n*\nEOF\n</code></pre> <p>comands to run</p> Text Only<pre><code>jobex -ri  -level mp2 &gt; FINAL.out 2&gt;FINAL.err\n</code></pre>"},{"location":"define.html#cc2-calculation","title":"CC2 calculation","text":"<p>In this example, calculations will be performed at the cc22/def2-TZVP level of theory (b/all def2-TZVP and cc/ricc2/cc2/geoopt model=cc2), inner core electrons will be freezed and conergence criteria  increaced (_mp2/freeze//cbas/b/all def2-TZVP//denconv/0.1E-07_*). The symmetry of the molecule is determined and will be utilized (desy). To speed up calculations RI-approximations will be used (ricc2). The molecule's charge is 0. An initial guess of the molecular orbitals will be done by eht* and up to 70 iterations will be done during scf cycle.</p> <p>coord file</p> Text Only<pre><code>$coord\n         0.00000000000000     -0.00000000000000      0.00000000000000  c\n        -1.18649579051912      1.18649579051912      1.18649579051912  h\n         1.18649579051912     -1.18649579051912      1.18649579051912  h\n        -1.18649579051912     -1.18649579051912     -1.18649579051912  h\n         1.18649579051912      1.18649579051912     -1.18649579051912  h\n$end\n</code></pre> <p>define</p> Text Only<pre><code>define &lt;&lt;EOF\n\n\na start-coord \ndesy\nired  \n*\nb\nall def2-TZVP\n*\neht  \nyes  \n0   \nyes\n\nscf\niter\n70\n\ncc\nfreeze\n*\ncbas\nb\nall def2-TZVP\n*   \ndenconv\n0.1E-07\nricc2\ncc2 \ngeoopt model=cc2 \n*\n*\n*\nEOF\n</code></pre> <p>comands to run</p> Text Only<pre><code>jobex -ri  -level cc2 &gt; FINAL.out 2&gt;FINAL.err\n</code></pre>"},{"location":"easybuild.html","title":"Easybuild","text":"<p>not changed to rocky yet</p>"},{"location":"easybuild.html#easybuild","title":"EasyBuild","text":"<p>under development, docs AND module positions/versions may change without notice</p> <p>EasyBuild is a package manager to install software packages. An advantage is to be able to relatively easily install consistent dependencies and multiple versions of a software. List of packages</p>"},{"location":"easybuild.html#modules","title":"Modules","text":"<p>EasyBuild is available on amp from AI lab</p> Text Only<pre><code>module use /illukas/software/modules\n</code></pre>"},{"location":"easybuild.html#user-build-software","title":"User build software","text":"<p>EasyBuild can also be used by users to manage their own software stack inside their home directory (be aware, this takes a lot of space!).</p> <p>See documentation on https://docs.easybuild.io/en/latest/</p> <p>A similar tool is SPACK. They support different lists of software packages. SPACK includes GPU-offloading compiler for both Nvidia and AMD, profiling tools (Tau, HPCToolkit) and engineering simulation packages (ElmerFEM, OpenFOAM), while EasyBuild seems to be more AI and Python oriented</p> <p>EasyBuild will be used on LUMI, while SPACK is used by University of Tartu, LRZ and HLRS.</p>"},{"location":"fea.html","title":"Fea","text":"<p>not changed to rocky yet</p>"},{"location":"fea.html#fea","title":"FEA","text":"<p>Finite Element Analysis</p> <p>The following software is installed on the cluster:</p> <ul> <li>ElmerFEM: Multiphysics ElmerModelsManual.pdf</li> <li>CalculiX: Mechanical analysis, heat, electromagnetic, CFD; solver makes use of the Abaqus input format. Overview of the finite element capabilities of CalculiX Version 2.18</li> <li>deal.ii (via SPACK): A C++ software library supporting the creation of finite element codes and an open community of users and developers.</li> <li>Abaqus: Commercial</li> <li>Comsol: Commercial, License belongs to a research group (...)</li> <li>code_aster (upcoming from generic package)</li> <li>ngsolve (upcoming from source)</li> </ul> <p>The following software is under consideration:</p> <ul> <li>FEniCS (under consideration) SPACK</li> <li>Dune (under consideration) source</li> <li>FEATool Multiphysics (under consideration) source</li> <li>FreeFEM (under consideration) SPACK</li> <li>MFEM (under consideration) SPACK</li> <li>MoFEM (under consideration) SPACK problematic</li> </ul>"},{"location":"fea.html#elmerfem","title":"ElmerFEM","text":"<p>Elmer is a multi-physics simulation software developed by CSC. It can perform coupled mechanical, thermal, fluid, electro-magnetic simulations and can be extended by own equations.</p> <p>Elmer homepage: http://www.elmerfem.org/blog/ Elmer manuals and tutorials: https://www.nic.funet.fi/pub/sci/physics/elmer/doc/</p>"},{"location":"fea.html#loading-the-module","title":"Loading the module","text":"<p>To use ElmerFEM the module needs to be loaded</p> Text Only<pre><code>module load elmerfem-9.0\n</code></pre> <p>This makes the following main commands <code>ElmerGrid</code>, <code>ElmerSolver</code> available (and <code>ElmerGUI</code> can be used on viz to setup the case file). The use of ElmerGUI for simulations is not recommended.</p>"},{"location":"fea.html#running-a-tutorial-case-quick-start-for-the-impatient","title":"Running a tutorial case (quick-start for the impatient)","text":"<p>Copy the tutorial directory (here the linear elastic beam):</p> Text Only<pre><code>cp -r /share/apps/HPC2/ElmerFEM/tutorials-CL-files/ElasticEigenValues/ linear-elastic-beam-tutorial\ncd linear-elastic-beam-tutorial\n</code></pre> <p>Start an interactive session on a node</p> Text Only<pre><code>srun -t 2:00:00 --pty bash\n</code></pre> <p>Create the mesh</p> Text Only<pre><code>ElmerGrid d 7 2 mesh.FDNEUT\n</code></pre> <p>run the solver</p> Text Only<pre><code>ElmerSolver\n</code></pre> <p>Postprocessing would be visualizing the <code>eigen_values.vtu</code> file in <code>paraview</code> on viz.</p>"},{"location":"fea.html#setting-up-a-simulation-for-new-users","title":"Setting up a simulation (for new users)","text":"<p>The follwing steps are needed to configure a simulation case (mostly on base):</p> <ol> <li>Create geometry in Gmsh, group and name physical volumes and surfaces (can be done on viz)</li> <li>Create mesh in Gmsh (large meshes can be created from the CLI in a batch job: <code>gmsh -3 geometry.geo</code>)</li> <li>Convert the mesh to elmer's format using ElmerGrid, including scaling if needed: `ElmerGrid 14 2 geometry.msh -scale 0.001 0.001 0.001</li> <li>Create a new project in ElmerGUI (can be done on viz)<ul> <li>create project</li> <li>load Elmer mesh (point to the created mesh directory)</li> <li>add equation(s)</li> <li>add material(s)</li> <li>add boundary conditions</li> <li>create sif</li> <li>edit &amp; save sif</li> </ul> </li> <li>Edit the <code>case.sif</code> file (mesh directory, some other parameters [e.g. calculate PrincipalStresses] can only be added in the sif file, not in the GUI)</li> <li>run simulation <code>srun ElmerSolver</code> (or create batch file and submit using sbatch)</li> <li>Postprocessing in ParaView (on viz)</li> </ol>"},{"location":"fea.html#an-example-simulation-case-from-start-to-finish","title":"An example simulation case from start to finish","text":""},{"location":"fea.html#calculix","title":"CalculiX","text":"<p>The two programs that form CalculiX are <code>cgx</code> and <code>ccx</code>, where <code>cgx</code> is a graphical frontend (pre- and post-processing) and <code>ccx</code> is the solver doing the actual numerics.</p> <p>As mentioned above, CalculiX uses the Abaqus format.</p>"},{"location":"fea.html#abaqus","title":"Abaqus","text":""},{"location":"fea.html#comsol","title":"Comsol","text":"<p>The license belongs to the research group of ... . In case you need access, please agree with them on a cooperation.</p>"},{"location":"fea.html#dealii","title":"deal.ii","text":"<p>Available through a SPACK module.</p> Text Only<pre><code>module use /share/apps/HPC2/SPACK/spack/share/spack/modules/linux-centos7-skylake_avx512/\n</code></pre> <p>deal.ii is a C++ library that can be used to write own FEA software.</p>"},{"location":"gpu.html","title":"Cluster GPU usage","text":"<p>only partially changed to rocky yet</p>"},{"location":"gpu.html#gpu-servers","title":"GPU-servers","text":"<p>job submission from \"base\"</p> <p>The AI-lab \"Illukas\" modules will NOT work on the cluster due to different OS</p> <p> </p>"},{"location":"gpu.html#hardware","title":"Hardware","text":"amp1   - CPU: 2x AMD EPYC 7742 64core (2nd gen EPYC, Zen2) - RAM: 1 TB  - GPUs: 8x A100 Nvidia 40GB - OS: Rocky8   amp2   - CPU: 2x AMD EPYC 7713 64core (3rd gen EPYC, Zen3) - RAM: 2 TB - GPUs: 8x A100 Nvidia 80GB - OS: Rocky8   ada[1,2]   - CPU: 2x AMD EPYC 9354 32core (4th gen EPYC, Zen4) - RAM: 1.5 TB - GPUs: 2x L40 Nvidia 48GB - avx512 - OS: Rocky8"},{"location":"gpu.html#login-and-localstorage","title":"Login and localstorage","text":"<p>No direct login, jobs are submitted from \"base\", use <code>srun -p gpu --gres=gpu:L40 --pty bash</code></p> <p>amp[1,2] have <code>/localstorage</code> a 10 TB NVMe partition for fast data access. Data in directory has a longer storage duration than data in the 4 TB <code>/tmp</code> (<code>/state/partition1</code> is the same as <code>/tmp</code>)</p> <p></p>"},{"location":"gpu.html#running-jobs","title":"Running jobs","text":"<p>Jobs need to be submitted using <code>srun</code> or <code>sbatch</code>, do not run jobs outside the batch system.</p> <p>Interactive jobs are started using <code>srun</code>:</p> Text Only<pre><code>srun -p gpu -t 1:00:00 --pty bash\n</code></pre> <p>GPUs have to be reserved/requested with:</p> Text Only<pre><code>srun -p gpu --gres=gpu:A100:1 -t 1:00:00 --pty bash\n</code></pre> <p>all nodes with GPUs are in the same partition (<code>-p gpu</code>, but also in <code>short</code>, which has higher priority, but shorter time-limit) so jobs that do not have specific requirements can run on any of the nodes. If you need a specific type, e.g. for testing performance or because of memory requirements:   - it is possible to request the feature \"A100-40\" (for the 40GB A100s), \"A100-80\" (for the 80GB A100s):** <code>--gres=gpu:A100:1 --constraint=A100-80</code> or <code>--gres=gpu:1 --constraint=A100-40</code></p> <ul> <li> <p>it is also possible to request the\"compute capability, e.g. nvcc80 (for A100) or nvcc89 (for L40) using <code>--gres=gpu:1 --constraint=nvcc89</code> = <code>--gres=gpu:L40:1</code></p> </li> <li> <p>another option is to request the job to run on a specific node, using the <code>-w</code> switch (e.g. <code>srun -p gpu -w amp1 --gres=gpu:A100:1 ...</code>)</p> </li> </ul> <p>You can see which GPUs have been assigned to your job using <code>echo $CUDA_VISIBLE_DEVICES</code>, the CUDA-deviceID in your programs always start with \"0\" (no matter which physical GPU was assigned to you by SLURM).</p> <p></p>"},{"location":"gpu.html#software-and-modules","title":"Software and modules","text":"<p>same modules as on all nodes, i.e. the rocky8 and rocky8-spack modules.</p>"},{"location":"gpu.html#from-ai-lab","title":"From AI lab","text":"<p>will not work due to different OS</p>"},{"location":"gpu.html#software-that-supports-gpus","title":"Software that supports GPUs","text":"<ul> <li>JupyterLab, see page on JupyterLab</li> <li>Gaussian, see page on Gaussian</li> <li>cp2k</li> <li>StarCCM+</li> <li>Julia</li> <li>Chapel</li> <li>Singularity (apptainer), see page on Singularity</li> </ul> <p>only partially changed to rocky yet</p>"},{"location":"gpu.html#gpu-libraries-and-tools","title":"GPU libraries and tools","text":"<p>The GPUs installed are Nvidia A100 with compute capability 80, compatible with CUDA 11. However, when developing own software, be aware of vendor lockin, CUDA is only available for Nvidia GPUs and does not work on AMD GPUs. Some new supercomputers (LUMI (CSC), El Capitan (LLNL), Frontier (ORNL)) are using AMD, and some plan the Intel \"Ponte Vecchio\" GPU (Aurora (ANL), SuperMUC-NG (LRZ)). To be future-proof, portable methods like OpenACC/OpenMP are recommended.</p> <p>Porting to AMD/HIP for LUMI: https://www.lumi-supercomputer.eu/preparing-codes-for-lumi-converting-cuda-applications-to-hip/</p>"},{"location":"gpu.html#nvidia-cuda-11","title":"Nvidia CUDA 11","text":"<p>Again, beware of the vendor lockin.</p> <p>To compile CUDA code, use the Nvidia compiler wrapper:</p> Text Only<pre><code>nvcc\n</code></pre>"},{"location":"gpu.html#offloading-compilers","title":"Offloading Compilers","text":"<ul> <li>PGI (Nvidia HPC-SDK) supports OpenACC and OpenMP offloading to Nvidia GPUs</li> <li>GCC-10.3.0</li> <li>GCC-11.2.0 with NVPTX supports GPU-offloading using OpenMP and OpenACC pragmas</li> <li>LLVM-13.0.0 (Clang/Flang) with NVPTX supports GPU-offloading using OpenMP pragmas </li> </ul> <p>See also: https://lumi-supercomputer.eu/offloading-code-with-compiler-directives/</p>"},{"location":"gpu.html#openmp-offloading","title":"OpenMP offloading","text":"<p>Since version 4.0 supports offloading to accelerators. It can be utilized by GCC, LLVM (C/Flang) and Nvidia HPC-SDK (former PGI compilers).</p> <ul> <li>GCC-10.3.0</li> <li>GCC-11.2.0 with NVPTX supports GPU-offloading using OpenMP and OpenACC pragmas</li> <li>LLVM-13.0.0 (Clang/Flang) with NVPTX supports GPU-offloading using OpenMP pragmas</li> <li>AOMP  </li> </ul> <p>List of compiler support for OpenMP: https://www.openmp.org/resources/openmp-compilers-tools/</p> <p>Current recommendation: use Clang or GCC or AOMP</p>"},{"location":"gpu.html#nvidia-hpc-sdk","title":"Nvidia HPC SDK","text":"<p>Compile option <code>-\u2060mp</code> for CPU-OpenMP or <code>-mp=gpu</code> for GPU-OpenMP-offloading.</p> <p>The table below summarizes useful compiler flags to compile you OpenMP code with offloading.</p> NVC/NVFortran Clang/Cray/AMD GCC/GFortran OpenMP flag -mp -fopenmp -fopenmp -foffload= Offload flag -mp=gpu -fopenmp-targets= -foffload= Target NVIDIA default nvptx64-nvidia-cuda nvptx-none Target AMD n/a amdgcn-amd-amdhsa amdgcn-amdhsa GPU Architecture -gpu= -Xopenmp-target -march= -foffload=\u201d-march="},{"location":"gpu.html#openacc-offloading","title":"OpenACC offloading","text":"<p>OpenACC is a portable compiler directive based approach to GPU computing. It can be utilized by GCC, (LLVM (C/Flang)) and Nvidia HPC-SDK (former PGI compilers).</p> <p>Current recommendation: use HPC-SDK</p>"},{"location":"gpu.html#nvidia-hpc-sdk_1","title":"Nvidia HPC SDK","text":"<p>Installed are versions 21.2, 21.5 and 21.9 (2021). These come with modulefiles, to use them, enable the the directory:</p> Text Only<pre><code>module load rocky8-spack\n</code></pre> <p>then load the module you want to use, e.g.</p> Text Only<pre><code>module load nvhpc\n</code></pre> <p>The HPC SDK also comes with a profiler, to identify regions that would benefit most from GPU acceleration.</p> <p>OpenACC is based on compiler pragmas enabling an incremental approach to parallelism (you never break the sequential program), it can be used for CPUs (multicore) and GPUs (tesla). </p> <p>Compiling an OpenACC program with the Nvidia compiler: get accelerator information</p> Text Only<pre><code>pgaccelinfo\n</code></pre> <p>compile for multicore (C and Fortran commands)</p> Text Only<pre><code>pgcc -fast -ta=multicore -Minfo=accel -I/opt/nvidia/hpc_sdk/Linux_x86_64/21.5/cuda/11.3/targets/x86_64-linux/include/  -o laplace jacobi.c laplace2d.c\npgfortran -fast -ta=multicore  -Minfo=accel -I/opt/nvidia/hpc_sdk/Linux_x86_64/21.5/cuda/11.3/targets/x86_64-linux/include/ -o laplace_multicore laplace2d.f90 jacobi.f90\n</code></pre> <p>compile for GPU (C and Fortran commands)</p> Text Only<pre><code>pgcc -fast -ta=tesla -Minfo=accel  -I/opt/nvidia/hpc_sdk/Linux_x86_64/21.5/cuda/11.3/targets/x86_64-linux/include/ -o laplace_gpu jacobi.c laplace2d.c\npgfortran -fast -ta=tesla,managed -Minfo=accel -I/opt/nvidia/hpc_sdk/Linux_x86_64/21.5/cuda/11.3/targets/x86_64-linux/include/ -o laplace_gpu laplace2d.f90 jacobi.f90\n</code></pre> <p>Profiling:</p> Text Only<pre><code>nsys profile -t nvtx --stats=true --force-overwrite true -o laplace ./laplace\nnsys profile -t openacc --stats=true --force-overwrite true -o laplace_data_clauses ./laplace_data_clauses 1024 1024\n</code></pre> <p>Analysing the profile using CLI:</p> Text Only<pre><code>nsys stat s laplace.qdrep\n</code></pre> <p>using the GUI:</p> Text Only<pre><code>nsys-ui\n</code></pre> <p>then load the <code>.qdrep</code> file.</p>"},{"location":"gpu.html#gcc-needs-testing","title":"GCC (needs testing)","text":"<ul> <li>GCC-10.3.0</li> <li>GCC-11.2.0 with NVPTX supports GPU-offloading using OpenMP and OpenACC pragmas</li> </ul>"},{"location":"gpu.html#hip-upcoming","title":"HIP (upcoming)","text":"<p>For porting code to AMD-Instinct based LUMI, the AMD HIP SDK will be installed.</p> <p></p> <p> </p>"},{"location":"hardware.html","title":"Hardware","text":"<p>not changed to rocky yet</p>"},{"location":"hardware.html#hardware-specification","title":"Hardware Specification","text":"<p>The use of the resources of the TalTech HPC Centre requires an active Uni-ID account, a procedure for non-employees/non-students can be found here (in Estonian).</p> <p>TalTech operates the following infrastructure:</p> <ul> <li>base.hpc.taltech.ee is the new cluster environment all nodes from HPC1 and HPC2 will be migrated here<ul> <li>a live diagram of the cluster load is here: load diagram</li> <li>UniID login (please ask at hpcsupport@taltech.ee to activate access)</li> <li>SLURM v20 scheduler</li> <li>CentOS 7</li> <li>home-directories' location: <code>/gpfs/mariana/home/uni-ID</code></li> <li>HPC1 home-directories accessible under <code>/beegfs/home/uni-ID</code></li> <li>home directory file system has 1.5 PB storage, with a 2 TB/user quota</li> <li>green nodes (former hpc2.ttu.ee nodes)<ul> <li>32 nodes 2 x Intel Xeon Gold 6148 20C 2.40 GHz, 96 GB DDR4-2666 R ECC RAM (green[1-32])</li> <li>25 Gbit Ethernet</li> <li>18 of these nodes are connected with low latency interconnect FDR InfiniBand (green-ib partition)</li> </ul> </li> <li>gray nodes (former hpc.ttu.ee nodes, migration in progress)<ul> <li>48 nodes with 2 x Intel Xeon E5-2630L 6C with 64 GB RAM and 1 TB local drive connected with low latency interconnect QDR InfiniBand (gray-ib partition)</li> <li>60 nodes with 2 x Intel Xeon E5-2630L 6C with 48 GB RAM and 1 TB local drive (memory upgrade to 64 GB in progress)</li> <li>1 Gbit Ethernet</li> </ul> </li> <li>mem1tb large memory node<ul> <li>1 node with 1TB of operating RAM</li> <li>4x Intel Xeon CPU E5-4640 (together 32 cores, 64 threads)</li> </ul> </li> <li>amp GPU node, specific guide for amp (amp.hpc.taltech.ee)<ul> <li>1 node with 8xNvidia A100</li> <li>2x 64core AMD EPYC 7742 (together 128 cores, 256 threads)</li> <li>1 TB RAM</li> <li>OpenACC, OpenMP, OpenCL, CUDA</li> <li>Ubuntu 20.04</li> </ul> </li> <li>viz.hpc.taltech.ee Visualization node (accessible within University network and FortiVPN)<ul> <li>this service is intended mainly for post-processing, especially of large datasets; it is not intended as a remote desktop service; job submission is not possible from here</li> <li>1 node with 2 nVidia Tesla K20Xm grapic cards (on displays :0.0 and :0.1)</li> <li>this node is not for computation on GPUs</li> <li>Debian 10</li> <li>UniID login with ssh-keys (no password login; previous login on base.hpc.taltech.ee required to create account)</li> <li>same home-directories as base, HPC1-homes accessible under /beegfs/home/uni-ID</li> <li>ParaView, VisIt and COVISE available for remote visualization</li> <li>VirtualGL, Xpra, VNC available for remote execution of programs (can be used with e.g. Mayavi, ParaView, VisIt, COVISE, OpenDX)</li> </ul> </li> <li>There is no backup please take care of backups yourself!</li> <li>partitions:<ul> <li>short: (default) time limit 4 hours, default time 10 min, default mem 1 GB/thread, green nodes</li> <li>common: time limit 8 days, default time 10 min, default mem 1 GB/thread, green nodes</li> <li>long: time limit 22 days, default time 10 min, default mem 1 GB/thread, green nodes</li> <li>green-ib: time limit 8 days, default time 10 min, default mem 1 GB/thread, green InfiniBand nodes</li> <li>gray-ib: time limit 8 days, default time 10 min, default mem 1 GB/thread, gray InfiniBand nodes</li> <li>gpu: amp GPU node, time limit 5 days, default time 10 min, default mem 1 GB/thread</li> <li>mem1tb: mem1tb node</li> </ul> </li> </ul> </li> <li>training.hpc.taltech.ee virtual training cluster <ul> <li>used for testing</li> <li>running on the TalTech ETAIS OpenStack cloud service</li> </ul> </li> <li>TalTech ETAIS Cloud: 4 node OpenStack cloud<ul> <li>5 compute (nova) nodes with 282GB or 768GB of RAM and 80 threads each</li> <li>65 TB CephFS storage (net capacity)</li> <li>accessible through the ETAIS website: https://etais.ee/using/</li> </ul> </li> </ul>"},{"location":"learning.html","title":"Learning","text":"<p>not changed to rocky yet</p>"},{"location":"learning.html#courses-and-introductions","title":"Courses and introductions","text":"<p>The HPC-Centre provides an online introduction course in moodle.</p>  The course consists of four modules:   - Module 1: Linux Command Line  - Module 2: Introduction  - Module 3: Introduction to OpenStack/ETAIS use (in preparation)  - Module 4: Remote visualization  <p> </p>"},{"location":"learning.html#resources-on-learning-linux","title":"Resources on learning Linux","text":"Linux command-line (shell, terminal):   - [Linux command-line tutorial (Learning the shell)](http://linuxcommand.org/lc3_learning_the_shell.php)  - [Linux command-line tutorial](https://tutorials.ubuntu.com/tutorial/command-line-for-beginners)  - [Linux command-line tutorial](https://ryanstutorials.net/linuxtutorial/)  - [Debian handbook](https://debian-handbook.info/)  - [Learn Linux the Hard Way](https://archive.is/xDb8o)  - [The Linux Command Line](https://sourceforge.net/projects/linuxcommand/)   TalTech courses:   - ICA0007 Linux Administration"},{"location":"learning.html#resources-for-learning-scientific-computing","title":"Resources for learning scientific computing","text":"TalTech courses:   - YFX1510 Scientific Computing (requires knowledge of Linux)  - YMX0110 Numerical Methods and packages of mathematics  - ECK0400 Computational Marine Hydrodynamics  - YFX9570 Computational Fluid Dynamics"},{"location":"learning.html#resources-for-learning-data-analysis","title":"Resources for learning data analysis","text":"TalTech courses:  - YFX0500 Introduction to Programming in Python  - YMX8170 Deep Learning for Science  - YFX1550 Scientific Python: Computing and Data Analysis  - NSO8062 Geophysical data analysis"},{"location":"lumi.html","title":"LUMI","text":""},{"location":"lumi.html#what-is-lumi","title":"What is LUMI?","text":"LUMI is the fastest supercomputer in Europe. It's an HPE Cray EX supercomputer consisting of several hardware partitions targeted different use cases:   - 2560 GPU-based nodes ([**LUMI-G**](https://docs.lumi-supercomputer.eu/hardware/lumig/)), each node with one 64 core AMD Trento CPU and four AMD MI250X GPUs. - 1536 dual-socket CPU nodes [**LUMI-C**](https://docs.lumi-supercomputer.eu/hardware/lumic/) with 64-core 3rd-generation AMD EPYC\u2122 CPUs, and between 256 GB and 1024 GB of memory.  - large memory GPU nodes [**LUMI-D**](https://docs.lumi-supercomputer.eu/hardware/lumid/), with a total of 32 TB of memory in the partition for data analytics and visualisation.  - Main storage - [**LUMI-P**](https://docs.lumi-supercomputer.eu/storage/parallel-filesystems/lumip/) has 4 independent Lustre file systems with 20 PB and an aggregate bandwidth of 240 GB/s each. Each Lustre file system is composed of 1 MDS (metadata server) and 32 Object Storage Targets (OSTs). - Flash storage - [**LUMI-F**](https://docs.lumi-supercomputer.eu/storage/parallel-filesystems/lumif/) has Lustre file system with a storage capacity of 8 PB and an aggregate bandwidth of 1 740 GB/s. - Object store -  [**LUMI-O**](https://docs.lumi-supercomputer.eu/storage/lumio/) provides 30 PB storage.  <p>More  about LUMI system architecture can be found in overview and LUMI\u2019s full system architecture. </p> <p>LUMI uses Slurm as job scheduler and resource manager. Slurm partitions can be allocated by node or by resources. More about partitions can be found here.</p> <p> </p>"},{"location":"lumi.html#why-lumi","title":"Why LUMI?","text":"There are several reasons to choose LUMI instead of HPC:  - if job is run using GPUs - if job needs large memory - if queue on HPC is too long"},{"location":"lumi.html#getting-started","title":"Getting started","text":""},{"location":"lumi.html#-how-to-get-access-to-lumi","title":"- How to get access to LUMI","text":""},{"location":"lumi.html#-software","title":"- Software","text":""},{"location":"lumi.html#-examples-of-jobs-and-slurm-scripts","title":"- Examples of jobs and slurm scripts","text":""},{"location":"lumi.html#-billing","title":"- Billing","text":""},{"location":"modules.html","title":"Module environment (lmod)","text":""},{"location":"modules.html#short-introduction","title":"Short introduction","text":"<p>HPC has a module step-system. To use some application, user needs to follow these two steps, and insert applications into the search path:</p> <ol> <li> <p>Determine the machine type (eg. amp or green) by command:</p> <p>module load rocky8-spack      # for most free programs (SPACK package manager) </p> <p>or</p> <p>module load rocky8/all        # for licensed programs and some free (non-SPACK managed)</p> </li> <li> <p>Load program needed:</p> <p>module load tau</p> </li> </ol> <p>The list of avalible modules can be looked by:</p> Text Only<pre><code>module avail\n</code></pre> <p>where: Lic   - a license is required, see user-guide for more information Uni   - commercial software with site licence, number of concurrent processes may be limited Reg   - registration required, see user-guide for more information L  -  module is loaded Dp  - deprecated (old modules, which have been superseeded by modules in the new structure) O  - obsolete (module moved or superseeded by SPACK module) Exp  - Experimental module, used while testing software installation, module name may change or software may be deleted D - default Module. <p> </p>"},{"location":"modules.html#long-version","title":"Long version","text":"<p>The module system is used to manage settings for different applications. Many applications and libraries are not in the standard search path, this way it is possible to install two different versions of the same software/library that would otherwise create conflicts. The module system is used to insert applications into the search path (or remove them from it) on a per user and per occasion basis.</p>"},{"location":"modules.html#useful-commands","title":"Useful commands","text":"<ul> <li> <p>All available modules can be looked through by command:</p> <p>module avail</p> <p>example output:</p> <p> <p></p> <p>modules are grouped in a hierarchy, there may be several versions of the same software installed, e.g. of the MPI library. Only one of these can be loaded at a single time. The default module of a group is marked by the <code>(D)</code>, if there is only one module in a group this is the default (unmarked).</p> <li> <p>To load a certain version of a module/program (here - Open MPI 4.1.1-gcc-8.5.0-r8-ib):</p> <p>module load openmpi/4.1.1-gcc-8.5.0-r8-ib </p> <p>To load the default module/program marked <code>(D)</code> (here - Open MPI 4.1.1-gcc-10.3.0-r8-tcp):</p> <p>module load openmpi/</p> </li> <li> <p>To list all loaded modules</p> <p>module list        </p> </li> <li> <p>Unloading a module (here - Cuda 11.3.1-gcc-10.3.0-ehi3):</p> <p>module unload cuda/11.3.1-gcc-10.3.0-ehi3</p> </li> <li> <p>Finding a module containing a certain part (here - fem):</p> Text Only<pre><code>module keyword fem\n</code></pre> <p>Will be listed all modules that have \"fem\" in the description:</p> <p> <p></p> <li> <p>To find out more about a specific module (here - mfem):</p> <p>module spider mfem</p> <p>gives</p> <p> <p></p> <li> <p>The <code>module whatis</code> command gives you a short explanation what the software is about, e.g.</p> <p> <p></p> <p>or</p> <p> <p></p> <p></p>"},{"location":"modules.html#files-modulerclua-and-bashrc","title":"Files .modulerc.lua and .bashrc","text":"<p>Personal preferences and resources can be specified in the files <code>.modulerc.lua</code> and  <code>.bashrc</code> in the user's <code>$HOME</code> directory. For example, it is possible to add a path for own module files for software installed by the user in the user's <code>$HOME</code> directory,  automatically load some modules on login and to define one's own \"default\" modules using the entry \"module_version(\"r/4.1.1-gcc-10.3.0-zwgc\",\"default\")\" or introduce abbreviations using an entry like \"module_alias(\"z13\", \"r/4.1.1-gcc-10.3.0-zwgc\")\" to define a module alias \"z13\".</p> <p>examplele of <code>.modulerc.lua</code> file</p> Text Only<pre><code>module_version(\"r/4.1.1-gcc-10.3.0-zwgc\",\"default\")\nmodule_alias(\"z13\",\"r/4.1.1-gcc-10.3.0-zwgc\")\n\nmodule_version(\"p/20.2-gcc-10.3.0-python-2.7.18-ij2m\",\"default\")\nmodule_alias(\"p20\",\"p/20.2-gcc-10.3.0-python-2.7.18-ij2m\")\n</code></pre> <p>examplele of <code>.bashrc</code> file</p> Text Only<pre><code># .bashrc\n\n# Source global definitions\nif [ -f /etc/bashrc ]; then\n    . /etc/bashrc\nfi\n\n# Uncomment the following line if you don't like systemctl's auto-paging feature:\n# export SYSTEMD_PAGER=\n\n# User specific aliases and functions\nmodule load rocky8-spack\nmodule load r/4.1.1-gcc-10.3.0-zwgc\n</code></pre> <p> </p>"},{"location":"modules.html#module-groups","title":"Module groups","text":"<p> We moved to a new module structure! Modules from <code>/share/apps/modules</code> are being retired. Software is compiled for <code>x86-64</code> and will run o all nodes (no special optimization). Optimized versions for some software for specific nodes may follow later (or not). <p>New modules are grouped, you can activate them by loading one or more of the following modules:</p> modulegroup description rocky8/all manually installed software rocky8-spack software installed with spack package manager <p></p> <p> </p>"},{"location":"modules.html#modules-used-on-viz","title":"Modules used on viz","text":"<p>In order to make the module system work on viz, the following needs to be added to your <code>$HOME/.bashrc</code></p> Text Only<pre><code>if  [[ $(hostname -s) = viz ]]; then\n   source /usr/share/lmod/6.6/init/bash\n   module use /gpfs/mariana/modules/system\nfi\n</code></pre> <p>Further access to viz and load modules needed. For example:</p> Text Only<pre><code>ssh -X -A -J UNI-ID@base.hpc.taltech.ee   UNI-ID@viz.hpc.taltech.ee\n\nmodule load viz-spack\nmodule load jmol\n</code></pre> <p>More about use of viz can be found at visualization page.</p> <p> </p>"},{"location":"modules.html#available-modules","title":"Available modules","text":"<p>Currently the following modules are available, this serves as an example, please note that the list on this page will be updated very seldom, use <code>module avail</code> after login to get an up-to-date list of the availabe modules.</p> Click to expand      ---------------------- /gpfs/mariana/modules/rocky8/all -----------------------        abaqus/2018                       (Uni)        abaqus/2021                       (Uni,D)        abcl/1.9.2        ansys-autodyn/23.2                (Lic,E)        ansys-fluids/23.2        gaussian/09        gaussian/16.c01        gaussian/16.c02        gaussian/16                       (D)        gaussview/6.1        lsdyna/intel-mpp-13.0.0-d         (Lic)        lsdyna/intel-mpp-13.0.0-s         (Lic)        lsdyna/smp-13.0.0-d               (Lic)        lsdyna/smp-13.0.0-s               (Lic,D)        matlab/2014b                      (Uni)        matlab/2018a                      (Uni)        matlab/2021b                      (Uni)        matlab/2022b                      (Uni)        matlab/2023b                      (Uni,D)        orca/4.1.2-lic        orca/4.1.2        orca/5.0.4                        (D)        sbcl/2.2.2                        (L)        star-ccm+/15.06.007-r8            (Lic)        star-ccm+/15.06.008-r8            (Lic)        star-ccm+/16.04.007-r8        star-ccm+/16.04.012-r8            (Lic)        star-ccm+/17.02.008-r8            (Lic)        star-ccm+/17.04.008-r8-cad-nx-con (Lic)        star-ccm+/17.04.008-r8            (Lic)        star-ccm+/17.06.007-r8            (Lic)        star-ccm+/18.04.008-r8            (Lic)        star-ccm+/18.04.009-r8            (Lic,D)        stda/1.6.3        vmd/1.9.3-text                    (Reg,L)       ----------------- /gpfs/mariana/modules/rocky8/rocky8-x86_64 ------------------        bcftools/1.19-gcc-10.3.0-openblas-2nph        bioawk/1.0-gcc-10.3.0-xzhf        chapel/1.24.1-gcc-10.3.0-kryx        cmake/3.29.2-gcc-10.3.0-jsyg        cuda/11.3.1-gcc-10.3.0-ehi3        cuda/12.2.2-gcc-10.3.0-5rec                                 (D)        curl/8.7.1-gcc-10.3.0-xdf5                                  (L)        elmerfem/9.0-gcc-10.3.0-4uqn        fastqc/0.12.1-gcc-10.3.0-56yh        fftw/3.3.10-gcc-10.3.0-ccxe        freefem/4.10-gcc-10.3.0-n4wl        gawk/5.3.0-gcc-8.5.0-a7a3        gcc/10.3.0-gcc-8.5.0-abqv                                   (L)        git/2.45.1-gcc-10.3.0-5xxc        glib/2.78.3-gcc-10.3.0-on3e        gnuplot/6.0.0-gcc-10.3.0-lysd        gperf/3.1-gcc-10.3.0-ogys        gromacs/2024.2-gcc-10.3.0-gqvn        hdf5/1.14.3-gcc-10.3.0-pkxk        hypre/2.31.0-gcc-10.3.0-kpoh        jmol/14.31.0-gcc-10.3.0-wajs        julia/1.10.2-gcc-10.3.0-openblas-a6d4        libcatalyst/2.0.0-gcc-10.3.0-openblas-vpu7        libfabric/1.18.0-gcc-10.3.0-qtmt        lmod/8.7.37-gcc-10.3.0-nprc        mawk/1.3.4-20240123-gcc-10.3.0-dw6a        mercurial/6.7.3-gcc-10.3.0-python-3.9.7-sqxv        mfem/4.6.0-gcc-10.3.0-c373        mfem/4.6.0-gcc-10.3.0-6mov                                  (D)        miniconda3/24.3.0-gcc-10.3.0-343s        molden/6.7-gcc-10.3.0-5ln6        netcdf-c/4.9.2-gcc-10.3.0-ergk        netcdf-fortran/4.6.1-gcc-10.3.0-24g7        netlib-lapack/3.11.0-gcc-10.3.0-5j7d        netlib-scalapack/2.2.0-gcc-10.3.0-nvhpc-openmpi-frty        nvhpc/22.9-gcc-10.3.0-c6t2        nvtop/3.0.1-gcc-10.3.0-pure        nvtop/3.0.1-gcc-10.3.0-v7ne        nvtop/3.0.1-gcc-10.3.0-zq2a                                 (D)        nwchem/7.2.2-gcc-10.3.0-netlib-lapack-klio        octave-symbolic/2.9.0-gcc-10.3.0-netlib-lapack-24iy        octave/9.1.0-gcc-10.3.0-netlib-lapack-tjlh        openblas/0.3.26-gcc-10.3.0-khv7                             (L)        openfoam-org/5.0-gcc-10.3.0-sblp        openfoam-org/10-gcc-10.3.0-jhub                             (D)        openfoam/1912_220610-gcc-10.3.0-j6wc        openfoam/2106_220610-gcc-10.3.0-2jnl        openfoam/2312-gcc-10.3.0-gclj                               (L,D)        openrasmol/2.7.5.2-gcc-10.3.0-rnlx        paraview/5.9.1-gcc-10.3.0-openblas-464b                     (L)        petsc/3.21.1-gcc-10.3.0-netlib-lapack-lqid        picard/3.1.1-gcc-10.3.0-madg        py-anyio/3.6.2-gcc-10.3.0-python-3.9.7-bugc        py-argon2-cffi-bindings/21.2.0-gcc-10.3.0-python-3.9.7-ir5u        py-argon2-cffi/21.3.0-gcc-10.3.0-python-3.9.7-e3q4        py-asttokens/2.4.0-gcc-10.3.0-python-3.9.7-7l43        py-attrs/23.1.0-gcc-10.3.0-python-3.9.7-lytk        py-babel/2.12.1-gcc-10.3.0-python-3.9.7-6e4r        py-backcall/0.2.0-gcc-10.3.0-python-3.9.7-omya        py-beautifulsoup4/4.12.2-gcc-10.3.0-python-3.9.7-3mbk        py-bleach/6.0.0-gcc-10.3.0-python-3.9.7-jkow        py-certifi/2023.7.22-gcc-10.3.0-python-3.9.7-u4vk        py-cffi/1.15.1-gcc-10.3.0-python-3.9.7-njeq        py-charset-normalizer/3.3.0-gcc-10.3.0-python-3.9.7-ahmd        py-comm/0.1.4-gcc-10.3.0-python-3.9.7-tbis        py-contourpy/1.0.7-gcc-10.3.0-openblas-nf6f        py-cycler/0.11.0-gcc-10.3.0-python-3.9.7-5qo2        py-debugpy/1.6.7-gcc-10.3.0-python-3.9.7-ixy4        py-decorator/5.1.1-gcc-10.3.0-python-3.9.7-6o6k        py-defusedxml/0.7.1-gcc-10.3.0-python-3.9.7-etbf        py-executing/1.2.0-gcc-10.3.0-python-3.9.7-jb5a        py-fastjsonschema/2.16.3-gcc-10.3.0-python-3.9.7-2tkd        py-fonttools/4.39.4-gcc-10.3.0-python-3.9.7-zbrq        py-gevent/23.7.0-gcc-10.3.0-python-3.9.7-ywdo        py-greenlet/2.0.2-gcc-10.3.0-python-3.9.7-g3wh        py-idna/3.4-gcc-10.3.0-python-3.9.7-xfuj        py-importlib-metadata/7.0.1-gcc-10.3.0-python-3.9.7-phev        py-importlib-resources/5.12.0-gcc-10.3.0-python-3.9.7-zc5z        py-ipykernel/6.23.1-gcc-10.3.0-openblas-ejda        py-ipython-genutils/0.2.0-gcc-10.3.0-python-3.9.7-7tjr        py-ipython/8.14.0-gcc-10.3.0-openblas-cevj        py-jedi/0.18.2-gcc-10.3.0-python-3.9.7-owt3        py-jinja2/3.1.2-gcc-10.3.0-python-3.9.7-ejri        py-json5/0.9.14-gcc-10.3.0-python-3.9.7-setp        py-jsonschema/4.17.3-gcc-10.3.0-python-3.9.7-5bd3        py-jupyter-client/8.2.0-gcc-10.3.0-python-3.9.7-wexx        py-jupyter-core/5.3.0-gcc-10.3.0-python-3.9.7-wld5        py-jupyter-server/1.21.0-gcc-10.3.0-python-3.9.7-b25c        py-jupyterlab-pygments/0.2.2-gcc-10.3.0-python-3.9.7-kttz        py-jupyterlab-server/2.22.1-gcc-10.3.0-python-3.9.7-ryk3        py-jupyterlab/3.4.2-gcc-10.3.0-openblas-xsvk        py-kiwisolver/1.4.5-gcc-10.3.0-python-3.9.7-yygr        py-markupsafe/2.1.3-gcc-10.3.0-python-3.9.7-bpul        py-matplotlib-inline/0.1.6-gcc-10.3.0-openblas-o5z6        py-matplotlib/3.8.4-gcc-10.3.0-openblas-6vmn        py-mistune/2.0.5-gcc-10.3.0-python-3.9.7-eid7        py-nbclassic/0.4.8-gcc-10.3.0-openblas-zx6b        py-nbclient/0.8.0-gcc-10.3.0-python-3.9.7-b7xq        py-nbconvert/7.14.1-gcc-10.3.0-python-3.9.7-hxvg        py-nbconvert/7.14.1-gcc-10.3.0-python-3.9.7-y6ii            (D)        py-nbformat/5.8.0-gcc-10.3.0-python-3.9.7-74ab        py-nest-asyncio/1.5.6-gcc-10.3.0-python-3.9.7-4hdc        py-notebook-shim/0.2.3-gcc-10.3.0-python-3.9.7-w5cs        py-numpy/1.21.6-gcc-10.3.0-openblas-kaao        py-packaging/23.1-gcc-10.3.0-python-3.9.7-bzma        py-pandocfilters/1.5.0-gcc-10.3.0-python-3.9.7-uitt        py-parso/0.8.3-gcc-10.3.0-python-3.9.7-qxxb        py-pexpect/4.8.0-gcc-10.3.0-python-3.9.7-wwkw        py-pickleshare/0.7.5-gcc-10.3.0-python-3.9.7-xyrl        py-pillow/10.3.0-gcc-10.3.0-python-3.9.7-spkd        py-pip/23.1.2-gcc-10.3.0-python-3.9.7-pmrk        py-platformdirs/3.10.0-gcc-10.3.0-python-3.9.7-7pms        py-prometheus-client/0.17.0-gcc-10.3.0-python-3.9.7-ajdv        py-prompt-toolkit/3.0.38-gcc-10.3.0-python-3.9.7-7733        py-psutil/5.9.5-gcc-10.3.0-python-3.9.7-bxii        py-ptyprocess/0.7.0-gcc-10.3.0-python-3.9.7-ubd4        py-pure-eval/0.2.2-gcc-10.3.0-python-3.9.7-7e7k        py-pybind11/2.12.0-gcc-10.3.0-python-3.9.7-yyzs        py-pycparser/2.21-gcc-10.3.0-python-3.9.7-l633        py-pygments/2.13.0-gcc-10.3.0-python-3.9.7-df5m        py-pyparsing/3.1.2-gcc-10.3.0-python-3.9.7-ojnc        py-pyrsistent/0.19.3-gcc-10.3.0-python-3.9.7-u6lp        py-python-dateutil/2.8.2-gcc-10.3.0-python-3.9.7-uc6q        py-pyzmq/25.0.2-gcc-10.3.0-python-3.9.7-vtzs        py-requests/2.31.0-gcc-10.3.0-python-3.9.7-evis        py-send2trash/1.8.0-gcc-10.3.0-python-3.9.7-bgvz        py-setuptools/59.4.0-gcc-10.3.0-python-3.9.7-mjeg        py-six/1.16.0-gcc-10.3.0-python-3.9.7-pz4t        py-sniffio/1.3.0-gcc-10.3.0-python-3.9.7-opfj        py-soupsieve/2.4.1-gcc-10.3.0-python-3.9.7-oanu        py-stack-data/0.6.2-gcc-10.3.0-python-3.9.7-nkn4        py-terminado/0.17.1-gcc-10.3.0-python-3.9.7-zfq4        py-tinycss2/1.2.1-gcc-10.3.0-python-3.9.7-ddln        py-tornado/6.3.3-gcc-10.3.0-python-3.9.7-c7p3        py-traitlets/5.9.0-gcc-10.3.0-python-3.9.7-r677        py-typing-extensions/4.8.0-gcc-10.3.0-python-3.9.7-ql62        py-urllib3/2.1.0-gcc-10.3.0-python-3.9.7-edpr        py-wcwidth/0.2.7-gcc-10.3.0-python-3.9.7-tcwk        py-webencodings/0.5.1-gcc-10.3.0-python-3.9.7-b533        py-websocket-client/1.6.3-gcc-10.3.0-python-3.9.7-5po6        py-zipp/3.17.0-gcc-10.3.0-python-3.9.7-5b4u        py-zope-event/4.6-gcc-10.3.0-python-3.9.7-ffgp        py-zope-interface/5.4.0-gcc-10.3.0-python-3.9.7-2q5a        python/3.9.7-gcc-10.3.0-kxnt                                (L)        qwt/6.1.6-gcc-10.3.0-vimf        r/4.4.0-gcc-10.3.0-netlib-lapack-mvzt        samtools/1.19.2-gcc-10.3.0-6l26        screen/4.9.1-gcc-10.3.0-rzjc        sortmerna/2017-07-13-gcc-10.3.0-3nqh        star/2.7.11b-gcc-10.3.0-26bs        su2/8.0.1-gcc-10.3.0-xdk6        tmux/3.4-gcc-10.3.0-5b7i        trimmomatic/0.39-gcc-10.3.0-qwqr        vcftools/0.1.16-gcc-10.3.0-r5ch      ------------------------ /gpfs/mariana/modules/system -------------------------        amp-spack/0.17.1                       (D)        amp-spack/0.19.0        amp/all        experimental/all        oldmodules/all                         (Dp)        openmpi/1.10.7-gcc-10.3.0-r8-cuda-12.2        openmpi/4.1.1-gcc-8.5.0-r8-ib        openmpi/4.1.1-gcc-9.3-amp        openmpi/4.1.1-gcc-9.3-amppmi        openmpi/4.1.1-gcc-9.3-amppmicuda11.4        openmpi/4.1.1-gcc-10.3.0-r8-cuda-12.2  (L)        openmpi/4.1.1-gcc-10.3.0-r8-ib-openib        openmpi/4.1.1-gcc-10.3.0-r8-ib-ucx        openmpi/4.1.1-gcc-10.3.0-r8-tcp        openmpi/4.1.1-gcc-10.3.0-r8            (D)        rocky8-spack/master                    (L)        rocky8/all        viz-spack/0.17.1        viz-spack/0.19.2                       (D)        viz/all     <p></p> <p>By default SPACK builds optimized for the CPU the software is build on. The packages from the amp nodes will work on the green nodes but slower than the optimized modules. Conversely, the skylake-optimized modules will try to use hardware features not present on the green nodes, so the software will not run there.</p> <p> </p>"},{"location":"mpi.html","title":"Available MPI versions (and comparison)","text":"<p>The cluster has OpenMPI installed.</p> <p>On all nodes:</p> Text Only<pre><code>module load openmpi/4.1.1-gcc-10.3.0-r8-tcp\n</code></pre> <p>NB: Currently only use TCP transport! There are several OpenMPI modules for diffrent versions, different modules for the same version only differ in environment variables for transport selection.</p> <p>Normally, OpenMPI will choose the fastest interface, it will try RDMA over Ethernet (RoCE) which causes \"[qelr_create_qp:683]create qp: failed on ibv_cmd_create_qp\" messages, these can be ignored, it will fail over to IB (higher bandwidth anyway) or TCP.</p> <p>NB: <code>mpirun</code> is not available, use <code>srun</code></p> <p>For MPI jobs prefer the green-ib partition (<code>#SBATCH -p green-ib</code>) or stay within a single node (<code>#SBATCH -N 1</code>).</p> Text Only<pre><code>export OMPI_MCA_btl_openib_warn_no_device_params_found=0 srun ./hello-mpi\n</code></pre> <p></p>"},{"location":"mpi.html#layers-in-openmpi","title":"Layers in OpenMPI","text":"<ul> <li>PML = Point-to-point Management Layer:</li> <li>UCX</li> <li>MTL = Message Transfer Layer:</li> <li>PSM, </li> <li>PSM2, </li> <li>OFI</li> <li>BTL = Byte Transfer Layer:</li> <li>TCP, </li> <li>openib</li> <li>self</li> <li>sm (OpenMPI 1), vader (OpenMPI 4) </li> </ul> <p>The layers can be confusing, so was openib originally developed for InfiniBand, but is now used for RoCE and is deprecated for IB. However, on some IB cards and configurations it is the only working option. Also, the MVAPICH implementation still uses the openib (verbs) instead of UCX.</p> <p>Layers can be selected using environment variables:</p> <p>To select TCP transport:</p> Text Only<pre><code>export OMPI_MCA_btl=tcp,self,vader\n</code></pre> <p>To select RDMA transport (verbs):</p> Text Only<pre><code>export OMPI_MCA_btl=openib,self,vader\n</code></pre> <p>To select UCX transport:</p> Text Only<pre><code>export OMPI_MCA_pml=ucx\n</code></pre> <p>NB! UCX is not supported on QLogic FastLinQ QL41000 Ethernet controllers.</p>  For further explanations and details see:  -  -  <p> </p>"},{"location":"mpi.html#different-mpi-implementations-exist","title":"Different MPI implementations exist:","text":"<ul> <li>OpenMPI</li> <li>MPICH</li> <li>MVAPICH</li> <li>IBM Platform MPI (MPICH descendant)</li> <li>IBM Spectrum MPI (OpenMPI descendant)</li> <li>(at least one for each network and CPU manufacturer) </li> </ul>"},{"location":"mpi.html#openmpi","title":"OpenMPI","text":"<ul> <li>available in any Linux or BSD distribution</li> <li>combining technologies and resources from several other projects (incl. LAM/MPI)</li> <li>can use TCP/IP, shared memory, Myrinet, Infiniband and other low latency interconnects</li> <li>chooses fastest interconnect automatically (can be manually choosen, too)</li> <li>well integrated into many schedulers (e.g. SLURM)</li> <li>highly optimized</li> <li>FOSS (BSD license) </li> </ul>"},{"location":"mpi.html#mpich","title":"MPICH","text":"<ul> <li>highly optimized</li> <li>supports TCP/IP and some low latency interconnects</li> <li>(older versions) DO NOT support InfiniBand (however, it supports MELLANOX IB)</li> <li>available in many Linux distributions</li> <li>? not intgrated into schedulers </li> <li>used to be a PITA to get working smoothly</li> <li>FOSS </li> </ul>"},{"location":"mpi.html#mvapich","title":"MVAPICH","text":"<ul> <li>highly optimized (maybe slightly faster than OpenMPI)</li> <li>fork of MPICH to support IB</li> <li>comes in many flavors to support TCP/IP, InfiniBand and many low latency interconnects: OpenSHMEM, PGAS</li> <li>need to install several flavors and users need to choose the right one for the interconnect they want to use</li> <li>generally not available in Linux distributions</li> <li>not integrated with schedulers (integrated with SLURM only after version 18)</li> <li>FOSS (BSD license) </li> </ul>"},{"location":"mpi.html#recommendation","title":"Recommendation","text":"<ul> <li>default: use OpenMPI on our clusters</li> <li>if unsatisfied with performance and running on single node or over TCP, try MPICH</li> <li>if unsatisfied with performance and running on IB try MVAPICH </li> </ul>  For a comparison, see for example:  -    -"},{"location":"performance.html","title":"Performance","text":"<p>How fast is your program? How good makes it use of available hardware? Could it run faster on the same hardware?</p> <p></p>"},{"location":"performance.html#benchmarking","title":"Benchmarking","text":"<p>Benchmarking is the art and skill to find out how fast your program and hardware is. Many factors influence the execution time of your program, the obvious ones are the speed and type of processors, speed of memory, etc, but also the less obvious ones have a huge impact (maybe larger than the speed of the processor). The less obvious ones include programming style and language! Yes, there are slow and fast languages. Slow languages include Python and basic, fast languages include C, Fortran, Julia and Rust. The programming style has an impact as well, as it influences memory access and optimization possibilities of the compiler/interpreter.</p> <p>In order to find out just how fast your hardware is, you need a software that uses all of the available components in an optimal way.</p> <p></p>"},{"location":"performance.html#desktop-vs-compute-node","title":"Desktop vs. Compute Node","text":"Why is my (single-thread) job not faster on the cluster than on my desktop?  - Compute nodes do not have higher clock frequencies than desktop computers, but they have more cores and more RAM - A single thread job on your desktop probably uses \u201cboost-frequency\u201d when the other cores are idle, a compute node has usually many busy cores and therefore \u201cboost-frequency\u201d is not possible - A CPU has typically 6 memory channels, several applications share these on the compute node, while on the desktop only your single application uses them.   Why is my x-ntask parallel job not x-times as fast than my sequential job?  - not enough to do for each core (task) - load imbalance, some tasks need to wait till another task finishes - communication between tasks introduces overhead - congestion of memory channels (see above)"},{"location":"performance.html#parallel-scaling","title":"Parallel scaling","text":"<p>The assumption that all programs run faster when more cores are used is generally wrong. Especially a seqential (single threaded) program will not be able to use more than 1 core, but also for parallel programs there is an optimum of how many cores to use. The relationship between program speed and number of cores used is called scaling. Usually the scaling needs to be tested! It is not uncommon that programs run slower when more cores are used. For the rational use of resources, it is necessary to determine the optimum of this particular program. </p>"},{"location":"performance.html#strong-scaling","title":"Strong scaling","text":"<p>Strong scaling is defined as how the solution time varies with the number of processors for a fixed total problem size.  </p> <p>If a simulation exhibits strong scaling, then the same problem is solved faster with linear speedup and the workload per processor is reduced. Strong scaling is mostly used for long-running CPU-bound applications. However, the speedup achieved by increasing the number of processes usually decreases.</p> <p></p>"},{"location":"performance.html#weak-scaling","title":"Weak scaling","text":"<p>Weak scaling is defined as how the solution time varies with the number of processors for a fixed problem size per processor.</p> <p>Thus, both the number of processors and the problem size are increased, which results in a constant workload per processor. Weak scaling is mostly used for large memory-bound applications.</p>"},{"location":"performance.html#how-many-cores-or-tasks-to-use","title":"How many cores or tasks to use?","text":"<p>NOTE: Parallel does not (necessarily) mean faster!!! Parallel execution introduces overhead (starting threads, communication)! For optimal execution time and optimal use of resources one needs to test (run the same (a typical) simulation with different numbers of parallel tasks) and find the sweet spot.</p> <p></p> <p>The division into the areas is a combined decision taking into account \"real\" (wall clock) and \"user\" (summed time of all threads) time (from the time command). \"Wall clock\" (real) time is the time one needs to wait till the job is finished, \"Summed thread time\" (user) is the sum of the times that all individual threads needed, it should be roughly user = numtreads x real. For parallel programs, one can expect that \"user\" time of the parallel run is larger than for the sequential, due to communication overhead, if it is smaller, that probably means the individual threads could make better use of cache.</p> area why explanation when to use sweet spot minimal \"user\" time = minimal heat production, optimal use of resources regular use good range linear speedup for \"real\", with constant or slightly increasing \"user\" approaching deadline OK range slightly less than linear speedup for \"real\", and slightly increasing \"user\" pushing hard to make a deadline avoid ascending slope in the diagram for \"real\" and \"user\" one actually needs to wait longer compared to the case with fewer cores NEVER <p>In our example case, it is recommended  to request 4 or 8 threads; 8 threads if the software does not benefit from HyperThreading: <code>-n 8 --ntasks-per-node 8</code> but use <code>mpirun -n 4</code>, this would make sure the software has the full cores for itself and not be sharing it with other software.</p> <p>A rule of thumb for FEA/CFD is: keep at least 50000 cells per core (500k cells means not more that 10 tasks or cores).</p> <p>Here are a couple more examples:</p> <p>Matrix-matrix multiplication of two 2000x2000 matrices:</p> <p>Wall-clock time of the whole job:</p> ntasks none bind cores bind threads bind sockets 2 57.198766 56.398039 57.485236 56.268668 4 28.018947 8 20.035140 16 11.012887 32 4.914047 5.050369 5.454213 40 3.951628 48 3.309776 <p>CPU-efficiency reported by SLURM:</p> ntasks none bind cores bind threads bind sockets 2 97.46% 97.41% 97.46% 97.41% 4 94.17% 8 81.82% 16 77.68% 32 71.18% 69.10% 73.61% 40 61.11% 48 60.19% <p>Billed CPU-time:</p> ntasks none bind cores bind threads bind sockets 2 1:56 4 2:00 8 3:28 16 3:44 32 4:48 40 6:00 48 7:12 <p> </p> <p></p> <p>Simple way to check if CPUs are busy enough: run an interactive session on the same node:</p> Text Only<pre><code>srun -w green&lt;Nnmber&gt; --pty bash \nhtop\n</code></pre> <p>ideally the bars of your process are all green. To find your the processor of your job, you can add a column or use </p> Text Only<pre><code>ps aux |grep $USER      # to get the PID \ntaskset -c -p PID\n</code></pre> <p>(add 1 to the number from taskset to find the processor in htop, because taskset starts from 0, htop from 1).</p> <p>These jobs have spend over 90% CPU-time in calculations:</p> <p> </p> <p>This jobs spends already much more CPU-time in communication:</p> <p></p> <p>This jobs spends less than 10% CPU-time in calculation and over 90% in communication, reducing the number of <code>ntasks</code> will probably speed it up considerably:</p> <p></p> <p></p>"},{"location":"performance.html#clean-vs-fast-code-python","title":"Clean vs. fast code (Python)","text":"<p>Clean Code states:</p> <p>The \ufb01rst rule of functions is that they should be small. The second rule of functions is that they should be smaller than that.</p>  This rule is unfortunately often taken to the extreme, leading to several problems:  1. Someone reading the code for the first time is hopping around the code to find out what the tiny functions are actually doing. 2. ***Hot*** and ***cold*** code is mixed (instruction cache misses). 3. Function calls are expensive, especially in Python, regarding computation time. (In contrast to Python, a C/C++/Fortran compiler may inline small functions, thus solving the issue, though there is no guarantee that the compiler will inline, not even with the inline statement.) 4. A lot of variables have to be passed over multiple levels of function calls, thus increasing memory use and cache misses. Object orientation and global variables are used as a remedy for this, but this leads to functions with side effects (the reader does not know what variables the function is changing by looking at the function call).  <p></p> <p>  An example of the time function calls can waste  </p>"},{"location":"performance.html#an-example-of-the-time-function-calls-can-waste","title":"An example of the time function calls can waste:","text":"<p>Time the following codes:</p> Text Only<pre><code>for i in range(n):\n    pass\n</code></pre> <p>and:</p> Text Only<pre><code>def f():\n    pass\n\nfor i in range(n):\n    f()\n</code></pre> <p>An average function call in Python costs about 150ns, this means that you loose about 2-3 orders of magnitude of processor speed!  That means that your fancy 2.5 GHz from 2021 runs as slow as a 25 MHz processor from 1991 (or even as slow as a 2.5 MHz processor)! Now that's something to think about.</p> <p>A good read on this are the-cost-of-a-python-function-call and python_function_call_overhead; and so is small-functions-considered-harmful.</p> <p>To learn:</p> <p>Use meaningful function blocks. Define functions for code-blocks that are re-used in other parts of the program. Do not define 1-line functions, except you have a very good reason!</p> <p> </p> <p>Are newer processors better/faster in every case? When does it make sense to switch to the newer nodes? When does it make sense to still use the older nodes?</p> <p>Intel Xeon E5-2630L 6C 2.00 GHz (max turbo 2.50 GHz) 4 memory channels 42.6 GB/s max memory bandwidth (~10.65 GiB/s memory bandwidth per channel) GFlops per core or CPU</p> <p>memory channels per core: 4/6 (0.75) bandwidth per core: ~7.99 GiB/s bandwidth per GFlop:</p> <p>Xeon Gold 6148 20C 2.40 GHz (max turbo 3.70 GHz) Turbo Frequency (20 Cores): 3.00 GHz 6 memory channels 119.21 GiB/s max  memory bandwidth 19.87 GiB/s memory bandwidth per channel GFlops per core or CPU</p> <p>memory channels per core: 6/20 (0.30) bandwidth per core: 5.961 GiB/s  bandwidth per GFlop: 0.133 GiB/s</p> <p>Bandwidth   Single 19.87 GiB/sDouble 39.74 GiB/sQuad 79.47 GiB/sHexa 119.21 GiB/s</p>"},{"location":"profiling.html","title":"Profiling","text":"<p> not changed to rocky yet</p>"},{"location":"profiling.html#profiling","title":"Profiling","text":"<p>Is the skill and art of finding which part of your code needs the most time, and therefore to find the place where you can (should, need to) optimize (first). The optimization can be different things, like using library functions instead of self-written ones, re-arranging memory access, removing function calls, writing C/Fortran functions for your Python code.</p> <p>The profiling can be done manually by adding time and print statements to your code or (better) by using tools like Valgrind, TAU, HPCToolkit, Score-P or Python's Scalene or cProfile.</p> <p>Tools to profile applications and perform efficiency, scaling and energy analysis are described in this document by the Virtual Institute High Performance Computing: https://www.vi-hps.org/cms/upload/material/general/ToolsGuide.pdf</p> <p></p>"},{"location":"profiling.html#monitoring-jobs-on-the-node","title":"Monitoring jobs on the node","text":"<p>It is possible to submit a second (this time) interactive job to the node where the main job is running, check with <code>squeue</code> where your job is running, then submit</p> Text Only<pre><code>srun -w &lt;nodename&gt; --pty htop\n</code></pre> <p>Note that there must be free slots on the machine, so you cannot use <code>-n 80</code> or <code>--exclusive</code> for your main job (use <code>-n 78</code>).</p> <p>Alternative method if you have X11, e.g. on Linux computers:</p> <p>When you login to base, use <code>ssh -X -Y UniID@base.hpc.taltech.ee</code>,</p> <p>then submit your main job with <code>srun --x11 -n &lt;numtasks&gt; --cpus-per-task=&lt;numthreads&gt; --pty bash and start an</code>xterm -e htop &amp;` in the session.</p> <p>In <code>sbatch</code> the option <code>--x11=batch</code> can be used, note that the ssh session to base needs to stay open!</p> <p></p>"},{"location":"profiling.html#valgrind","title":"Valgrind","text":"<p>Manual: http://valgrind.org/docs/manual/manual.html</p> <p>Valgrind is an instrumentation framework for building dynamic analysis tools. There are Valgrind tools that can automatically detect many memory management and threading bugs, and profile your programs in detail. You can also use Valgrind to build new tools.</p> <p>The Valgrind distribution currently includes seven production-quality tools: a memory error detector, two thread error detectors, a cache and branch-prediction profiler, a call-graph generating cache and branch-prediction profiler, and two different heap profilers. It also includes an experimental SimPoint basic block vector generator. </p>"},{"location":"profiling.html#callgrind","title":"Callgrind","text":""},{"location":"profiling.html#cachegrind","title":"Cachegrind","text":""},{"location":"profiling.html#profiling-python","title":"Profiling Python","text":"<p>Python is very slow, the best improvement is achieved by rewriting (parts of) the program in Fortran or C. See also \"Python Performance Matters\" by Emery Berger (Strange Loop 2022) https://www.youtube.com/watch?v=vVUnCXKuNOg</p>"},{"location":"profiling.html#python-scalene","title":"Python Scalene","text":"<p>Scalene is a CPU, GPU and memory profiler for Python that is very performant (introduces very little overhead).</p> <p>Installation: load your favourite Python module, e.g.</p> Text Only<pre><code>module load green-spack\nmodule load python/3.8.7-gcc-10.3.0-plhb\npy-pip/21.1.2-gcc-10.3.0-python-3.8.7-bj7d\n</code></pre> <p>then install using pip:</p> Text Only<pre><code>python -m pip install --user scalene\n</code></pre> <p>Homepage and quickstart: https://github.com/plasma-umass/scalene</p>"},{"location":"profiling.html#python-cprofile","title":"Python cProfile","text":""},{"location":"profiling.html#perf","title":"perf","text":"<p>https://perf.wiki.kernel.org/index.php/Main_Page</p> <p>perf is powerful: it can instrument CPU performance counters, tracepoints, kprobes, and uprobes (dynamic tracing). It is capable of lightweight profiling. It is also included in the Linux kernel, under tools/perf, and is frequently updated and enhanced.</p> <p>perf began as a tool for using the performance counters subsystem in Linux, and has had various enhancements to add tracing capabilities.</p> <p></p>"},{"location":"profiling.html#tau-jumpshot-paraprof","title":"TAU, Jumpshot, Paraprof","text":"<p>TAU can be used for profiling and for MPI tracing (not at the same time, though). See e.g. https://wiki.mpich.org/mpich/index.php/TAU_by_example</p> <p>Load the spack TAU module:</p> Text Only<pre><code>module load green-spack\nmodule load tau/2.30.2-gcc-10.3.0-2wge\n</code></pre> <p>Profiling</p> <p>TAU supports different methods of instrumentation:</p> Text Only<pre><code>- Dynamic: statistical sampling of a binary through preloading of libraries\n- Source: parser-aided automatic instrumentation at compile time\n- Selective: a subcategory of source, it is automatic, but guided source code instrumentation\n</code></pre> <p>The simplest and only for existing binary software is dynamic profiling through <code>tau_exec</code>, just run</p> Text Only<pre><code>srun tau_exec your_program\n</code></pre> <p>several <code>profile.*</code> files will be created. This method can unfortunately only profile MPI functions and not user-defined ones. Note, that profile files are only generated if the program exits normally, not if an error occurs or SLURM kills it!</p> <p>You can generate reports with <code>pprof</code> and visualize with <code>paraprof</code>.</p> <p>MPI tracing</p> <p>The tracing can take a lot of space, it is not uncommon that tracefiles are several GB in size for each MPI-task!</p> Text Only<pre><code>export TAU_TRACE=1\nsrun  -n 2 tau_exec ./pingpong-lg-mpi4\ntau_treemerge.pl\n</code></pre> <p>TAU does not have a tracing visualizer, but provides tools to convert its traces to other formats, e.g. slog2 for jumpshot, otf(2) or paraver:</p> Text Only<pre><code>tau2slog2 tau.trc tau.edf -o tau.slog2\ntau_convert -paraver tau.trc tau.edf trace.prv\n</code></pre> <p>The traces can be visualized using <code>jumpshot</code> (in the tau module), just run</p> Text Only<pre><code>jumpshot tau.slog2\n</code></pre> <p>jumpshot may open a huge window (larger than screen size), in this case use the \"maximize\" option of your window manager (fvwm: in the left window corner menu), jumpshot opens 3 windows: \"jumpshot-4\", \"Legend\" and \"Timeline\" (if you cannof find them, use window manager menu, e.g. fvwm: right mouse button on desktop background).</p> <p></p>"},{"location":"profiling.html#eztrace-vite","title":"EZTrace + ViTE","text":"<p>EZTrace 1.1 and ViTE 1.2 are installed on amp and viz.</p> <p>EZTrace is a tool to analyze event traces, it has several modules:</p> Text Only<pre><code>stdio   Module for stdio functions (read, write, select, poll, etc.)\nstarpu  module for StarPU framework\npthread Module for PThread synchronization functions (mutex, semaphore, spinlock, etc.)\npapi    Module for PAPI Performance counters\nopenmpi Module for MPI functions\nmemory  Module for memory functions (malloc, free, etc.)\n</code></pre> <p>ViTE is the visualization tool to visualize the generated traces, it can visualize also .otf2 traces obtained from other MPI tracing tools (e.g. converted from TAU)</p> <p></p>"},{"location":"profiling.html#hpctoolkit","title":"HPCToolkit","text":"<p>Load modules</p> Text Only<pre><code>module load hpctoolkit\n</code></pre> <p>run application with binary instrumentation</p> Text Only<pre><code>srun -n 2 -p green-ib hpcrun &lt;your_application&gt;\nhpcstruct `which &lt;your_application&gt;`\nhpcprof hpctoolkit-&lt;your_application&gt;-measurements-&lt;PID&gt;\n</code></pre> <p>run GUI tool for interpretation</p> Text Only<pre><code>hpcviewer hpctoolkit-&lt;your_application&gt;-database-&lt;PID&gt;\n</code></pre> <p>starts <code>hpcviewer</code> and opens the database.</p> <p></p>"},{"location":"profiling.html#paraver-trace-visualizer","title":"Paraver trace visualizer","text":"<p>Load the module</p> Text Only<pre><code>module load green\n\nmodule load Paraver\n</code></pre> <p>start paraver</p> Text Only<pre><code>wxparaver\n</code></pre> <p>then load the .prv trace file</p> <p></p>"},{"location":"profiling.html#extrae","title":"Extrae","text":""},{"location":"profiling.html#score-p","title":"Score-P","text":"<p>Scalable Performance Measurement Infrastructure for Parallel Codes</p> Text Only<pre><code>module load green-spack\nmodule load scorep\n</code></pre> <p>The module with hash \"mlw5\" contains the PDT instrumenter. The module with hash \"o4v3\" contains the PDT instrumenter and libunwind.</p> <p>https://scorepci.pages.jsc.fz-juelich.de/scorep-pipelines/docs/scorep-4.1/html/quickstart.html</p> <p>compilation: prefix the compiler command with \"scorep\", e.g. <code>scorep gcc ...</code> or <code>scorep mpicc ...</code>, this can also be used in Makefiles:</p> Text Only<pre><code>MPICC = $(PREP) mpicc\n</code></pre> <p>(and analogously for linkers and other compilers). One can then use the same makefile to either build an instrumented version with the</p> Text Only<pre><code>make PREP=\"scorep\"\n</code></pre> <p>a simple <code>make</code> will generate an uninstrumented binary.</p> <p>The environment variables SCOREP_ENABLE_TRACING and SCOREP_ENABLE_PROFILING control whether event trace data or profiles are stored in this directory. By setting either variable to true, the corresponding data will be written to the directory. The default values are true for SCOREP_ENABLE_PROFILING and false for SCOREP_ENABLE_TRACING.</p> <p></p>"},{"location":"profiling.html#scalasca","title":"Scalasca","text":"<p>Scalasca is a software tool that supports the performance optimization of parallel programs by measuring and analyzing their runtime behavior. The analysis identifies potential performance bottlenecks \u2013 in particular those concerning communication and synchronization \u2013 and offers guidance in exploring their causes.</p> Text Only<pre><code>module load green-spack\nmodule load scalasca\n</code></pre> <p></p>"},{"location":"profiling.html#openspeedshop","title":"Open|SpeedShop","text":"Text Only<pre><code>module load green-spack\nmodule load openspeedshop\nmodule load openspeedshop-utils\n</code></pre>"},{"location":"profiling.html#discopop","title":"DiscoPoP","text":"<p>to be installed</p> <p>analyses source-code to find areas that can be parallelized</p> <p>works only with LLVM 11.1 (included in the module)</p> Text Only<pre><code>module load green\nmodule load discopop\n</code></pre> <p></p>"},{"location":"profiling.html#pgi-nvidia-hpc-sdk","title":"PGI / Nvidia HPC SDK","text":"<p>On the GPU servers, the Nvidia HPC SDK is installed, which contains the PGI compilers and profilers.</p> <p></p>"},{"location":"profiling.html#igprof","title":"IgProf","text":"<p>The Ignominous Profiler. IgProf is a simple nice tool for measuring and analysing application memory and performance characteristics. IgProf requires no changes to the application or the build process.</p> <p>Quick start: https://igprof.org/running.html</p>"},{"location":"putty.html","title":"Putty","text":"<p>not changed to rocky yet</p>"},{"location":"putty.html#putty","title":"Putty","text":"<p>PuTTY is a free and open-source terminal emulator for Windows, Mac and Linux.  For TalTech Windows machines PuTTY client should be download from Microsoft Apps, in a all other cases Putty can be download from the official webpage.  </p> <p></p>"},{"location":"putty.html#setup","title":"Setup","text":"<ol> <li> <p>Open Putty</p> </li> <li> <p>Write host name, name of a session and save it.</p> <p></p> </li> <li> <p>To enable run of graphical applications, go to SSH and enable X11 forwarding.</p> <p></p> </li> </ol> <p> </p>"},{"location":"quickstart.html","title":"Quickstart: Cluster","text":""},{"location":"quickstart.html#accessing-the-cluster","title":"Accessing the cluster","text":"<p>NB! To access the cluster, user must have an active Uni-ID account. For people who are neither students nor employees of Taltech Uni-ID non-contractual account should be created by the head of a structural unit.</p> <p>To get access to HPC contact us by email (hpcsupport@taltech.ee) or Taltech portal. We need the following information: uni-ID, department, project that covers costs.</p> <p>The login-node of the cluster can be reached by SSH. SSH (the Secure SHell) is available using the command <code>ssh</code> in Linux/Unix, Mac and Windows-10.  A guide for Windows users using PuTTY (an alternative SSH using a graphical user interface (GUI)) is here.</p> <p>For accessing the cluster base.hpc.taltech.ee use command:</p> Text Only<pre><code>ssh uni-ID@base.hpc.taltech.ee\n</code></pre> <p>where uni-ID should be changed to user's uni-ID.</p> <p>The cluster is accessible from inside the university and from major Estonian network providers. If you are traveling (or not on one of the major networks), the access requires FortiVPN (with previously shown command) or a two-step login using a jump-host:</p> Text Only<pre><code>ssh -l uni-ID@intra.ttu.ee uni-ID@proksi.intra.ttu.ee\nssh uni-ID@base.hpc.taltech.ee\n</code></pre> <p>where all uni-ID should be changed to user's uni-ID.</p> <p>For using graphical applications add the <code>-X</code> switch to the SSH command, and for GLX (X Window System) forwarding additionally the <code>-Y</code> switch, so to be able to start a GUI program that uses GLX the connection command would be:</p> Text Only<pre><code>ssh -X -Y uni-ID@base.hpc.taltech.ee\n</code></pre> <p>NB! The login-node is for some light interactive analysis. For heavy computations, request a (interactive) session on a compute node with the resource manager SLURM or submit job for execution by SLURM sbatch script!</p> <p>We strongly recommend to use SSH-keys for logging to the cluster.</p>"},{"location":"quickstart.html#ssh-fingerprints-of-host-keys","title":"SSH fingerprints of host-keys","text":"<p>SSH key fingerprint is a security feature for easy identification/verification of the host, user is connecting to. This option allows to connect to the server without a password. On first connect, user is shown a fingerprint of a host-key, and asked if it should be added to the list of known hosts.</p> <p>Please compare the fingerprint to the ones below, if one matches, the host can be added, if the fingerprint does not match, then there is a problem (e.g. man-in-the-middle-attack).SSH host keys of our servers <p>base.hpc.taltech.ee -   ECDSA SHA256:OEfQiOB/eIG8hYoQ25sQk9T5tx9EtQbhi6sNM4C8mME -   ED25519 SHA256:t0CSTU0AnSsJThzuM68tucrcfnn2wLKabjSnuRKX8Yc -   RSA SHA256:qYrmOw/YN7wf640yBHADX3wnAOPu0OOXlcu4LKBxzG8   </p> <p>.  </p> <p>amp.hpc.taltech.ee -   ECDSA SHA256:yl6+VaKow6qDZAXL3rQY8+3d3pcH0kYg7MjGgNVTWZs -   ED25519 SHA256:YOjtpcEL2+AWm6vDFjVl0znYuQPMSVCkyFGvdO5fm8o -   RSA SHA256:4aaOxumH1ATNfiIA4mZSNMefvxfdFm5zZoUj6VR7TYo   </p> <p>.   </p> <p>viz.hpc.taltech.ee -   ECDSA SHA256:z2/bxleZ3T3vErkg4C7kvDPKKEU0qaoR8bL29EgMfGA -   ED25519 SHA256:9zRBmS3dxD7BNISZKwg6l/2+6p4HeqlOhA4OMBjD9mk -   RSA SHA256:Q6NDm88foRVTKtEAEexcRqPqMQNGUzf3rQdetBympPg</p> <p>How to get SSH keys.</p> <p> </p>"},{"location":"quickstart.html#structure-and-file-tree","title":"Structure and file tree","text":"<p>By accessing the cluster, the user gets into his home directory or <code>$HOME</code> (<code>/gpfs/mariana/home/$USER/</code>).</p> <p>In the home directory, the user can create, delete, and overwrite files and perform calculations (if slurm script does not force program to use <code>$SCRATCH</code> directory). The home directory is limited in size of 500 GB and backups are performed once per week.</p> <p>The home directory can be accessed from console or by GUI programs, but it cannot be mounted. For mounting was created special <code>smbhome</code> and <code>smbgroup</code> folders (<code>/gpfs/mariana/smbhome/$USER/</code> and <code>/gpfs/mariana/smbgroup/</code>, respectively). More about <code>smb</code> folders can be found here.</p> <p>Some programs and scripts suppose that files will be transfer to <code>$SCRATCH</code> directory at compute node and calculations will be done there. If job will be killed, for example due  to the time limit back transfer will not occur. In this case, user needs to know at which node this job was running (see <code>slurm-$job_id.stat</code>), to connect to exactly this node (in example it is green11). <code>$SCRATCH</code> directory will be in <code>/state/partition1/</code> and corresponds to jobID number.</p> Text Only<pre><code>srun -w green11 --pty bash\ncd /state/partition1/\n</code></pre> <p>Please note that the scratch is not shared between nodes, so parallel MPI jobs that span multiple nodes cannot access each other's scratch files.</p> <p> </p>"},{"location":"quickstart.html#running-jobs-with-slurm","title":"Running jobs with SLURM","text":"<p>SLURM is a management and job scheduling system at Linux clusters. SLURM quick reference can be found here.</p> <p>Examples of slurm scripts are usually given on the program's page with some recommendations for optimal use of resources for this particular program. List of the programs installed at HPC is given on our software page. At software page or program's page also can be found information about licenses, since programs installed at HPC have varying licence agreement. To use some licensed programs (for example, Gaussian), the user must be added to the appropriate group. For this contact us by email (hpcsupport@taltech.ee) or Taltech portal.</p>  The most often used SLURM commands are:   - `srun` - to start a session or an application (in real time)  - `sbatch` - to start a computation using a batch file (submit for later execution)  - `squeue` - to check the load of the cluster and status of own jobs  - `sinfo` - to check the state of the cluster and partitions  - `scancel` - to delete a submitted job (or stop a running one).  <p></p> <p>For more parameters see the man-pages (manual) of the commands <code>srun</code>, <code>sbatch</code>, <code>sinfo</code> and <code>squeue</code>. For this use the command <code>man</code> followed by the program-name whose manual you want to see, e.g.:</p> Text Only<pre><code>man srun\n</code></pre> <p>Requesting resources with SLURM can be done either with parameters to <code>srun</code> or in a batch script invoked by <code>sbatch</code>.</p> The following defaults are used if not otherwise specified:   - **default memory** -- is 1 GB/thread (for larger jobs request more memory)  - **short partition** -- **default time limit** is 10 min and  **max time limit** is 4 hours (longer jobs need to be submitted to partitions common, green-ib or gpu partitions)  - **common partition** --  **default time** is 10 min and **max time limit** is 8 days. -   **long partition** -- **default time** is 10 min and **time limit** 15 days.  - **green-ib partition** -- **default time** is 10 min and **max time limit** is 8 days  - **bigmem partition**  -- **default time** is 10 min and **max time limit** is 8 days  - **gpu partition** -- **default time** is 10 min and **max time limit** is 5 days   <p></p> <p>Running an interactive session longer than default 10 min. (here 1 hour):</p> Text Only<pre><code>srun -t 01:00:00 --pty bash\n</code></pre> <p>This logs you into one of the compute nodes, there you can load modules and run interactive applications, compile your code, etc.</p> <p>With <code>srun</code> is reccomended to use CLI (command-line interface) instead of GUI (Graphical user interface) programs if it is possible. For example, use octave-CLI or octave instead of octave-GUI.</p> <p>Running a simple non-interactive single process job that lasts longer than default 4 hours (here 5 hours):</p> Text Only<pre><code>srun --partition=common -t 05:00:00 -n 1 ./a.out\n</code></pre> <p>NB! Environment variables for OpenMP are not set automatically, e.g.</p> Text Only<pre><code>srun  -N 1 --cpus-per-task=28 ./a.out\n</code></pre> <p>would not set <code>OMP_NUM_THREADS</code> to 28, this has to be done manually. So usually, for parallel jobs it is recommended to use scripts for <code>sbatch</code>. </p> <p>Below is given an example of batch slurm script (filename: <code>myjob.slurm</code>) with explanation of the commands. </p> Text Only<pre><code>#!/bin/bash\n#SBATCH --partition=common    ### Partition\n#SBATCH --job-name=HelloOMP   ### Job Name           -J\n#SBATCH --time=00:10:00       ### WallTime           -t\n#SBATCH --nodes=4             ### Number of Nodes    -N \n#SBATCH --ntasks-per-node=7   ### Number of tasks (MPI processes)\n#SBATCH --cpus-per-task=4     ### Number of threads per task (OMP threads)\n#SBATCH --account=hpcrcf      ### In case of several accounts, specifies account used for job submission\n#SBATCH --mem-per-cpu=100     ### Min RAM required in MB\n#SBATCH --array=13-18         ### Array tasks for parameter sweep\n\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK     ### setup environment\nmodule load gcc                     ### setup environment\n./hello_omp $SLURM_ARRAY_TASK_ID            ### only for arrays, setup output files with system information\nmpirun -n 28 ./hello_mpi                ### run program\n</code></pre> <p>In this example are listed some of the more common submission parameters. There are many more possible job-submission options, moreover, some of the options listed above  are not useful to apply together. An explanation of the variables used inside SLURM/SBATCH can be found here. In contrast to e.g. GridEngine, SLURM allows fine-grained resource requests, using parameters like <code>--ntasks-per-core</code> or <code>--ntasks-per-node</code>.</p> An example script for submitting:   - a single process job  - an OpenMP parallel job   - an MPI parallel job (OpenFOAM)   - an array (parameter sweep) job   - a GPU job   - a job using the scratch partition (sequential or OpenMP parallel)  <p></p> <p>The job is then submitted to SLURM by</p> Text Only<pre><code>sbatch myjob.slurm\n</code></pre> <p>and will be executed when the requested resources become available.</p> <p>Output of applications and error messages are by default written to a <code>slurm-$job_id.out</code> file. More about SLURM finished job statistics can be found here.</p>  Some useful online resources:   - [SLURM scheduler workload manager](https://slurm.schedmd.com/pdfs/summary.pdf)  - Victor Eijkhout: Introduction to High-Performance Scientific Computing   - [Charles Severance, Kevin Dowd: High Performance Computing](http://cnx.org/content/col11136/1.5/)  - [OpenMP standard](https://www.openmp.org/)  - [MPI standard](https://www.mpi-forum.org/)  - [SLURM Quick Reference (Cheat Sheet)](https://slurm.schedmd.com/pdfs/summary.pdf)   <p> </p>"},{"location":"quickstart.html#slurm-accounts","title":"SLURM accounts","text":"<p>In SLURM exist accounts for billing, these are different from the login account!</p> <p>Each user has his/her own personal SLURM-account, which will have a monthly limit and at least one project account for larger calculations.</p> <p>SLURM user-accounts start with <code>user_</code> and project accounts with <code>project_</code> and course accounts with <code>course_</code>, followed by uniID/projectID/courseID. You can check which SLURM accounts you belong to, by <code>sacctmgr show associations format=account%30,user%30 | grep uniID</code> . Currently (almost) all users belong to the SLURM-account \"vaikimisi\" (default), it is possible to submit jobs under this account, especially if no <code>user_</code> or project account has been created for you yet, however, \"vaikimisi\" will be discontinued in the near future.</p> <p>When submitting a job, it is important to use the correct SLURM-account <code>--account=SLURM-ACCOUNT</code>, as this is connected to the financial source.</p> <p> </p>"},{"location":"quickstart.html#monitoring-jobs-resources","title":"Monitoring jobs &amp; resources","text":""},{"location":"quickstart.html#monitoring-a-job-on-the-node","title":"Monitoring a job on the node","text":""},{"location":"quickstart.html#status-of-a-job","title":"Status of a job","text":"<p>User can check the status his jobs (whether they are running or not, and on which node) by the command:</p> Text Only<pre><code>squeue -u $USER\n</code></pre> <p></p>"},{"location":"quickstart.html#load-of-the-node","title":"Load of the node","text":"<p>User can check the load of the node his job runs on, status and configuration of this node by command</p> Text Only<pre><code>scontrol show node &lt;nodename&gt;\n</code></pre> <p>the load should not exceed the number of hyperthreads (CPUs in SLURM notation) of the node.</p> <p></p> <p>In case of MPI parallel runs statistics of several nodes can be monitored by specifying nodes names. For example:  </p> Text Only<pre><code>scontrol show node=green[25-26]\n</code></pre> <p>Node features for node selection using <code>--constraint=</code>:</p> feature what it is A100-40 has A100 GPU with 40GB A100-80 has A100 GPU with 80GB L40 has L40 GPU with 48GB nvcc80 GPU has compute capability 8.0 (A100, L40) nvcc89 GPU has compute capability 8.9  (L40) nvcc35 GPU has compute capability 3.5 (K20Xm, A100, L40) zen2 AMD Zen CPU architecture 2nd generation (amp1) zen3 AMD Zen CPU architecture 3rd generation (amp2) zen4 AMD Zen CPU architecture 4th generation (ada*) avx512 CPU has avx512 (skylake, zen4) skylake Intel SkyLake CPU architecture (green*) sandybridge Intel SandyBridge CPU architecture (mem1tb, viz) ib InfiniBand network interface"},{"location":"quickstart.html#monitoring-with-interactive-job","title":"Monitoring with interactive job","text":"<p>It is possible to submit a second interactive job to the node where the main job is running, check with <code>squeue</code> where your job is running, then submit</p> Text Only<pre><code>srun -w &lt;nodename&gt; --pty htop\n</code></pre> <p>Note that there must be free slots on the machine, so if you cannot use <code>-n 80</code> or <code>--exclusive</code> for your main job (use <code>-n 79</code>).</p> <p></p> <p>Press <code>q</code> to exit.</p> You can also add a column that shows the CPU number of the program (for more details click here).  For Linux **F1-F10** keys should be used, for **Mac** - just click on the corresponding buttons.  ![htop-1](pictures/htop-1.png)  Will appear a new column, showing the CPU number of the program.  ![htop-2](pictures/htop-2.png)   <p></p>"},{"location":"quickstart.html#monitoring-jobs-using-gpus","title":"Monitoring jobs using GPUs","text":"<p>Log to amp or amp2. Command </p> Text Only<pre><code>echo ${SLURM_STEP_GPUS:-$SLURM_JOB_GPUS}\n</code></pre> <p>shows the GPU IDs allocated to your job.</p> <p>GPUs load can be checked by command:</p> Text Only<pre><code>nvidia-smi\n</code></pre> <p></p> <p>Press <code>control+c</code> to exit.</p> <p>Another option is to logging to amp or amp2, check which GPUs are allocated to your job, and give command:</p> Text Only<pre><code>nvtop\n</code></pre> <p></p> <p>Press <code>q</code> to exit.</p> <p>An alternative method on Linux computers, if you have X11. Logging to base/amp with <code>--X</code> key:</p> Text Only<pre><code>ssh --X UniID@base.hpc.taltech.ee\n</code></pre> <p>then submit your main interactive job </p> Text Only<pre><code>srun --x11 -n &lt;numtasks&gt; --cpus-per-task=&lt;numthreads&gt; --pty bash\n</code></pre> <p>and start an <code>xterm -e htop &amp;</code> in the session.</p> <p>In <code>sbatch</code> the option <code>--x11=batch</code> can be used, note that the ssh session to base needs to stay open!</p>"},{"location":"quickstart.html#monitoring-resource-usage","title":"Monitoring resource usage","text":"<p>Default disc quota for <code>home</code> (that is backed up weekly) is 500 GB and for <code>smbhome</code> (that is not backed up) -- 2 TB per user. For <code>smbgroup</code> there is no limits and no backup.</p> <p>The easiest way to check your current disk usage is to look at the table that appears when you log in to HPC. </p> <p></p> <p>You can also monitor your resource usage by <code>taltech-lsquota.bash</code> script and <code>sreport</code> command. </p> <p>Current disk usage:</p> Text Only<pre><code>taltech-lsquota.bash\n</code></pre> <p></p> <p>CPU usage during last day:</p> Text Only<pre><code>sreport -t Hours cluster UserUtilizationByAccount Users=$USER\n</code></pre> <p></p> <p>CPU usage in specific period (e.g. since beginning of this year):</p> Text Only<pre><code>sreport -t Hours cluster UserUtilizationByAccount Users=$USER start=2024-01-01T00:00:00 end=2024-12-31T23:59:59\n</code></pre> <p>Where <code>start=</code> and <code>end=</code> can be changed depending on the desired period of time. </p> <p></p> <p>For convenience, a tool <code>taltech-history</code> was created, by default it shows the jobs of the current month, use <code>taltech-history -a</code> to get a summary of the useh hours and costs of the current month.</p> <p> </p>"},{"location":"quickstart.html#copying-data-tofrom-the-clusters","title":"Copying data to/from the clusters","text":"<p>Since HPC disk quota is limited, it is recommended to have your own copy of important calculations and results. Data from HPC can be transferred by several commands: <code>scp</code>, <code>sftp</code>, <code>sshfs</code> or <code>rsync</code>.</p> <ol> <li> <p><code>scp</code> is available on all Linux systems, Mac and Windows10 PowerShell. There are also GUI versions available for different OS (like PuTTY).</p> <p>Copying to the cluster with <code>scp</code>:</p> <p>scp local_path_from_where_copy/file uni-id@base.hpc.taltech.ee:path_where_to_save</p> <p></p> <p>Copying from the cluster with <code>scp</code>:</p> <p>scp uni-id@base.hpc.taltech.ee:path_from_where_copy/file local_path_where_to_save </p> <p></p> <p>Path to the file at HPC can be checked by  <code>pwd</code> command.</p> </li> <li> <p><code>sftp</code> is the secure version of the <code>ftp</code> protocol vailable on Linux, Mac and Windows10 PowerShell. This command starts a session, in which files can be transmitted in both directions using the <code>get</code> and <code>put</code> commands. File transfer can be done in \"binary\" or \"ascii\" mode, conversion of line-endings (see below) is automatic in \"ascii\" mode. There are also GUI versions available for different OS (FileZilla, gFTP and WinSCP (Windows))</p> <p>sftp uni-id@base.hpc.taltech.ee</p> <p></p> </li> <li> <p><code>sshfs</code> can be used to temporarily mount remote filesystems for data transfer or analysis. Available in Linux. The data is tunneled through an ssh-connection. Be sware that this is usually not performant and can creates high load on the login node due to ssh-encryption.</p> Text Only<pre><code>sshfs uni-id@base.hpc.taltech.ee:remote_dir/ /path_to_local_mount_point/\n</code></pre> </li> <li> <p><code>rsync</code> can update files if previous versions exist without having to transfer the whole file. However, its use is recommended for the advanced user only since one has to be careful with the syntax.</p> </li> </ol>"},{"location":"quickstart.html#smbcifs-exported-filesystems","title":"SMB/CIFS exported filesystems","text":"<p>One of the simple and convenient ways to control and process data based on HPC is mounting. Mounting means that user attaches his directory placed at HPC to a directory on his computer and can process files as if they were on this computer. These can be accessed from within university or from EduVPN.</p> <p>Each user automatically has a directory within <code>smbhome</code>. It does not match with <code>$HOME</code> directory, so calculations should be initially done at <code>smbhome</code> directory to prevent copying or files needed should be copied from <code>home</code> directory to the <code>smbhome</code> directory by commands:</p> Text Only<pre><code>pwd ### look path to the file \ncp path_to_your_file/your_file /gpfs/mariana/smbhome/$USER/ ### copying\n</code></pre> <p>To get a directory for group access, please contact us (a group and a directory need to be created).</p> <p>The HPC center exports two filesystems as Windows network shares:</p> local path on cluster Linux network URL Windows network URL /gpfs/mariana/smbhome/$USER smb://smb.hpc.taltech.ee/smbhome \\\\smb.hpc.taltech.ee\\smbhome /gpfs/mariana/smbgroup smb://smb.hpc.taltech.ee/smbgroup \\\\smb.hpc.taltech.ee\\smbgroup /gpfs/mariana/home/$USER not exported not exported <p>This is the quick-access guide, for more details, see here</p>"},{"location":"quickstart.html#windows-access","title":"Windows access","text":"<p>The shares can be found using the Explorer \"Map Network Drive\".</p> Text Only<pre><code>server &gt;&gt;&gt; \\\\smb.hpc.taltech.ee\\smbhome\nusername &gt;&gt;&gt; INTRA\\&lt;uni-id&gt;\n</code></pre> <p>From Powershell: </p> Text Only<pre><code> net use \\\\smb.hpc.taltech.ee\\smbhome /user:INTRA\\uni-id\n get-smbconnection\n</code></pre>"},{"location":"quickstart.html#linux-access","title":"Linux access","text":"<p>On Linux with GUI Desktop, the shares can be accessed with the nautilus browser.</p> <p>From commandline, the shares can be mounted as follows:</p> Text Only<pre><code>dbus-run-session bash\ngio mount smb://smb.hpc.taltech.ee/smbhome/\n</code></pre> <p>you will be asked for \"User\" (which is your UniID), \"Domain\" (which is \"INTRA\"), and your password.</p> <p>To disconnect from the share, unmount with</p> Text Only<pre><code>gio mount -u smb://smb.hpc.taltech.ee/smbhome/\n</code></pre>"},{"location":"quickstart.html#special-considerations-for-copying-windows-linux","title":"Special considerations for copying Windows - Linux","text":"<p>Microsoft Windows is using a different line ending in text files (ASCII/UTF8 files) than Linux/Unix/Mac: CRLF vs. LF When copying files between Windows-Linux, this needs to be taken into account. The FTP (File Transfer Protocol) has ASCII and BINARY modes, in ASCII-mode the line-end conversion is automatic.</p> <p>There are tools for conversion of the line-ending, in case the file was copied without line conversion: <code>dos2unix</code>, <code>unix2dos</code>, <code>todos</code>, <code>fromdos</code>, the stream-editor <code>sed</code> can also be used.</p> <p> </p>"},{"location":"quickstart.html#backup","title":"Backup","text":"<p>There are 2 major directories where users can store data:</p> <ul> <li><code>/gpfs/mariana/home/</code> default home directory which is limited to 500GB and is backed up, excluding specific directories: <code>[*/envs/, */.cache/, */pkgs/]</code>.</li> <li><code>/gpfs/mariana/smbhome/</code> has a limit of 2TB and is not backed up.</li> </ul> <p>The home directory is meant for critical data like configurations and scripts, whereas smbhome is meant for data.</p> <p>The backup will run weekly. If the home directory is larger than 500GB [usage is displayed upon login to the cluster] it will not be backed up.</p> <p>If your home directory is larger than 500G please move the data to smbhome.</p> <p>At HPC are installed programs with varying licence agreement. To use some licensed programs (for example, Gaussian), the user must be added to the appropriate group. For this contact us email (hpcsupport@taltech.ee) or Taltech portal. More about available programs and licenses can be found at software page.</p> <p></p>"},{"location":"samba.html","title":"SAMBA Network Drive","text":"<p>not changed to rocky yet</p>"},{"location":"samba.html#accessing-smbcifs-network-shares","title":"Accessing SMB/CIFS network shares","text":"<p>The HPC center exports two filesystems as Windows network shares:</p> local path on cluster Linux network URL Windows network URL /gpfs/mariana/smbhome/$USER smb://smb.hpc.taltech.ee/smbhome \\\\smb.hpc.taltech.ee\\smbhome /gpfs/mariana/smbgroup smb://smb.hpc.taltech.ee/smbgroup \\\\smb.hpc.taltech.ee\\smbgroup /gpfs/mariana/home/$USER not exported not exported <p>These can be accessed from within university or from EduVPN.</p> <p>Each user automatically has a directory within smbhome. To get a directory for group access, please contact us (a group and a directory need to be created).</p>"},{"location":"samba.html#windows-access","title":"Windows access","text":"<p>From Windows, the shares can be found using the Explorer \"Map Network Drive\".</p> <p>GUI:</p> <p>right click on myPC add network location or Map network</p> Text Only<pre><code>server &gt;&gt;&gt; \\\\smb.hpc.taltech.ee\\smbhome\nusername &gt;&gt;&gt; INTRA\\&lt;uni-id&gt;\n</code></pre> <p> </p> <p>Powershell:</p> <p>run <code>net use \\\\smb.hpc.taltech.ee\\smbhome /user:INTRA\\&lt;uni-id&gt;</code> check success with <code>get-smbconnection</code></p>"},{"location":"samba.html#linux-access","title":"Linux access","text":"<p>On Linux with GUI Desktop, the shares can be accessed with nautilus browser.</p> <p>From Linux commandline, the shares can be mounted as follows:</p> Text Only<pre><code>dbus-run-session bash\ngio mount smb://smb.hpc.taltech.ee/smbhome/\n</code></pre> <p>or</p> Text Only<pre><code>dbus-run-session bash\ngio mount smb://smb.hpc.taltech.ee/smbgroup/\n</code></pre> <p>you will be asked for \"User\" (which is your UniID), \"Domain\" (which is \"INTRA\"), and your password.</p> <p>To disconnect from the share, unmount with</p> Text Only<pre><code>gio mount -u smb://smb.hpc.taltech.ee/smbhome/\ngio mount -u smb://smb.hpc.taltech.ee/smbgroup/\n</code></pre> <p>If you get \"Error mounting location: Location is not mountable\", then you are not in the correct network (e.g. VPN is not running), or you don't have a dbus session.</p> <p>On Debian, the following packages need to be installed: <code>gvfs gvfs-common gvfs-daemons gvfs-fuse gvfs-libs libsmbclient gvfs-backends libglib2.0-bin</code></p>"},{"location":"singularity.html","title":"Containers (Singularity &amp; Docker)","text":"<p>Containers are a popular way of creating a reproducible software environment. Container solutions are Docker and Singularity/Apptainer, we support singularity.</p> <p>The Singularity user guides are a great resource for learning what you can do with singularity</p> <p></p>"},{"location":"singularity.html#running-a-container","title":"Running a container","text":"<p>native installation from Rocky 8 EPEL of <code>singularity-ce version 4.1.5-1.el8</code>, no modules to load.</p>"},{"location":"singularity.html#on-all-nodes-using-cpu-only","title":"On all nodes using CPU only","text":"<p>pull the docker image you want, here ubuntu:18.04</p> Text Only<pre><code>singularity pull docker://ubuntu:18.04\n</code></pre> <p>write an sbatch file (here called <code>ubuntu.slurm</code>):</p> Text Only<pre><code>#!/bin/bash\n#SBATCH -t 0-00:30\n#SBATCH -N 1\n#SBATCH -c 1\n#SBATCH --cpus-per-task=2   #singularity can use multiple cores\n#SBATCH --mem-per-cpu=4000\nsingularity exec docker://ubuntu:18.04 cat /etc/issue\n</code></pre> <p>submit to the queueing system with</p> Text Only<pre><code>sbatch ubuntu.slurm\n</code></pre> <p>and when the resources become available, your job will be executed.</p>"},{"location":"singularity.html#on-gpu-nodes-using-gpu","title":"On GPU nodes (using GPU)","text":"<p>When running singularity through SLURM (srun, sbatch) only GPUs reverved through SLURM are visible to singularity.</p> <p>pull the docker image you want, here ubuntu:20.04:</p> Text Only<pre><code>singularity pull docker://ubuntu:20.04\n</code></pre> <p>write an sbatch file (here called <code>ubuntu.slurm</code>):</p> Text Only<pre><code>#!/bin/bash\n#SBATCH -t 0-00:30\n#SBATCH -N 1\n#SBATCH -c 1\n#SBATCH -p gpu\n#SBATCH --gres=gpu:A100:1     #only use this if your job actually uses GPU\n#SBATCH --mem-per-cpu=4000\nsingularity exec --nv docker://ubuntu:20.04 nvidia-smi\n# or singularity exec --nv ubuntu_20.04.sif nvidia-smi\n# the --nv option to singularity passes the GPU to it\n</code></pre> <p>submit to the queueing system with</p> Text Only<pre><code>sbatch ubuntu.slurm\n</code></pre> <p>and when the resources become available, your job will be executed.</p> <p>More on singularity and GPUs, see https://sylabs.io/guides/3.9/user-guide/gpu.html.</p>"},{"location":"singularity.html#hints","title":"Hints","text":"<p>By default there is no network isolation in Singularity, so there is no need to map any port (-p in docker). If the process inside the container binds to an IP:port, it will be immediately reachable on the host. Singularity also mounts $HOME and $TMP by default so the directory you run the container from will be the working directory within the container (unless the directory is not on the same filesystem as $HOME).</p> <p>Singularity will use all cores reserved using <code>--cpus-per-task</code>, if less should be used, the singularity parameter <code>--cpus</code> can be used, similarly, if a container should use less memory, this can be restricted by the singularity parameter <code>--memory</code>. These parameters can be useful, if a single batch job starts several containers concurrently.</p>"},{"location":"singularity.html#example-interactive-pytorch-job-without-and-with-gpu","title":"Example: Interactive PyTorch job (without and with GPU)","text":"<p>Start an interactive session on amp, make the modules available and run the docker image in singularity:</p> <p>Without GPU:</p> Text Only<pre><code>srun -t 1:00:00 --pty bash\nsingularity exec docker://pytorch/pytorch python\n</code></pre> <p>With GPU:</p> Text Only<pre><code>srun -t 1:00:00 -p gpu --gres=gpu:1 --pty bash\nsingularity exec --nv docker://pytorch/pytorch python\n</code></pre> <p>inside the container python session run</p> Text Only<pre><code>import torch\ntorch.cuda.is_available()\ntorch.cuda.get_device_name()\n</code></pre> <p>You can also shorten it to a single command</p> Text Only<pre><code>srun -t 1:00:00 -p gpu --mem 32G --gres=gpu:1 singularity exec docker://pytorch/pytorch python -c \"import torch;print(torch.cuda.is_available())\"\n</code></pre> <p>which should give the same result (without the GPU name). If you remove the <code>--nv</code> flag the result changes as singularity no longer exposes the gpu.</p> <p></p>"},{"location":"singularity.html#example-interactive-tensorflow-job-without-and-with-gpu","title":"Example: Interactive TensorFlow job  (without and with GPU)","text":"<p>Start an interactive session on amp, make the modules available and run the docker image in singularity:</p> <p>Without GPU:</p> Text Only<pre><code>srun -t 1:00:00 --mem=16G --pty bash\nsingularity run docker://tensorflow/tensorflow\n</code></pre> <p>With GPU:</p> Text Only<pre><code>srun -t 1:00:00 -p gpu --gres=gpu:1 --mem=16G --pty bash\nsingularity run --nv docker://tensorflow/tensorflow:latest-gpu\n</code></pre> <p>With GPU and jupyter:</p> Text Only<pre><code>srun -t 1:00:00 -p gpu --gres=gpu:1 --mem=16G --pty bash\nsingularity run --nv docker://tensorflow/tensorflow:latest-gpu-jupyter\n</code></pre> <p>inside the container run</p> Text Only<pre><code>python\nfrom tensorflow.python.client import device_lib\nprint(device_lib.list_local_devices())\n</code></pre> <p>The following is the \"TensorFlow 2 quickstart for beginners\" from https://www.tensorflow.org/tutorials/quickstart/beginner, continue inside the python:</p> Text Only<pre><code>import tensorflow as tf\nprint(\"TensorFlow version:\", tf.__version__)\nmnist = tf.keras.datasets.mnist\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\n\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\n  tf.keras.layers.Dense(128, activation='relu'),\n  tf.keras.layers.Dropout(0.2),\n  tf.keras.layers.Dense(10)\n])\npredictions = model(x_train[:1]).numpy()\npredictions\ntf.nn.softmax(predictions).numpy()\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nloss_fn(y_train[:1], predictions).numpy()\nmodel.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=5)\nmodel.evaluate(x_test,  y_test, verbose=2)\nprobability_model = tf.keras.Sequential([\n  model,\n  tf.keras.layers.Softmax()\n])\nprobability_model(x_test[:5])\n</code></pre> <p></p>"},{"location":"singularity.html#example-job-for-opendronemap-odm","title":"Example job for OpenDroneMap (ODM)","text":"<p>OpenDroneMap needs a writable directory for the data. This directory needs to contain a subdirectory named <code>images</code>.</p> <p>Assume you keep your ODM projects in the directory <code>opendronemap</code>:</p> Text Only<pre><code>opendronemap\n|\n|-Laagna-2021\n| |\n| |-images\n|\n|-Paldiski-2015\n| |\n| |-images\n|\n|-Paldiski-2018\n| |\n| |-images\n|\n|-TalTech-2015\n| |\n| |-images\n</code></pre> <p>If you want to create a 3D model for Laagna-2021, you would run the following Singularity command:</p> Text Only<pre><code>singularity run --bind $(pwd)/opendronemap/Laagna-2021:/datasets/code docker://opendronemap/odm --project-path /datasets\n</code></pre> <p>For creating a DEM, you would need to add <code>--dsm</code> and potentially <code>-v \"$(pwd)/odm_dem:/code/odm_dem\"</code></p> <p>GPU use for singularity is enabled with the <code>--nv</code> switch, be aware that ODM uses the GPU only for the matching, which is only a small percentage of the time of the whole computation.</p> <p>The SLURM job-script looks like this:</p> Text Only<pre><code>#!/bin/bash\n#SBATCH --nodes 1\n#SBATCH --ntasks 1\n#SBATCH --cpus-per-task=10\n#SBATCH --time 01:30:00\n#SBATCH --partition gpu\n#SBATCH --gres=gpu:A100:1\n\n\nsingularity run --nv --bind $(pwd)/opendronemap/Laagna-2021:/datasets/code docker://opendronemap/odm --project-path /datasets --dsm\n</code></pre> <p></p>"},{"location":"singularity.html#obtaining-and-building-singularity-containers","title":"Obtaining and Building Singularity Containers","text":"<p>When you want to use a container with the cluster you'll need to get the image from somewhere and you cannot build containers on the cluster for security reasons (even with <code>--fakeroot</code>) so there are two ways to get your containers into the cluster.</p>"},{"location":"singularity.html#from-container-registries","title":"From Container Registries","text":"<p>Singularity can pull and convert docker images from docker container registeries (most significantly dockerhub) directly into singularity images. This is the method used in the previous examples. You can read more here: https://docs.sylabs.io/guides/3.9/user-guide/singularity_and_docker.html</p> <p>You can also use GitHub's Container Registry or TalTech's Software Science Gitlab (You'll need to sign in with an access token to pull containers from the registry, more on that here https://docs.sylabs.io/guides/3.9/user-guide/endpoint.html)</p>"},{"location":"singularity.html#building-images-locally-then-moving-to-cluster","title":"Building images locally then moving to cluster","text":"<p>Since Singularity images are single files you can transfer them quite easily with any tool used to sync data with the cluster, <code>scp</code>, <code>rsync</code> etc. You can build locally with either just the <code>singularity</code> tool or <code>singularity</code> and <code>docker</code> - Building images from singularity definition file then transferring to the cluster. - Building images with docker from dockerfiles then saving the image docker save to an archive e.g</p> Bash<pre><code>docker build -t pytorch .\ndocker save pytorch | gzip &gt; pytorch.tar.gz\n</code></pre> <p>creates a file <code>pytorch.tar.gz</code> which you can either convert to a singularity image locally with <code>singularity build docker-archive//pytorch.tar.gz</code> or you can move the archive to the cluster and build from there. Building from a docker archive is the only form of image building allowed in the cluster.</p> <p></p>"},{"location":"slurm.html","title":"Slurm","text":"<p>not changed to rocky yet</p>"},{"location":"slurm.html#examples-of-slurm-scripts","title":"Examples of slurm scripts","text":""},{"location":"slurm.html#a-single-process-job","title":"A single process job","text":"<p>The following script  launches job named bowtie2 using one thread. A directory bowtie2-results will be created where results will be written into bowtie2-%J.log output file (\"%j\" is a job allocation number).</p> Text Only<pre><code>#!/bin/bash \n#SBATCH --job-name=bowtie2      ### job name \n#SBATCH --output=bowtie2-%J.log ### output file \n#SBATCH --ntasks=1          ### number of cores\n\n## load bowtie2 environment \nmodule load bowtie2-2.1.0\n\n## creating directory for results \nmkdir bowtie2-results \ncd bowtie2-results\n\n## building bowtie2 index \nbowtie2-build $BT2_HOME/example/reference/lambda_virus.fa lambda_virus\n\n## aligning against the index, output to eg1.sam file \nbowtie2 -x lambda_virus -U $BT2_HOME/example/reads/reads_1.fq -S eg1.sam\n</code></pre> <p> </p>"},{"location":"slurm.html#an-openmp-parallel-job","title":"An OpenMP parallel job","text":"<p>The following script launches job named HelloOMP using OpenMP. For this job slurm reserves one node and 12 threads. Maximum run time is 10 minutes. </p> <p>Note: Each thread needs sufficient work to do to make up for the time spent in launching the thread. Therefore it is not useful to run small/short jobs in parallel.</p> Text Only<pre><code>#!/bin/bash\n#SBATCH --job-name=HelloOMP     ### job name           -J\n#SBATCH --time=00:10:00         ### time limit         -t\n#SBATCH --nodes=1               ### number of nodes    -N \n#SBATCH --ntasks-per-node=1     ### number of tasks (MPI processes)\n#SBATCH --cpus-per-task=12      ### number of threads per task (OMP threads)\n\n## load environment\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n\n## run job\n./hello_omp\n</code></pre> <p>NOTE: Parallel does not (necessarily) mean faster!!! Parallel execution introduces overhead (starting threads, communication)! For optimal execution time and optimal use of resources one needs to test and find the sweet spot.</p> <p> </p>"},{"location":"slurm.html#a-script-for-mpi-parallel-job-openfoam","title":"A script for MPI parallel job (OpenFOAM)","text":"<p>The following script reserves 4 CPU-cores for 10 hours, loads the OpenMPI module, the OpenFOAM variables, changes into the case directory and runs the typical commands necessary for a parallel OpenFOAM job. It also sets OpenMPI transport properties to use Ethernet TCP!</p> <p>It would be possible to request all tasks to be on the same node using the <code>-N</code> and <code>--tasks-per-node</code> options, this would be useful to make use of the very low latency shared memory communication of MPI (provided the job fits into the RAM of a single node).</p> <p>Note: Each task needs sufficient work to do to make up for the time spent with inter-process communication. Therefore it is not useful to run small/short jobs in parallel.</p> Text Only<pre><code>#!/bin/bash -l    \n#SBATCH -n 4                ### number of CPUs \n#SBATCH -t 10:00:00             ### run time \n#SBATCH -J openfoam-damBreak        ### job name\n#SBATCH --partition=green-ib        ### partition\n\n## load environment\nmodule load mpi/openmpi-x86_64\nsource /share/apps/HPC2/OpenFOAM/OpenFOAM-v1912/etc/bashrc\n\n## run program\ncd $WM_PROJECT_USER_DIR/damBreak/damBreak\nblockMesh\ndecomposePar\nsetFields\nmpirun -n $SLURM_NTASKS interFoam -parallel\nreconstructPar\n</code></pre> <p>NOTE: Parallel does not (necessarily) mean faster!!! Parallel execution introduces overhead (starting threads, communication)! For optimal execution time and optimal use of resources one needs to test and find the sweet spot.</p> <p></p> <p>The division into the areas is a combined decision taking into account \"real\" (wall clock) and \"user\" (summed time of all threads) time (from the time command). \"Wall clock\" (real) time is the time one needs to wait till the job is finished, \"Summed thread time\" (user) is the sum of the times that all individual threads needed, it should be roughly user = numtreads x real. For parallel programs, one can expect that \"user\" time of the parallel run is larger than for the sequential, due to communication overhead, if it is smaller, that probably means the individual threads could make better use of cache.</p> area why explanation when to use sweet spot minimal \"user\" time = minimal heat production, optimal use of resources regular use good range linear speedup for \"real\", with constant or slightly increasing \"user\" approaching deadline OK range slightly less than linear speedup for \"real\", and slightly increasing \"user\" pushing hard to make a deadline avoid ascending slope in the diagram for \"real\" and \"user\" one actually needs to wait longer compared to the case with fewer cores NEVER <p>Recommended in this case would be to request 8 threads <code>-n 8 --ntasks-per-node 8</code> but use <code>mpirun -n 4</code>. OpenFOAM does not seem to benefit from hyperthreading .</p> <p> </p>"},{"location":"slurm.html#an-array-parameter-sweep-job","title":"An array (parameter sweep) job","text":"Text Only<pre><code>#!/bin/bash \n#SBATCH --job-name=array-parameter-scan     ### job name\n#SBATCH --output=arrayjob           ### output file \n#SBATCH --ntasks=10             ### number of cores  \n#SBATCH --array=13-1800             ### array tasks for parameter sweep\n\n## run job\n./myarrayjob  $SLURM_ARRAY_TASK_ID\n</code></pre>"},{"location":"slurm.html#a-gpu-job","title":"A GPU job","text":"<p>The GPU scripts can be run only on amp. </p> <p>The following script reserves 1 gpu (Nvidia A100), uses gpu partition and has time limit 0f 10 minutes. </p> Text Only<pre><code>#!/bin/bash \n#SBATCH --job-name=uses-gpu     ### job name\n#SBATCH -p gpu          ### use gpu\n#SBATCH --gres=gpu:A100:1       ### specifying the GPU type\n#SBATCH -t 00:10:00         ### time limit\n\n./mygpujob\n</code></pre> <p>This script reserves 4 gpu without specifying the GPU type.</p> Text Only<pre><code>#!/bin/bash \n#SBATCH --job-name=uses-gpu     ### job name\n#SBATCH -p gpu          ### use gpu\n#SBATCH --gres=gpu:4        ### number of gpu\n#SBATCH -t 00:10:00         ### time limit\n\n./mygpujob\n</code></pre> <p> </p>"},{"location":"slurm.html#a-job-using-the-scratch-partition-sequential-or-openmp-parallel","title":"A job using the scratch partition (sequential or OpenMP parallel)","text":"<p>The following script creates a directory on the scratch partition of the node (fast local storage, that does not require network), runs the job, and copies the output files back into the permanent home-directory once the job is completed.</p> Text Only<pre><code>#!/bin/bash -l\n\n#SBATCH -N 1            ### job name\n#SBATCH -t 00:10:00         ### time limit  \n#SBATCH -J using-scratch        ### job name\n\n## creates scratch directory, copy files from working directory to scratch directory, goes to scratch directory\nmkdir /state/partition1/scratch-%x-%A\ncp -r $SLURM_SUBMIT_DIR/* /state/partition1/scratch-%x-%A/\ncd /state/partition1/scratch-%x-%A/\n\n## run job\nmyjob\n\n## copy files from scratch directory to working directory and remove scratch directory\ncp -r /state/partition1/scratch-%x-%A/* $SLURM_SUBMIT_DIR/\nrm -rf /state/partition1/scratch-%x-%A\n</code></pre> <p>Please note that the scratch is not shared between nodes, so parallel MPI jobs that span multiple nodes cannot access each other's scratch files.</p> <p></p>"},{"location":"slurm_example.html","title":"Slurm example","text":"<p>not changed to rocky yet</p>"},{"location":"slurm_example.html#examples-of-slurm-scripts-for-submitting-jobs","title":"Examples of slurm scripts for submitting jobs","text":""},{"location":"slurm_example.html#a-single-process-job","title":"A single process job","text":"<p>The following script  launches job named bowtie2 using one thread. A directory bowtie2-results will be created where results will be written into bowtie2-%J.log output file (<code>%J</code> is a job allocation number and step number in format jobid.stepid).</p> Text Only<pre><code>#!/bin/bash \n#SBATCH --job-name=bowtie2      ### job name \n#SBATCH --output=bowtie2-%J.log     ### output file \n#SBATCH --ntasks=1          ### number of threads\n\n## load bowtie2 environment \nmodule load bowtie2-2.1.0\n\n## creating directory for results \nmkdir bowtie2-results \ncd bowtie2-results\n\n## building bowtie2 index \nbowtie2-build $BT2_HOME/example/reference/lambda_virus.fa lambda_virus\n\n## aligning against the index, output to eg1.sam file \nbowtie2 -x lambda_virus -U $BT2_HOME/example/reads/reads_1.fq -S eg1.sam\n</code></pre> <p> </p>"},{"location":"slurm_example.html#an-openmp-parallel-job","title":"An OpenMP parallel job","text":"<p>The following script launches job named HelloOMP using OpenMP. For this job slurm reserves one node and 12 threads. Maximum run time is 10 minutes.</p> <p>  Even though it is <code>--cpus-per-task</code> slurm reserves threads, not CPU, since  \"cpu\" in SLURM's language is the smallest unit.  </p> <p>Note: Each thread needs sufficient work to do to make up for the time spent in launching the thread. Therefore it is not useful to run small/short jobs in parallel.</p> Text Only<pre><code>#!/bin/bash\n#SBATCH --job-name=HelloOMP     ### job name           -J\n#SBATCH --time=00:10:00             ### time limit         -t\n#SBATCH --nodes=1                   ### number of nodes    -N \n#SBATCH --ntasks-per-node=1         ### number of tasks (MPI processes)\n#SBATCH --cpus-per-task=12          ### number of threads per task (OMP threads)\n\n## load environment    \nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n\n## run job\n./hello_omp\n</code></pre> <p>NOTE: Parallel does not (necessarily) mean faster!!! Parallel execution introduces overhead (starting threads, communication)! For optimal execution time and optimal use of resources one needs to test and find the sweet spot.</p> <p> </p>"},{"location":"slurm_example.html#a-script-for-mpi-parallel-job-openfoam","title":"A script for MPI parallel job (OpenFOAM)","text":"<p>The following script reserves 4 CPU-cores for 10 hours    (since <code>mpirun</code> uses cores by default),   loads the OpenMPI module, the OpenFOAM variables, changes into the case directory and runs the typical commands necessary for a parallel OpenFOAM job. It also sets OpenMPI transport properties to use Ethernet TCP!</p> <p>It would be possible to request all tasks to be on the same node using the <code>-N</code> and <code>--tasks-per-node</code> options, this would be useful to make use of the very low latency shared memory communication of MPI (provided the job fits into the RAM of a single node).</p> <p>  Flag <code>-l</code> in #!/bin/bash row means that settings in /home/user/.bash_profile will be executed.  </p> <p>Note: Each task needs sufficient work to do to make up for the time spent with inter-process communication. Therefore it is not useful to run small/short jobs in parallel. </p> Text Only<pre><code>#!/bin/bash -l    \n#SBATCH -n 4                ### number of CPUs \n#SBATCH -t 10:00:00             ### run time   \n#SBATCH -J openfoam-damBreak        ### job name\n#SBATCH --partition=infiniband      ### partition\n\n## load environment    \nmodule load rocks-openmpi\nsource /share/apps/OpenFOAM/OpenFOAM-v1912/etc/bashrc\n\n## run program\ncd $WM_PROJECT_USER_DIR/damBreak/damBreak\nblockMesh\ndecomposePar\nsetFields\nmpirun -n $SLURM_NTASKS interFoam -parallel\nreconstructPar\n</code></pre> <p>NOTE: Parallel does not (necessarily) mean faster!!! Parallel execution introduces overhead (starting threads, communication)! For optimal execution time and optimal use of resources one needs to test and find the sweet spot.</p> <p></p> <p>The division into the areas is a combined decision taking into account \"real\" (wall clock) and \"user\" (summed time of all threads) time (from the time command). \"Wall clock\" (real) time is the time one needs to wait till the job is finished, \"Summed thread time\" (user) is the sum of the times that all individual threads needed, it should be roughly user = numtreads x real. For parallel programs, one can expect that \"user\" time of the parallel run is larger than for the sequential, due to communication overhead, if it is smaller, that probably means the individual threads could make better use of cache.</p> area why explanation when to use sweet spot minimal \"user\" time = minimal heat production, optimal use of resources regular use good range linear speedup for \"real\", with constant or slightly increasing \"user\" approaching deadline OK range slightly less than linear speedup for \"real\", and slightly increasing \"user\" pushing hard to make a deadline avoid ascending slope in the diagram for \"real\" and \"user\" one actually needs to wait longer compared to the case with fewer cores NEVER <p>Recommended in this case would be to request 8 threads <code>-n 8 --ntasks-per-node 8</code> but use <code>mpirun -n 4</code>. OpenFOAM does not seem to benefit from hyperthreading .</p> <p> </p>"},{"location":"slurm_example.html#an-array-parameter-sweep-job","title":"An array (parameter sweep) job","text":"<p> Tis script reserves 10 threads and run array of jobs in range of 13-1800.  The <code>$SLURM_ARRAY_TASK_ID</code> variable calls the input files in the given range in turn and data is written in output files arrayjob, which also contain job allocation ID and  job array index number (<code>-%A</code> and <code>-%a</code>, respectively).   </p> Text Only<pre><code>#!/bin/bash \n#SBATCH --job-name=array-parameter-scan ### job name  \n#SBATCH --output=arrayjob-%A-%a         ### output file \n#SBATCH --ntasks=10                     ### number of threads  \n#SBATCH --array=13-1800                     ### Array tasks for parameter sweep\n\n## run job\n./myarrayjob  $SLURM_ARRAY_TASK_ID\n</code></pre> <p> </p>"},{"location":"slurm_example.html#a-gpu-job","title":"A GPU job","text":"<p>The GPU scripts can be run only on amp. </p> <p>The following script reserves 1 gpu (Nvidia A100), uses gpu partition and has time limit 0f 10 minutes. </p> Text Only<pre><code>#!/bin/bash \n#SBATCH --job-name=uses-gpu     ### job name\n#SBATCH -p gpu          ### use gpu\n#SBATCH --gres=gpu:A100:1       ### specifying the GPU type\n#SBATCH -t 00:10:00         ### time limit\n\n## run job    \n./mygpujob\n</code></pre> <p>This script reserves 4 gpu without specifying the GPU type.</p> Text Only<pre><code>#!/bin/bash \n#SBATCH --job-name=uses-gpu     ### job name\n#SBATCH -p gpu          ### use gpu\n#SBATCH --gres=gpu:4        ### number of gpu\n#SBATCH -t 00:10:00         ### time limit\n\n## run job    \n./mygpujob\n</code></pre> <p> </p>"},{"location":"slurm_example.html#a-job-using-the-scratch-partition-sequential-or-openmp-parallel","title":"A job using the scratch partition (sequential or OpenMP parallel)","text":"<p>  The following script creates a directory named scratch-%x-%j (where -%x is a job name and %j is a jobid of the running job). This scratch directory is done on the scratch partition of the node to provide fast local storage, that does not require network. After, slurm script runs the job, and copies the output files back into the permanent home-directory once the job is completed.  </p> Text Only<pre><code>#!/bin/bash -l\n\n#SBATCH -N 1            ### number of nodes\n#SBATCH -t 00:10:00         ### time limit  \n#SBATCH -J using-scratch        ### job name\n\n## creates scratch scratch directory, copy files from working directory to scratch directory, goes to scratch directory\nmkdir /state/partition1/scratch-%x-%j\ncp -r $HOME/were/input/is/* /state/partition1/scratch-%x-%j/\ncd /state/partition1/scratch-%x-%j/\n\n## run job\nmyjob\n\n## copy files from scratch directory to working directory and remove scratch directory\ncp -r /state/partition1/scratch-%x-%j/* $HOME/were/input/is\nrm -rf /state/partition1/scratch-%x-%j\n</code></pre> <p>Please note that the scratch is not shared between nodes, so parallel MPI jobs that span multiple nodes cannot access each other's scratch files.</p> <p></p>"},{"location":"slurm_statistics.html","title":"Slurm statistics","text":"<p>not changed to rocky yet</p>"},{"location":"slurm_statistics.html#slurm-finished-job-statistics","title":"SLURM finished job statistics","text":"<p>A statistics file is created after finishing a job which shows hardware and job related info to the user. Statistics files are separated by the ID of the job and the filename is generated as such using the SLURM environment variable allocated to the job as the ID: <code>slurm-$SLURM_JOB_ID.stat</code>. The <code>.stat</code> file created in the user\u2019s current working directory where the job is run. </p>  The statistics file contains:  -   JobID -   The state of the job upon completion -   Nodelist -   Cores per node -   CPU Utilized (This figure represents the time that the CPUs were used.) -   CPU Efficiency (Represents the percentage that the CPUs were used versus the overall runtime of the job.) -   Job Wall-clock time (The duration of the job.) -   Memory Utilized (Total amount of memory utilized in kilobytes) -   Memory Efficiency (Percentage of total memory allocated) -   MaxDiskWrite (Displays disk usage) -   Command (Displays the command that was run for this job, makes it easier for users to identify where an error might have occurred). This info may be used to better allocate resources depending on the job or modify the job scripts so that they use the allocated resources more efficiently.  <p>Disabling this feature: Create the file <code>.no_slurm_stat</code> by running the command <code>touch ~/.no_slurm_stat</code>.</p> <p>To re-enable this feature simply remove the <code>.no_slurm_stat</code> by running the command <code>rm ~/.no_slurm_stat</code>.</p>"},{"location":"software.html","title":"Software packages","text":"<p>  Software on our systems is installed in the following ways:  </p> <ol> <li>as packages from the Linux distribution (free open-source software when available and recent enough) -- no modules needed</li> <li> <p>through the SPACK package manager (free open-source software when available in SPACK) - load the <code>*-spack</code> modules by command:</p> Text Only<pre><code>module load rocky8-spack\n</code></pre> </li> <li> <p>manually (mostly non-free software (not GPL/BSD license) - load the <code>*/all</code> modules by command:</p> Text Only<pre><code>module load rocky8/all\n</code></pre> </li> </ol> <p>  Here is a list of important software for special purpose: </p> <ul> <li>CAD &amp; Meshing software  ---  FreeCAD, Salome, BRL-CAD, Gmsh and netgen; see CAD-Mesh</li> <li>Finite element software for multiphysical problems ---  ElmerFEM, CalculiX, Abaqus</li> <li>Computational Fluid Dynamics -- OpenFOAM, SU2</li> <li>Conformational search  ---  xtb-CREST </li> <li>General purpose computational chemistry, biology and physics software packages ---  Gaussian, ORCA, NWChem, xTB, CP2K</li> <li>Wavefunction analysis ---  Multiwfn</li> <li>Visualization software for computational chemistry, biology and physics ---  Molden, Avogadro, JMol, VMD, RasMol</li> <li>Interactive and non-interactive Jupyter notebooks for Julia, Python, Octave</li> <li>Matlab-compatible computation environment ---  Octave</li> <li>Data analysis ---  R, Matlab, Octave, Julia, awk, Python, GNUplot </li> <li>Visualization software ---  MayaVi2, ParaView, VisIt, COVISE, OpenDX, GNUplot </li> </ul> <p> A more detailed description of available softwares, as well as a division by area of use, is given below. <p>If software you want to use is missing in the list above, it means that it is not installed, but can be installed by your request to hpcsupport@taltech.ee or create a ticket in Helpdesk Portal. In the case of licensed software, the user must provide the license himself and the corresponding program will be installed.  </p>"},{"location":"software.html#engineering","title":"Engineering","text":""},{"location":"software.html#cad-mesh-tools","title":"CAD &amp; Mesh-Tools","text":"<p>Computer-aided design (CAD) is software for building models in a virtual space, that allows to visualize various properties of an object, such as height, width, distance, material, etc. This category contains software that is essential for the pre-processing of many simulations: CAD and mesh generation. More about CAD and meshing options on our HPC can be found here.</p>"},{"location":"software.html#freecad","title":"FreeCAD","text":"<p>FreeCAD is a CAD software, which uses Gmsh or Netgen for meshing. It can also serve as a frontend for CalculiX and ElmerFEM, thus providing similar functionality as SolidWorks. More about FreeCAD on our HPC can be found here.</p>"},{"location":"software.html#salome","title":"Salome","text":"<p>Salome is a CAD program with interfaces to meshing software. It can be used by a GUI or python scripts. More about Salome on our HPC can be found here.</p>"},{"location":"software.html#brl-cad","title":"BRL-CAD","text":"<p>BRL-CAD is a CAD software that has been in development since 1979 and is open-source since 2004. It is based on CSG modeling. BRL-CAD does not provide volume meshing, however, the CSG geometry can be exported to BREP (boundary representation, like STL, OBJ, STEP, IGES, PLY), the g- tols are for this, while the -g tools are for importing. More about BRL-CAD on our HPC can be found here.</p>"},{"location":"software.html#blender","title":"Blender","text":"<p>[Blender]{https://blender.org} is a 3D modeling software developed for computer animation (movie production).</p>"},{"location":"software.html#gmsh","title":"Gmsh","text":"<p>Gmsh is an open source 3D finite element mesh generator with a built-in CAD engine and post-processor. More about Gmsh on our HPC can be found here. </p>"},{"location":"software.html#netgen","title":"Netgen","text":"<p>Netgen is a part of the NGsolve suite. Netgen is a  automatic 3d tetrahedral mesh generator containing modules for mesh optimization and hierarchical mesh refinement. More about Netgen on our HPC can be found here.</p>"},{"location":"software.html#finite-element-analysis-fea","title":"Finite Element Analysis (FEA)","text":"<p>The Finite Element Method (FEM) is an general numerical method for solving partial differential equations in two or three space variables perfommed by dividing a large system into smaller parts (finite elements). The method is used for numerically solving differential equations in engineering and mathematical modeling.</p> <p>See also under computational-fluid-dynamics-CFD.</p>"},{"location":"software.html#elmerfem","title":"ElmerFEM","text":"<p>Elmer is a multi-physics simulation software developed by CSC. It can perform coupled mechanical, thermal, fluid, electro-magnetic simulations and can be extended by own equations. Elmer manuals and tutorials can be found here and for more details and example job scripts go here.</p>"},{"location":"software.html#calculix","title":"CalculiX","text":"<p>CalculiX is a finite-element analysis application. The two programs that form CalculiX are <code>cgx</code> and <code>ccx</code>, where <code>cgx</code> is a graphical frontend (pre- and post-processing) and <code>ccx</code> is the solver doing the actual numerics. CalculiX can be used for grid data generation or mech data generation. It can be applied in such areas as mechanical analysis, heat transfer, electromagnetic calculations, computational fluid dynamics, etc. For more detais see overview of the finite element capabilities of CalculiX Version 2.18.  Solver makes use of the Abaqus input format.</p>"},{"location":"software.html#freefem","title":"FreeFEM","text":"<p>FreeFEM is a software focused on solving partial differential equations using the finite element method. Can be used for linear, non-linear elasticity, thermal diffusion/convection/radiation, magnetostatics, electrostatics, CFD, fluid structure interaction; continuous and discontinuous Galerkin method.  Allows to implement own physics modules using the FreeFEM language.</p>"},{"location":"software.html#dealii","title":"deal.II","text":"<p>deal.II - an open source finite element library</p>"},{"location":"software.html#mfem","title":"MFEM","text":"<p>MFEM is a free, lightweight, scalable C++ library for finite element methods.</p>"},{"location":"software.html#abaqus","title":"Abaqus","text":"<p>Abaqus is a commercial software suite for finite element analysis and computer-aided engineering. The Abaqus products use Python for scripting and customization. User modules can be written in Fortran or C/C++, our installation is configured to use gcc-10.3.0. Abaqus versions 2018 and 2021 are installed. The number of concurrent processes is limited and managed by flexlm.</p> Text Only<pre><code>module load rocky8\nmodule load abaqus\n</code></pre> <p>To use Abaqus' PlatformMPI, SLURM's Global Task ID needs to be cleared</p> Text Only<pre><code>unset SLURM_GTIDS\n</code></pre>"},{"location":"software.html#comsol","title":"Comsol","text":"<p>Commercial multi-physics finite element simulation software. License belongs to a research group.</p>"},{"location":"software.html#lsdyna","title":"LSDyna","text":"<p>License belongs to a research group.</p> Text Only<pre><code>module load rocky8\nmodule load LSDyna/SMP-13.0.0-D\n</code></pre>"},{"location":"software.html#star-ccm","title":"Star-CCM+","text":"<p>Commercial software. System wide installation, bring your own license.  Star-CCM+ can be used with PowerOnDemand (PoD) keys. Power on demand (PoD) licensing for STAR-CCM+ is essentially cloud licensing. To use PoD licensing, a PoD key must be copied from the Star-CCM+ support center and put into the STAR-CCM+ interface.</p>  More information about licenses:  - Simcenter STAR-CCM+ demand - How faculty members in academic institutions can get access to Simcenter STAR-CCM+ - Guide for students to run Simcenter STAR-CCM+ <p></p> Text Only<pre><code>module load rocky8\nmodule load star-ccm+/18.04.009-R8\nstarccm+\n</code></pre>"},{"location":"software.html#computational-fluid-dynamics-cfd","title":"Computational Fluid Dynamics (CFD)","text":"<p>See also under FEA.</p>"},{"location":"software.html#openfoam","title":"OpenFOAM","text":"<p>OpenFOAM is an open source software for computational fluid dynamics (CFD). OpenFOAM has a wide range of  tools for modelling  complex fluid flows and can be used for solving such problems as chemical reactions, turbulence and heat transfer, to acoustics, solid mechanics and electromagnetics. More about OpenFOAM on our HPC can be found here.</p>"},{"location":"software.html#su2","title":"SU2","text":"<p>SU2 is a computational analysis and design package that has been developed to solve multiphysics analysis and optimization tasks using unstructured mesh topologies. SU2 is intalled through SPACK.</p>"},{"location":"software.html#water-wave-modelling","title":"Water Wave Modelling","text":""},{"location":"software.html#wam","title":"WAM","text":"<p>WAM is a third generation wave model, developed and maintained by GKSS. It describes the evolution of the wave spectrum by solving the wave energy transfer equation. WAM predicts directional spectra and wave properties (such as wave height, direction and frequency, swell height and mean direction), wind stress fields etc. WAM can be coupled to a range of other models (NEMO, RegCM, SEAOM, etc.). More info how to use it on HPC see here.</p>"},{"location":"software.html#swan","title":"Swan","text":"<p>SWAN is a third generation wave model for obtaining realistic estimates of wave parameters in coastal areas, lakes and estuaries from given wind, bottom and current conditions, developed and maintained by TU Delft. The model is based on the wave action balance equation with sources and sinks. SWAN allows to use two types of grids (structured and unstructured) and nesting approach. More info how to use it on HPC see here.</p> <p></p>"},{"location":"software.html#chemistry-biology-and-physics","title":"Chemistry, biology and physics","text":""},{"location":"software.html#conformational-search-sampling","title":"Conformational search &amp; sampling","text":""},{"location":"software.html#xtb-crest","title":"xtb-CREST","text":"<p>Conformer\u2013Rotamer Ensemble Sampling Tool (xtb-CREST) is designed as conformer sampling program by Grimme's group. CREST uses meta-dynamics, regular MD simulations and Genetic Z-matrix crossing (GC) algorithms with molecular mechanics or semiempirical methods (GFNn-xTB). Conformational search can be done in gas or solvent (using several continuum models). More about xtb-CREST on our HPC can be found here.</p>"},{"location":"software.html#general-purpose-computational-chemistry-biology-and-physics","title":"General purpose computational chemistry, biology and physics","text":""},{"location":"software.html#gaussian","title":"Gaussian","text":"<p>Gaussian is a general purpose package for calculation of electronic structures. It can calculate properties of molecules (structures, energies, spectroscopic and thermochemical properties, atomic charges, electron affinities and ionization potentials, electrostatic potentials and electron densities etc.) and reactions properties (such as reaction pathways, IRC)sing different methods (such as Molecular mechanics,  Semi-empirical methods,  Hartree-Fock,  Density functional, M\u00f8ller-Plesset perturbation theory, coupled cluster). More about Gaussian on HPC can be found here.</p>"},{"location":"software.html#orca","title":"ORCA","text":"<p>ORCA is a multi-purpose quantum-chemical software package developed in the research group of Frank Neese. ORCA includes a wide variety of methods (semi-empirical, density functional theory, many-body perturbation, coupled cluster, multireference, nudged elastic band (NEB) methods). In ORCA, molecules' and  spectroscopic properties calculations are available, and environmental (MD (including ab initio), QM/MM, Crystal-QMMM) as well as relativistic effects can be taken into account. ORCA is parallelized, and uses the resolution of the identity (RI) approximation and domain based local pair natural orbital (DLPNO) methods, which significantly speed calculations. More about ORCA on HPC can be found here</p>"},{"location":"software.html#xtb","title":"xtb","text":"<p>Extended tight binding - xTB program developed in the research group of Stefan Grimme for solutions of common chemical problems. The workhorses of xTB are the GFN methods, both semi-empirical and force-field. The program contains several  implicit solvent  models: GBSA, ALPB, ddCOSMO, and CPCM-X. xTB functionality covers single-point energy calculations, geometry optimization, frequency calculations, reaction path methods. Also allows to perform molecular dynamics, meta-dynamics, and ONIOM calculations. More about xTB on HPC can be found here</p>"},{"location":"software.html#nwchem","title":"NWChem","text":"<p>The North West computational chemistry (NWChem) is an ab initio computational chemistry software package which includes quantum chemical ( HF, DFT, MP2, MCSCF,  and CC, including the tensor contraction engine (TCE)) and molecular dynamics (using either force fields (AMBER or CHARMM) or DFT) functionality. In NWChem, ab initio methods can be coupled with the classical MD to perform mixed quantum-mechanics and molecular-mechanics simulations (QM/MM). Various molecular response properties, solvent models, nudged elastic band (NEB) method, relativistic and resolution of the identity (RI) approaches are also available. </p> <p>NWChem was developed to enable large scale calculations by  using many CPUs and has  parallel scalability and performance. Additionally,  python programs may be embedded into the NWChem input and used to control the execution of NWChem. </p>"},{"location":"software.html#wavefunction-analysis","title":"Wavefunction analysis","text":""},{"location":"software.html#multiwfn","title":"Multiwfn","text":"<p>Multiwfn it is an interactive program that performs almost all important wavefunction analyzes. In addition, Multiwfn is able to display plots of the predicted spectra. More about Multiwfn on HPC can be found here.</p>"},{"location":"software.html#visualization-software-for-computational-chemistry-biology-and-physics","title":"Visualization software for computational chemistry, biology and physics","text":"<ul> <li>Molden </li> <li>Avogadro </li> <li>JMol </li> <li>VMD</li> <li>RasMol</li> </ul>"},{"location":"software.html#data-analysis","title":"Data analysis","text":""},{"location":"software.html#gnu-r","title":"GNU R","text":"<p>GNU R, often called \"GNU S\" is an open-source implementation of the S statistics language. R offers many build-in features for data analysis, has a large collection of well maintained packages in CRAN (the Comprehensive R Archive Network) and most importantly produces high-quality graphics. While the plots may not look fancy at first sight, they are well layed out with font sizes and they are vector graphics. Another feature is that R integrates well with LaTeX2e documents using Sweave (comes with R) or knitr. That makes it possible to write the data analysis using R code within LaTeX2e documents and have R create figures and tables automatically.</p> <p>CRAN Packages can be installed by the users themselves from inside R </p> Text Only<pre><code>install.packages(\"packagename\",lib=paste(Sys.getenv(\"HOME\"),\"/.R/library\",sep=\"\"),repos=\"http://cran.r-project.org\")\n</code></pre> <p>The package will be placed inside the user's $HOME directory (installation into system directories will not be allowed).</p>"},{"location":"software.html#julia","title":"Julia","text":"<p>Julia is an easy to learn and high-performance interactive language. Julia is as easy (or easier) to learn as Python, but with the speed of C or Fortran for numerics.</p>"},{"location":"software.html#octavematlab","title":"Octave/Matlab","text":"<p>Octave is software featuring a high-level programming language, intended for prototyping numerical computations. Octave solves linear and nonlinear problems, and for performing other numerical experiments using a language that is mostly compatible with MATLAB. It may also be used as a batch-oriented language. More information about Matlab and Octave on HPC can be found here.</p>"},{"location":"software.html#gnuplot","title":"GNUplot","text":"<p>GNUplot is a very capable and portable command-line driven graphing utility for Linux and other operating systems.</p>"},{"location":"software.html#python","title":"Python","text":"<p>Different versions are available as spack modules.  Packages for Python can be installed by the users themselves using pip (python2) or pip3 (python3)</p> Text Only<pre><code>pip install --user packagename\n</code></pre> <p>or</p> Text Only<pre><code>pip3 install --user packagename\n</code></pre> <p>the option <code>--user</code> will install the package into the user's $HOME directory (installation into system directories will not be allowed).</p>"},{"location":"software.html#jupyterlab","title":"JupyterLab","text":"<p>JupyterLab notebook is an open-source web application that allows creation and sharing documents containing live code, equations, visualizations, and  text. Jupyter notebooks allow data transformation, numerical simulation, statistical modeling, data visualization, machine learning, etc. using Julia, Python and Octave. More about Jupyter on our HPC is here.</p> <p> </p>"},{"location":"software.html#visualization-software","title":"Visualization software","text":"<ul> <li>ParaView (all nodes spack module paraview)</li> <li>VisIt (all nodes spack module visit)</li> <li>COVISE (viz: run <code>/usr/local/covise/bin/covise</code>)</li> <li>MayaVi (all nodes: spack module py-mayavi)</li> <li>GNUplot  (all nodes spack module gnuplot) </li> <li>OpenDX (currently not available will come soon)</li> <li>Software for computational chemistry:<ul> <li>Molden </li> <li>Avogadro </li> <li>JMol </li> <li>VMD(all nodes: spack module vmd)</li> <li>RasMol</li> </ul> </li> <li>Software for movie animation<ul> <li>Blender (all nodes module blender) </li> </ul> </li> </ul> <p>The recommended way is now to use a desktop session in OnDemand, see also the visualization page on how to start these and on GPU acceleration. </p> <p> </p>"},{"location":"spack.html","title":"SPACK","text":"<p>not changed to rocky yet</p>"},{"location":"spack.html#spack","title":"SPACK","text":"<p>under development, docs AND module positions/versions may change without notice</p> <p>SPACK is a package manager to install software packages. An advantage is to be able to relatively easily install consistent dependencies and multiple versions of a software. The following link contains a list of software that should be easy to install: SPACK package list</p>"},{"location":"spack.html#modules","title":"Modules","text":"<p>SPACK makes use of the module system, to enable SPACK build modules use:</p> <p>For gray and all nodes (optimized for Xeon E5-2630L Haswell CPUs will also run on green nodes, but will not use all hardware features):</p> Text Only<pre><code>module load gray-spack\n</code></pre> <p>Optimized for green nodes (optimized for Xeon Gold 6148 Skylake CPUs may not run on gray nodes):</p> Text Only<pre><code>module load green-spack\n</code></pre> <p>Optimized for amp (AMD EPYC 7742 Zen2), available only on amp:</p> Text Only<pre><code>module load amp-spack\n</code></pre> <p>By default SPACK builds optimized for the CPU the software is build on. The packages from the gray nodes will work on the green nodes but slower than the optimized modules. Conversely, the skylake-optimized modules will try to use hardware features not present on the gray nodes, so the software will not run there.</p> <p>You can autoactivate the correct module path in your <code>.bashrc</code> with a code block like this:</p> Text Only<pre><code>if [[ $(hostname -s) = base ]]; then\n  module load green-spack\nelif [[ $(hostname -s) = green* ]]; then\n  module load green-spack\nelif  [[ $(hostname -s) = gray* ]]; then\n  module load gray-spack\nelif  [[ $(hostname -s) = amp ]]; then\n  module load amp-spack\nfi\n</code></pre>"},{"location":"spack.html#user-build-software","title":"User build software","text":"<p>SPACK can also be used by users to manage their own software stack inside their home directory (be aware, this takes a lot of space!).</p> <p>see documentation on https://spack.readthedocs.io/en/latest/</p> <p>A similar tool is EasyBuild. They support different lists of software packages. SPACK includes GPU-offloading compiler for both Nvidia and AMD, profiling tools (Tau, HPCToolkit) and engineering simulation packages (ElmerFEM, OpenFOAM), while EasyBuild seems to be more AI and Python oriented https://docs.easybuild.io/en/latest/version-specific/Supported_software.html.</p> <p>SPACK is used by University of Tartu, LRZ and HLRS, while EasyBuild will be used on LUMI.</p>"},{"location":"ssh.html","title":"Ssh","text":"<p>not changed to rocky yet</p>"},{"location":"ssh.html#getting-ssh-keys-to-work","title":"Getting SSH keys to work","text":""},{"location":"ssh.html#generationg-ssh-keys","title":"Generationg  SSH keys","text":"<p>In Linux and macOS SSH keys can be generated in cmd by command:</p> Text Only<pre><code>ssh-keygen -t rsa\n</code></pre> <p>In Windows SSH keys can be generated in powershell by command:</p> Text Only<pre><code>ssh-keygen.exe -t rsa\n</code></pre> <p>The program prompts the user through the process, just sets the location and asks the user to set a passphrase, which should be created. This process ends with two keys in the path it showed. One of them is named (by default) <code>id_rsa</code> and the other <code>id_rsa.pub</code>. The <code>.pub</code> key is your public key, the other is a private key. </p> <p></p> <p>NB! Never share your private key with anyone.</p>  More detail guides can be found here:   - Linux - [https://linuxhint.com/generate-ssh-keys-on-linux/](https://linuxhint.com/generate-ssh-keys-on-linux/)  - macOS - [https://docs.tritondatacenter.com/public-cloud/getting-started/ssh-keys/generating-an-ssh-key-manually/manually-generating-your-ssh-key-in-mac-os-x](https://docs.tritondatacenter.com/public-cloud/getting-started/ssh-keys/generating-an-ssh-key-manually/manually-generating-your-ssh-key-in-mac-os-x)  - Windows - [https://phoenixnap.com/kb/generate-ssh-key-windows-10](https://phoenixnap.com/kb/generate-ssh-key-windows-10)  <p> </p>"},{"location":"ssh.html#uploading-ssh-keys-to-base","title":"Uploading SSH keys to base","text":"<p>Once the keys are created, the public (.pub) key needs to be uploaded to base. There are a couple of ways to do it. On base, SSH public keys are found in <code>/etc/AuthorizedKeys/$USER</code> file, there is a link to it from <code>.ssh/authorized_keys</code> file.</p> <p>Several SSH keys can be used simultaneously to access the same user account in case of use several different devices. </p> <p>On Mac and Linux to copy keys to the cluster:</p> Text Only<pre><code>ssh-copy-id Uni-ID@base.hpc.taltech.ee\n</code></pre> <p> </p> <p>On windows it can be copied in powershell with this command:</p> Text Only<pre><code>type $env:USERPROFILE\\.ssh\\id_rsa.pub | ssh Uni-Id@base.hpc.taltech.ee \"cat &gt;&gt; .ssh/authorized_keys\"\n</code></pre> <p>A more thorough explanation with an example can be found here.</p>"},{"location":"visualization.html","title":"Visualization","text":"<p>The recommended way of doing visualizations is now using the desktop session on https://ondemand.hpc.taltech.ee.</p> <p>OnDemand is a graphical user interface that allows access to HPC via a web browser. Within the OnDemand environment users can access to a HPC files, submit jobs to a cluster, monitor jobs and HPS resources, run interactive applications like Jupyter.  </p> <p>The default desktop environment is xfce, which is configurable, lightweight and fast.</p> <p></p>"},{"location":"visualization.html#ondemand-desktop-on-any-node-and-cpu","title":"OnDemand Desktop on any node and CPU","text":"<p>The menu only contain very few programs from the operating system. However, all installed software can be open an XTerminal using the module system as you would from the command-line. To do that: </p> <ol> <li> <p>Choose \"TalTech HPC Desktop\". </p> <p> <p></p> <li> <p>Set up and launch an interactive desktop (1 core and 1 GB of memory is usually enough if no calculations are planned).</p> <p> <p></p> <p>NB! Check and your account. </p> <li> <p>Firstly, your request will be put into a queue and this picture will appear. </p> <p> <p></p> <li> <p>When needed resources will become available, your session will start and this picture will appear. </p> <p>We recommend to use default settings for \"Compression\" and \"Image Quality\", unless you require high-quality screenshots.</p> <p> <p></p> <p>NB! Do not use quality settings \"Compression 0\" and/or \"Image Quality 9\", this will cause a zlib error message. The message box can be removed by reloading the browser tab. </p> <p> <p></p> <li> <p>To start interactive desktop press \"Launch TalTech HPC Desktop\"</p> <p>Will appear your HPC Desktop, where user can open XTerminal.</p> <p> <p></p> <li> <p>To start interactive desktop press \"Launch TalTech HPC Desktop\"</p> <p>Load environment and program needed and start vizualization. More detailed instructions on environment and program loading are given below. </p> <p> <p></p>"},{"location":"visualization.html#available-visualization-software-on-compute-nodes","title":"Available visualization software on compute nodes","text":"<p>Program from a list below and its environment can be loaded by:</p> Text Only<pre><code>module load rocky8-spack\nmodule load &lt;program name&gt;\n</code></pre> <p>where program must be written in lowercase letters </p> <ul> <li>ParaView </li> <li>VisIt </li> </ul> <ul> <li>Py-MayaVi </li> </ul> <ul> <li>Molden</li> </ul> <ul> <li>VMD </li> <li>Ovito</li> <li>Ospray (raytracer)</li> <li>PoVray (raytracer)</li> </ul> <p></p> <p>Programs are run by corresponding names in lowercase letters: paraview / visit / vmd.</p>"},{"location":"visualization.html#gaussview-avogadro","title":"GaussView &amp; Avogadro","text":"<p>GaussView can be started by commands:</p> Text Only<pre><code>module load rocky8/all\nmodule load gaussview\ngview.sh &lt;job name&gt;\n</code></pre> <p>To run Avogadro:</p> Text Only<pre><code>module load rocky8/all\nmodule load avogadro\navogadro &lt;job name&gt;\n</code></pre> <p></p>"},{"location":"visualization.html#ondemand-desktop-on-gpu-nodes-hardware-rendering","title":"OnDemand Desktop on GPU nodes (hardware rendering)","text":"<p>Requires of course to be submitted to a GPU node and a GPU to be reserved. The nodes are configured in a way that requires EGL rendering, and therefore may require other modules to be loaded (e.g. ParaView).</p> <p>Otherwise the Desktop works as the regular (software rendering) one, see above.</p> <p>Please note that for most applications software rendering is fast enough, only heavy visulalization, like volume visualization in ParaView, COVISE, VisIt, VMD, Star-CCM+ and Ansys may require the GPU rendering.</p> <p>Check using <code>nvtop</code> that your application actually uses the GPU!!!</p>"},{"location":"visualization.html#paraview-with-egl-acceleration","title":"ParaView with EGL acceleration","text":"<p>It is not possible to have EGL rendering and the OpenGL GUI compiled together, therefore the EGL accelerated <code>pvserver</code> and the OpenGL GUI come from different modules and can run on different compute nodes.</p> <p>The startup procedure for EGL accelerated rendering is the same as for use of ParaView in distributed mode.</p> <ol> <li>Start an OnDemand desktop on a GPU node and request a GPU</li> <li>Open 2 XTerms</li> <li>in Xterm 1: <code>module load rocky8-spack paraview/5.12.1-gcc-10.3.0-dotq</code> and start the ParaView GUI <code>paraview</code></li> <li>in Xterm 2: <code>module load rocky8 paraview/5.12.1-egl</code> and start the ParaView server <code>pvserver</code> (alternatively, you could ssh into base and start a separate job on a gpu node with srun or sbatch)</li> <li>in GUI select \"Connect\" and connect to either localhost:11111 or the gpu node the pvserver runs on, use \"manual\" connect, then choose \"connect\".</li> </ol> <p>A similar procedure can also be used to connect a client running on your desktop computer to the pvserver on the compute node.</p> <p>For more explanations, see (ParaView WIKI)[https://www.paraview.org/Wiki/Reverse_connection_and_port_forwarding].</p>"},{"location":"visualization.html#starccm-with-hardware-rendering","title":"StarCCM+ with hardware rendering","text":"Text Only<pre><code>vglrun starccm+ -clientldpreload /usr/lib64/libvglfaker.so -graphics native -rgpu auto  -power -fabric TCP -podkey $YOURPODKEY ...\n</code></pre>"},{"location":"visualization.html#in-situ-visualization-in-preparation","title":"In-situ visualization (in preparation)","text":"<p>In-situ visualization creates the visualization during the simulation instead of during the postprocesssing phase. The simulation code needs to be connected to in-situ visualization libraries. e.g. Catalyst (ParaView), LibSim (VisIt) and Ascent.</p> <p>The following are installed on our cluster</p> <ul> <li>(Catalyst)[https://www.paraview.org/hpc-insitu/]</li> <li>(Ascent)[https://github.com/Alpine-DAV/ascent]</li> <li>LibSim</li> <li>SENSEI</li> </ul> <p>Ascent on all nodes</p> Text Only<pre><code>module load rocky8-spack\nmodule load ascent\n</code></pre> <p>Catalyst on all nodes</p> Text Only<pre><code>module load rocky8-spack\nmodule load libcatalyst/2.0.0-gcc-10.3.0-openblas-bp26\n</code></pre> <p>Catalyst can be used within OpenFOAM and (NEK5000)[https://github.com/KTH-Nek5000/InSituPackage] simulations.</p> <p> </p>"},{"location":"chemistry/cp2k.html","title":"Cp2k","text":"<p>not changed to rocky yet</p>"},{"location":"chemistry/cp2k.html#cp2k","title":"cp2k","text":""},{"location":"chemistry/cp2k.html#cp2k-short-introduction","title":"cp2k short introduction","text":"<ol> <li>Make cp2k.slurm batch script for parallel calculations:</li> </ol> Text Only<pre><code>    #!/bin/bash\n    #SBATCH -p common\n    #SBATCH --ntasks=1\n    #SBATCH --cpus-per-task=2         # CPU cores per MPI process\n    #SBATCH --mem-per-cpu=1G          # host memory per CPU core\n    #SBATCH --time=0-03:00            # time (DD-HH:MM)\n    export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n    module load green/spack\n    module load cp2k/8.2-gcc-10.3.0-7cv4\n    srun cp2k.psmp -i H2O-32.inp -o H2O-32.out\n</code></pre> <ol> <li>Copy job-input file H2O-32.inp</li> <li>Submit the job on base:Text Only<pre><code>   sbatch cp2k.slurm\n</code></pre> </li> </ol>"},{"location":"chemistry/cp2k.html#cp2k-long-version","title":"cp2k long version","text":""},{"location":"chemistry/cp2k.html#environment","title":"Environment","text":"<p>Environment is set up by the commands:</p> Text Only<pre><code>    module load green-spack\n    module load cp2k\n</code></pre>"},{"location":"chemistry/cp2k.html#running-cp2k-jobs","title":"Running cp2k jobs","text":"<p>cp2k is MPI and SMP parallelized, it requires OpenMPI environment to be initialized, which means <code>mpirun</code> or <code>srun</code> need to be used to start it.</p>"},{"location":"chemistry/cp2k.html#time","title":"Time","text":""},{"location":"chemistry/cp2k.html#memory","title":"Memory","text":""},{"location":"chemistry/cp2k.html#how-to-cite","title":"How to cite:","text":""},{"location":"chemistry/cp2k.html#cp2k-with-gpus-on-amp","title":"cp2k with GPUs on amp","text":"<p>The version 7.1 has a bug, use only a single MPI task and a single GPU,, for multiple MPI tasks and GPUs (1 GPU per ntask) use version 9.1!</p> <p>Login to amp or amp2 using ssh (ssh-keys need to be configured)</p> Text Only<pre><code>ssh amp2\n</code></pre> <p>initialize the environment</p> Text Only<pre><code>source /usr/share/lmod/6.6/init/bash\nmodule use /gpfs/mariana/modules/system\nmodule load amp-spack/0.19.0\nmodule load ucx/1.13.0-gcc-9.3.0-5yyu\nmodule load openmpi/4.1.1-gcc-9.3-amp\nmodule load cp2k/7.1-gcc-9.3.0-openblas-m7xt\n</code></pre> <p>either run the job with srun</p> Text Only<pre><code>srun -p gpu-test --gres=gpu:2 -n 2 --cpus-per-task=1 --mem=16G cp2k.psmp -i H2O-32.inp -o log-H2O-32\n</code></pre> <p>or better use the cp2k-gpu.slurm script to submit the job:</p> Text Only<pre><code>#!/bin/bash\n#SBATCH -p gpu-test\n#SBATCH --gres=gpu:3                  # total number of GPUs\n#SBATCH --ntasks=2\n##SBATCH --ntasks-per-gpu=1        # total of 2 MPI processes\n#SBATCH --cpus-per-task=2         # CPU cores per MPI process\n#SBATCH --mem-per-cpu=5G          # host memory per CPU core\n#SBATCH --time=0-03:00            # time (DD-HH:MM)\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\nsource /usr/share/lmod/6.6/init/bash\nmodule use /gpfs/mariana/modules/system\nmodule load amp-spack/0.19.0\nmodule load ucx/1.13.0-gcc-9.3.0-5yyu\nmodule load openmpi/4.1.1-gcc-9.3-amp\nmodule load cp2k/9.1-gcc-9.3.0-openblas-vgen\n\nsrun cp2k.psmp -i H2O-32.inp -o log-H2O-32.out\n</code></pre>"},{"location":"chemistry/crest.html","title":"CREST","text":""},{"location":"chemistry/crest.html#crest-short-introduction","title":"CREST short introduction","text":"<ol> <li> <p>Make crest.slurm batch script for parallel calculations:</p> <p>#!/bin/bash    #SBATCH --job-name=CREST-test    #SBATCH --mem-per-cpu=2GB    #SBATCH --nodes=1    #SBATCH --ntasks=1    #SBATCH --cpus-per-task=24    #SBATCH -t 1-00:00:00    #SBATCH --partition=common</p> <p>module load rocky8/all    module load xtb-crest</p> <p>#Run calculations     crest geometry.xyz --gfn2 --T 24 &gt; final.out</p> </li> <li> <p>Copy job-input file geometry.xyz</p> </li> <li> <p>Submit the job on base:</p> <p>sbatch crest.slurm</p> </li> </ol> <p>NB! CREST can be run only on 1 node. If job requires large memory amount, bigmem or gpu partition with 1TB RAM can be used. </p> <p>NB! It is recommended to optimize the geometries obtained from the CREST by more accurate methods. In the end of this page are given home-made bash scripts that can be helpful during this process. </p> <p></p>"},{"location":"chemistry/crest.html#crest-long-version","title":"CREST long version","text":"<p>CREST (Conformer\u2013Rotamer Ensemble Sampling Tool) was designed as conformer sampling program by Grimme's group. Conformational search can be done by various levels of theory including molecular mechanics and semiempirical methods (GFNn-xTB) in gas or solvent (using several continuum models). By default CREST uses root-mean-square-deviation (RMSD) based meta-dynamics, short regular MD simulations and Genetic Z-matrix crossing (GC) algorithms for generation of new conformers. CREST can be also used for searching of protonation states, tautomerism studies and non-covalent complexes modelling. More can be found in the original article.</p>"},{"location":"chemistry/crest.html#environment","title":"Environment","text":"<p>Environment is set up by the commands:</p> Text Only<pre><code>module load rocky8/all\nmodule load xtb-crest\n</code></pre>"},{"location":"chemistry/crest.html#running-crest-jobs","title":"Running CREST jobs","text":"<p>CREST input file should be in <code>.xyz</code> format and is executed by the command <code>crest</code>. This command is usually placed in <code>slurm</code> script. </p> Text Only<pre><code>crest geometry.xyz --gfn2 --gbsa h2o --T 24 &gt; final.out\n</code></pre> <p>In CREST calculation options are specified as command line arguments. <code>--T</code> is number of processors used, <code>--gfn2</code> -- calculation method (here GFN2-xTB), <code>--g h2o</code> -- GBSA implicit solvation model for water. More about command line arguments and some examples of CREST commands.</p>"},{"location":"chemistry/crest.html#time","title":"Time","text":"<p>Calculation time depends on size of molecule, its flexibility, chosen energy window, methods used, and can only be determined empirically. For example, for a flexible organic molecule of 65 atoms, conformational search using GFN-FF method and 24 cores took about 15-20 minutes and semiempirical GFN2 needed 5-8 hours. However, a lot depend on energy window applied to conformational search.</p>"},{"location":"chemistry/crest.html#memory","title":"Memory","text":"<p>Our experience shows that memory is the main limiting factor in conformational search calculations by CREST. Since memory consumption depends on many factors (size of molecule, its flexibility, chosen energy window, methods used), it can only be determined through trial and error, and perhaps mem1tb or gpu partition with 1TB RAM can be used.  In our test runs for a flexible organic molecule of 54 atoms using semiempirical GFN2 method, 1 GB per core was sufficient, but for 65 atoms molecule using the same level of theory already 2 GB per core were needed.</p>"},{"location":"chemistry/crest.html#how-to-cite","title":"How to cite:","text":"<p>The main publication for the CREST program - DOI: 10.1039/C9CP06869D.</p> <p></p>"},{"location":"chemistry/crest.html#useful-bash-scripts","title":"Useful bash scripts","text":"<p>It is recommended to optimise the geometries obtained from the CREST by more accurate methods. Here are home-made bash scripts that can be helpful. </p> <ul> <li> <p>Start-orca.sh &amp; start-gaussian.sh</p> <p>Start-orca.sh should be run from the directory where CREST conformer search was done. It splits CREST output into single geometries, prepare ORCA inputs and launch calculations. NB! orca.slurm must be in the same folder as <code>start-orca.sh</code> and CREST calculations. NB! Charge, Multiplisity and Number of conformers must be given as  command line arguments <code>-c</code>, <code>-m</code> and <code>-n</code>.</p> Text Only<pre><code>sh start-orca.sh -c 0 -m 1 -n 500\n</code></pre> <p>By default ORCA calculations will be done using the following method - RI-BP86-BJD3/def2-SVP . If it does not suit, the method can be changed in the <code>start-orca.sh</code> in the section \"ORCA method\".</p> <p>Start-gaussian.sh by analogy with <code>start-orca.sh</code> will create input for Gaussian and launch calculations.  </p> <p>By default Gaussian calculations will be done using the following method - BP86-BJD3/def2-SVP SMD(chloroform, Surface=SAS, Radii=Bondi) . If it does not suit, the method can be changed in the <code>start-gaussian.sh</code> in the section \"Gaussian method\".</p> <p>NB! if Surface=SAS &amp; Radii=Bondi are not used just replace them by one space and remove <code>read</code> from <code>scrf</code> keywords. NB! gaussian.slurm must be in the same folder as <code>start-gaussian.sh</code> and CREST calculations. NB! Charge, Multiplisity and Number of conformers must be given as  command line arguments <code>-c</code>, <code>-m</code> and <code>-n</code>.</p> </li> <li> <p>Check.sh verifies if all calculations ended normally.</p> </li> </ul> <p>NB! If Gaussian calculations were done - activate disabled rows starting with <code>#</code> and disable above rows for ORCA search by adding <code>#</code> mark before them.</p> <ul> <li>Crest-sorting.sh available only for ORCA calculations.                   1. creates CREST folder and move the initial CREST calculations there                 2. merges individual ORCA optimised geometries into a shared file <code>ALL.xyz</code>                 3. creates a single CREST file, which then will be treated by CREST algorithms to delete double structures and sort remained structures by energy.</li> </ul>"},{"location":"chemistry/gaussian.html","title":"Gaussian","text":"<p>Important note: To run Gaussian, user should be added to the Gaussian group. Contact to <code>hpcsupport@taltech.ee</code>.</p> <p></p>"},{"location":"chemistry/gaussian.html#gaussian-short-introduction","title":"Gaussian short introduction","text":"<ol> <li> <p>Make gaussian.slurm batch script:</p> <p>#!/bin/bash    #SBATCH --job-name=Job_Name    #SBATCH --mem-per-cpu=2GB    #SBATCH --nodes=1    #SBATCH --ntasks=1    #SBATCH --cpus-per-task=24    #SBATCH -t 1-00:00:00    #SBATCH --partition=common    #SBATCH --no-requeue</p> Text Only<pre><code>module load rocky8/all \nmodule load gaussian/16.c02\n\nSCRATCH=/state/partition1/$SLURM_JOB_ID\nexport GAUSS_SCRDIR=$SCRATCH\nmkdir -p $SCRATCH\n\ng16 -m=48gb -p=24 &lt; job.com &gt; job.log\n\n#Clean after yourself\nrm -rf  $SCRATCH\n</code></pre> </li> <li> <p>Copy job-input file job.com.</p> </li> <li> <p>Submit the job on base:</p> <p>sbatch gaussian.slurm</p> <p>NB! More cores does not mean faster!!! See benchmarks.</p> </li> <li> <p>Check results using visualization software.</p> </li> </ol> <p> </p>"},{"location":"chemistry/gaussian.html#gaussian-long-version","title":"Gaussian long version","text":"<p>Gaussian is a general purpose package for calculation of electronic structures. It can calculate properties of molecules (structures, energies, spectroscopic and thermochemical properties, atomic charges, electron affinities, electrostatic potentials, electron densities etc.) as well as reactions properties (such as reaction pathways, IRC) using different methods (such as Molecular mechanics, Semi-empirical methods,  Hartree-Fock, Density functional theory, M\u00f8ller-Plesset perturbation theory, coupled cluster). More about Gaussian features can be found here. </p>"},{"location":"chemistry/gaussian.html#environment","title":"Environment","text":"<p>There are several versions of Gaussian available at HPC: g09 (revision C.01) and g16 (revision B.01, C.01 and C.02). Environment and Gaussian version are set up by the commands:</p> Text Only<pre><code>module load rocky8/all\nmodule load gaussian/16.c02\n</code></pre>"},{"location":"chemistry/gaussian.html#running-gaussian-jobs","title":"Running Gaussian jobs","text":"<p>Gaussian input files are executed by the commands <code>g09</code> or <code>g16</code> depending on the version of Gaussian used. This command is usually placed in <code>slurm</code> script.</p> Text Only<pre><code>g16 &lt; job.com &gt; job.log\n</code></pre>"},{"location":"chemistry/gaussian.html#single-core-calculations","title":"Single core calculations","text":"<p>Gaussian by default executes jobs on only a single processor. </p> <p>NB! If more processors are defined in <code>slurm</code> script, they will be reserved but not used.</p>"},{"location":"chemistry/gaussian.html#parallel-jobs","title":"Parallel jobs","text":"<p>To run multiple processors/cores job  a number of cores should be specified. The number of cores can be defined via the <code>-p</code> flag (e.g. -p=4) in command line of <code>slurm</code> script or by adding the <code>%NprocShared</code> keyword into  Gaussian input file (e.g. %NprocShared=4). For more information see Gaussian manual. The number of processors requested should correspond to the number of processors requested in <code>slurm</code> script.</p> <p>NB! More cores does not mean faster!!! See benchmarks.</p> <p>Example of <code>slurm</code> script:</p> Text Only<pre><code>#!/bin/bash\n\n#SBATCH --job-name=Job_Name # Job name\n#SBATCH --mem=8GB       # Memory\n#SBATCH --nodes=1       # Number of nodes \n#SBATCH --ntasks=1      # Number of threads \n#SBATCH --cpus-per-task=4\n#SBATCH -t 1-00:00:00       # Time\n#SBATCH --partition=common  # Partition\n#SBATCH  --no-requeue       # Job will not be restarted by default\n\nmodule load rocky8/all\nmodule load gaussian/16.c02\n\nSCRATCH=/state/partition1/$SLURM_JOB_ID\nexport GAUSS_SCRDIR=$SCRATCH\nmkdir -p $SCRATCH\n\ng16 &lt; job.com &gt; job.log\n\nrm -rf  $SCRATCH\n</code></pre> <p>Example of Gaussian input:</p> Text Only<pre><code>%Mem=8GB\n%NprocShared=4\n%chk=job.chk\n#P B3LYP/6-311++G** Opt EmpiricalDispersion=GD3BJ\n\nJob_Name\n\n0,1\nC          0.67650        0.42710        0.00022\nH          0.75477        1.52537        0.00197\nO          1.62208       -0.30498       -0.00037\nS         -1.01309       -0.16870        0.00021\nH         -1.58104        1.05112       -0.00371\n</code></pre>"},{"location":"chemistry/gaussian.html#memory","title":"Memory","text":"<p>The default dynamic memory requested by Gaussian is frequently too small for successful job termination. Herein, if amount of memory requested is insufficient, the job will crash. There is no golden rule for memory requests. Usually, for common calculations (e.g. optimization, frequency etc.)  2 GB per 1 core is sufficient. This can be done by the <code>-m</code> flag in the command line (e.g. -m=48gb) or by adding the <code>%Mem</code> keyword in Gaussian input file (e.g. %Mem=2GB). For more information see Gaussian manual and taltech user-guides.</p> <p>However, there are calculations that require more memory (e.g TD-DFT, large SCF calculations, etc.). Data from a <code>slurm-JOBID.stat</code> file can be useful to determine the amount of memory required for a computation. In <code>slurm-JOBID.stat</code> file the efficiency of memory utilization is shown. </p> <p>Bad example:</p> Text Only<pre><code>Memory Utilized: 3.08 GB \nMemory Efficiency: 11.83% of 26.00 GB\n</code></pre> <p>Good example:</p> Text Only<pre><code>Memory Utilized: 63.12 GB \nMemory Efficiency: 98.62% of 64.00 GB\n</code></pre>"},{"location":"chemistry/gaussian.html#time","title":"Time","text":"<p>Time limits depend on time partition used. If calculation time exceeds the time limit requested in the <code>slurm</code> script, the job will be killed, and in the end of <code>slurm-JOBID.out</code> will be written \"error: *** JOB 317255 ON green23 CANCELLED AT 2023-08-11T22:28:01 DUE TO TIME LIMIT *** \"</p> <p>Therefore, it is recommended to request more time than is usually needed for calculation and create checkpoint files (by <code>%chk=job.chk</code> line in input file) that allows to restart job.</p>"},{"location":"chemistry/gaussian.html#using-gpus","title":"Using GPUs","text":"<p>GPUs are effective for large molecules, their energies, gradients and frequencies calculations. GPUs are not effective for small jobs, as well as for MP2 or CCSD calculations. </p> <p>GPU jobs can be run only on amp or amp2. To access amp user has to have ssh-keys copied to the base (how to do that).</p> <p>amp can be accessed by command:</p> Text Only<pre><code>ssh -J uni-ID@base.hpc.taltech.ee uni-ID@amp\n</code></pre> <p>Each GPU must be controlled by a specific CPU, wherein, CPUs used as GPU controllers do not participate as compute nodes during the calculations.</p> <p>The GPUs and CPUS used for calculations are specified with the <code>%GPUCPU</code> command, where gpu- and cpu-lists are a comma-separated lists, possibly including numerical ranges (e.g., 0-4,6). The corresponding items in the two lists are the GPU and its controlling CPU.</p> Text Only<pre><code>%cpu=0-9\n%gpucpu=0-1=0-1\n</code></pre> <p>NB! The controlling CPUs are included in <code>%CPU</code> command. NB! The GPU and CPU count starts from zero.</p> <p>Example of gaussian-gpu.slurm script for amp:</p> Text Only<pre><code>#!/bin/bash\n#SBATCH --job-name=Job_Name\n#SBATCH -t 1:00:00\n#SBATCH  --no-requeue\n#SBATCH --partition=gpu     # Partition\n#SBATCH --gres=gpu:A100:2   # 2 GPU are reserved\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=10  # 10 CPU are reserved\n#SBATCH --mem=160GB     # Memory\n\nmodule use /gpfs/mariana/modules/system\nmodule load amp/all\nmodule load Gaussian/16.c02\n\nSCRATCH=/state/partition1/$SLURM_JOB_ID\nexport GAUSS_SCRDIR=$SCRATCH\nmkdir -p $SCRATCH\n\ng16 job.com &gt; job.log\n\n#Clean after yourself\nrm -rf  $SCRATCH\n</code></pre> <p>Example of Gaussian input job-gpu.com (bad example, since molecule is small):</p> Text Only<pre><code>%mem=160GB\n%cpu=0-9\n%gpucpu=0-1=0-1\n# wb97xd/cc-pVDZ opt\n\nGPU test\n\n0 1\nN                  0.15134980    0.09540020    1.45819090\nC                  0.75130720   -1.21343470    1.83361500\nH                 -0.39763650    0.48328420    2.23924210\nH                  0.89227330    0.78663430    1.26643050\nC                  2.08354180   -1.05427080    2.58684570\nH                  2.46028270   -2.07052410    2.83958100\nH                  1.89047670   -0.54642730    3.56027240\nH                  0.94017970   -1.74157910    0.87502690\nC                 -0.27659390   -2.02386680    2.62322330\nH                 -1.22744720   -2.09807790    2.06259160\nH                  0.09671630   -3.04868580    2.81503150\nH                 -0.48448270   -1.55043170    3.60777900\nC                  3.18448030   -0.29819020    1.82025510\nC                  4.47118690   -0.16399430    2.65811660\nC                  3.50345720   -0.94054930    0.45675520\nH                  2.82725210    0.74302520    1.61940860\nC                  5.57163930    0.60375030    1.90697440\nH                  4.83763400   -1.18683360    2.90651280\nH                  4.24528280    0.32880900    3.62913610\nC                  4.59840130   -0.17643230   -0.29951390\nH                  3.82853470   -1.99290760    0.62773340\nH                  2.59267050   -0.99453480   -0.16920790\nC                  5.87589210   -0.03780040    0.54257470\nH                  6.49233690    0.65984540    2.52562860\nH                  5.23630520    1.65412070    1.74873850\nH                  4.81114970   -0.67947290   -1.26682010\nH                  4.21177720    0.83532620   -0.55856640\nH                  6.64325110    0.55322730   -0.00130230\nH                  6.31606730   -1.04822960    0.70571860\n</code></pre>"},{"location":"chemistry/gaussian.html#allocation-of-memory","title":"Allocation of memory","text":"<p>Allocating sufficient amounts of memory for GPU jobs is even more important when for CPU jobs. GPUs can have up to 40GB (amp1) and 80GB (amp2) of memory, wherein, must be at least an equal amount of memory given to the GPU and each control CPU thread from available 1 TB of RAM.</p> <p>Gaussian gives equal shares of memory to each thread, this means that the total memory allocated should be the number of threads times the memory required. </p>"},{"location":"chemistry/gaussian.html#restarting-a-failedinterrupted-calculation","title":"Restarting a failed/interrupted calculation","text":"<p>Killed or failed jobs can be restarted, but for this checkpoint file should be generated via a <code>%Chk</code> command within the Gaussian input file. For more information see  Gaussian FAQ, Gaussian restart and Using Gaussian Checkpoint Files.</p> <p>NB! Checkpoint files are very heavy and are readable only on the machine on which they were generated. After successful completion of the calculation, it is recommended to delete these files.</p>"},{"location":"chemistry/gaussian.html#how-to-cite","title":"How to cite:","text":"<ul> <li>Gaussian 16 - https://gaussian.com/citation/</li> <li>Gaussian 09 - Gaussian 09, Revision C.01, Frisch, M.J.; Trucks, G.W.; Schlegel, H.B.; Scuseria, G.E.; Robb, M.A.; Cheeseman, J.R.; Scalmani, G.; Barone, V.; Mennucci, B.; Petersson, G.A.; Nakatsuji, H.; Caricato, M.; Li, X.; Hratchian, H.P.; Izmaylov, A.F.; Bloino, J.; Zheng, G.; Sonnenberg, J.L.; Hada, M.; Ehara, M.; Toyota, K.; Fukuda, R.; Hasegawa, J.; Ishida, M.; Nakajima, T.; Honda, Y.; Kitao, O.; Nakai, H.; Vreven, T.; Montgomery, J.A., Jr.; Peralta, J.E.; Ogliaro, F.; Bearpark, M.; Heyd, J.J.; Brothers, E.; Kudin, K.N.; Staroverov, V.N.; Kobayashi, R.; Normand, J.; Raghavachari, K.; Rendell, A.; Burant, J.C.; Iyengar, S.S.; Tomasi, J.; Cossi, M.; Rega, N.; Millam, N.J.; Klene, M.; Knox, J.E.; Cross, J.B.; Bakken, V.; Adamo, C.; Jaramillo, J.; Gomperts, R.; Stratmann, R.E.; Yazyev, O.; Austin, A. J.; Cammi, R.; Pomelli, C.; Ochterski, J. W.; Martin, R.L.; Morokuma, K.; Zakrzewski, V.G.; Voth, G.A.; Salvador, P.; Dannenberg, J.J.; Dapprich, S.; Daniels, A.D.; Farkas, \u00d6.; Foresman, J.B.; Ortiz, J.V.; Cioslowski, J.; Fox, D.J. Gaussian, Inc., Wallingford CT, 2009. </li> </ul>"},{"location":"chemistry/gaussian.html#benchmarks-for-parallel-jobs","title":"Benchmarks for parallel jobs","text":"<p>Gaussian example benchmarks performed with Gaussian 16 C01. The job had 37 atoms.</p>    ![Molecule](Porphyrin-1.png)   <p></p> <p></p> <p></p> <p></p>"},{"location":"chemistry/multiwfn.html","title":"Multiwfn","text":"<p>This manual is work in progress, please check regularly for updates</p> <p></p>"},{"location":"chemistry/multiwfn.html#multiwfn-short-introduction","title":"Multiwfn short introduction","text":"<ol> <li> <p>Make Multiwfn input <code>.mwfn</code>, <code>.wfn</code>, <code>.wfx</code>, <code>.fch</code>, <code>.molden</code>, <code>.gms</code> (or <code>.cub</code>, <code>.grd</code>, <code>.pdb</code>, <code>.xyz</code>, <code>.mol</code> - for specific purposes).</p> </li> <li> <p>Accesse viz by remote access programs (more preferable) or by ssh protocol (less preferable):</p> Text Only<pre><code>ssh -X -Y -J  UNI-ID@base.hpc.taltech.ee UNI-ID@viz\n</code></pre> </li> <li> <p>Load enviroment:</p> Text Only<pre><code>module use /gpfs/mariana/modules/green/chemistry/\nmodule load MultiWFN/3.7\n</code></pre> </li> <li> <p>Run Multiwfn in interactive mode:</p> Text Only<pre><code>srun Multiwfn job.wfn\n</code></pre> <p>Multiwfn also can be run by multiwfn.slurm batch script as a non-interactive mode with pre-prepared responses:</p> Text Only<pre><code>#!/bin/bash\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --job-name=test\n#SBATCH --mem=2GB\n#SBATCH -t 1:00\n#SBATCH --partition=short\n\nmodule load green/all\nmodule load MultiWFN/3.7\n\nMultiwfn  job.wfn &lt;&lt; EOF &gt; /dev/null\n2\n2\n-4\n6\n0\n-10\n100\n2\n1\nmol.pdb\nq\nEOF\n</code></pre> <p>In this case job is submitted using <code>sbatch</code> command:</p> Text Only<pre><code>sbatch multiwfn.slurm\n</code></pre> </li> <li> <p>Visualize results if needed:</p> Text Only<pre><code>display job.png\n</code></pre> <p>or </p> Text Only<pre><code>module use /gpfs/mariana/modules/gray/spack/\nmodule load vmd\nvmd job.pdb\n</code></pre> <p>NB! It is recommended to visualize Multiwfn results in VMD program, corresponding scripts are provided in Multiwfn examples (/gpfs/mariana/software/green/MultiWFN/Multiwfn_3.7_bin_Linux/examples/).</p> </li> </ol> <p></p>"},{"location":"chemistry/multiwfn.html#multiwfn-long-version","title":"Multiwfn long version","text":""},{"location":"chemistry/multiwfn.html#options","title":"Options","text":"<p>Multiwfn is an interactive program performing almost all of important wavefunction analyszes (showing molecular structure and orbitals, calculating real space function, topology analysis, population analysis, orbital composition analysis, bond order/strength analysis, plotting population density-of-states, plotting various kinds of spectra (including conformational weighted spectrum), quantitative analysis of molecular surface, charge decomposition analysis, basin analysis, electron excitation analyses, orbital localization analysis, visual study of weak interaction, conceptual density functional theory (CDFT) analysis, energy decomposition analysis).</p> <p>For many frequently used analyszes Multiwfn has short youtube videos and \"quick start\" examples (/gpfs/mariana/software/green/MultiWFN/Multiwfn_3.7_bin_Linux/examples/). More information can be found in the manual. </p>"},{"location":"chemistry/multiwfn.html#input","title":"Input","text":"<p>As an input Multiwfn uses output files of other quantum chemistry programs, including Gaussian, ORCA, GAMESS-US, NWChem, xtb, Turbomole. For example, <code>.wfn</code> (wavefunction file), <code>.fch</code> (Gaussian check file), <code>.molden</code> (Molden input file), <code>.gms</code> (GAMESS-US output file), <code>.mwfn</code> (Multiwfn wavefunction file). Other types of files, such as  such as <code>.cub</code>, <code>.grd</code>, <code>.pdb</code>, <code>.xyz</code>, <code>.log</code>, <code>.out</code>  and <code>.mol</code> files, may be used in certain cases and purposes.</p>"},{"location":"chemistry/multiwfn.html#environment","title":"Environment","text":"<p>On viz environment is set up by the commands:</p> Text Only<pre><code>module use /gpfs/mariana/modules/green/chemistry/\nmodule load MultiWFN/3.7\n</code></pre> <p>The first time use, user has to agree to the licenses: </p> Text Only<pre><code>touch ~/.licenses/multiwfn-accepted\n</code></pre> <p>if this is the first user license agreement, the following commands should be given:</p> Text Only<pre><code>mkdir .licenses\ntouch ~/.licenses/multiwfn-accepted\n</code></pre> <p>NB! After agreeing to the license, user has to log out and log in again to be able run <code>Multiwfn</code>.  </p> <p>On base environment is set up by the commands:</p> Text Only<pre><code>module load rocky8/all\nmodule load MultiWFN/3.7\n</code></pre> <p>User also needs to agree with the licenses, as described above.</p>"},{"location":"chemistry/multiwfn.html#running-multiwfn","title":"Running Multiwfn","text":"<p>NB!  Since Multiwfn has a lot of functionality, we recommend that the user first study the corresponding section in the manuals (text, video) or examples (/gpfs/mariana/software/green/MultiWFN/Multiwfn_3.7_bin_Linux/examples/). This will greatly simplify the selection of answers in the interactive menu.</p> <p>The best practice is to try to reproduce something from the examples folder. To do this, the corresponding files will need to be copied to the user's derictory using the following commands:</p> Text Only<pre><code>mkdir examples\ncp -r /gpfs/mariana/software/green/MultiWFN/Multiwfn_3.7_bin_Linux/examples/* examples/\n</code></pre> <p>NB! The user can run Multiwfn only from his own folder, not from the shared.</p> <p>For visualization that does not perform additional calculations, but only reads outputs (for example spectra visualization), Multiwfn can be run in interactive mode using <code>srun</code>:</p> Text Only<pre><code>srun Multiwfn job.log\n</code></pre> <p>or using several threads (here - 4):    </p> Text Only<pre><code>srun -n 4 Multiwfn job.log\n</code></pre> <p>To exit interactive mode press <code>q</code> key. </p> <p>For jobs connected to electron density analysis especially in large systems it is recommended to run multiwfn.slurm batch script with pre-prepared responses. Below is shown slurm script for Critical Points (CPs) search using job.wfn:</p> Text Only<pre><code>#!/bin/bash\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --job-name=test\n#SBATCH --mem=2GB\n#SBATCH -t 1:00\n#SBATCH --partition=short\n\nmodule load rocky8/all\nmodule load MultiWFN/3.7\n\nMultiwfn  job.wfn &lt;&lt; EOF &gt; /dev/null\n2\n2\n-4\n6\n0\n-10\n100\n2\n1\nmol.pdb\nq\nEOF\n</code></pre> <p>Job is submitted by <code>sbatch</code> command:</p> Text Only<pre><code>sbatch multiwfn.slurm\n</code></pre>"},{"location":"chemistry/multiwfn.html#results-visualization","title":"Results visualization","text":"<p>By default, plots made by Multiwfn will be written in the <code>.png</code> format and can be visualized by command:</p> Text Only<pre><code>display job.png\n</code></pre> <p>Although Multiwfn has its own graphical interface, we recommend to visualize Multiwfn results in VMD (Visual Molecular Dynamics) program, corresponding scripts are provided in Multiwfn  examples (/gpfs/mariana/software/green/MultiWFN/Multiwfn_3.7_bin_Linux/examples/) (with <code>.vmd</code> extensions). More about visualization on viz can be found here and about VMD - here.</p> <p>On base VMD environment is set up by the commands:</p> Text Only<pre><code>module load green\nmodule load VMD\n</code></pre> <p>VMD is run by command <code>vmd</code>:</p> Text Only<pre><code>vmd job.pdb\n</code></pre>"},{"location":"chemistry/multiwfn.html#how-to-cite","title":"How to cite:","text":"<p>Citing the original paper of Multiwfn is mandatory - DOI: 10.1002/jcc.22885</p> In addition, the following articles should be cited depending on the analyzes performed:  - Quantitative molecular surface analysis (main function 12) - DOI:[10.1016/j.jmgm.2012.07.004](https://www.sciencedirect.com/science/article/abs/pii/S1093326312000903) - Hole-electron analysis (subfunction 1 of main function 18) - DOI:[10.1016/j.carbon.2020.05.023](https://www.sciencedirect.com/science/article/abs/pii/S0008622320304644) - Electrostatic potential evaluation algorithm - DOI:[10.1039/D1CP02805G](https://pubs.rsc.org/en/content/articlelanding/2021/cp/d1cp02805g) - Orbital composition analysis (main function 8). - _Tian Lu, Feiwu Chen, Calculation of Molecular Orbital Composition, Acta Chim. Sinica, 69, 2393-2406 (2011)_ (in Chinese) ([http://sioc-journal.cn/Jwk_hxxb/CN/abstract/abstract340458.shtml](http://sioc-journal.cn/Jwk_hxxb/CN/abstract/abstract340458.shtml)) - Charge decomposition analysis (CDA) (main function 16) - _Meng Xiao, Tian Lu, Generalized Charge Decomposition Analysis (GCDA) Method, Journal of Advances in Physical Chemistry, 4, 111-124 (2015)_ (in Chinese) ([http://dx.doi.org/10.12677/JAPC.2015.44013](http://dx.doi.org/10.12677/JAPC.2015.44013)) - Atomic dipole moment corrected Hirshfeld (ADCH) - DOI:[10.1142/S0219633612500113](https://www.worldscientific.com/doi/10.1142/S0219633612500113) - Population analysis module (main function 7) - DOI:[10.3866/PKU.WHXB2012281](http://www.whxb.pku.edu.cn/EN/10.3866/PKU.WHXB2012281) - Laplacian bond order (LBO) - DOI: [10.1021/jp4010345](https://pubs.acs.org/doi/10.1021/jp4010345) - Statistical analysis of area in different ESP ranges on vdW surface - DOI:[10.1007/s11224-014-0430-6](https://pubs.acs.org/doi/10.1021/jp4010345) - Charge-transfer spectrum  - DOI:[10.1016/j.carbon.2021.11.005](https://www.sciencedirect.com/science/article/abs/pii/S0008622321010745?via%3Dihub) - Electron localization function (ELF) - DOI:[10.3866/PKU.WHXB20112786](http://www.whxb.pku.edu.cn/EN/10.3866/PKU.WHXB20112786) - Analysis of valence electron and deformation density - DOI:[10.3866/PKU.WHXB201709252](http://www.whxb.pku.edu.cn/EN/10.3866/PKU.WHXB201709252) - Predicting binding energy of hydrogen bonds based properties of bond critical point - DOI:[10.1002/jcc.26068](https://onlinelibrary.wiley.com/doi/abs/10.1002/jcc.26068) - electron analysis based on localized molecular orbitals (e.g. subfunction 22 of main function 100) - DOI:[10.1007/s00214-019-2541-z](https://link.springer.com/article/10.1007/s00214-019-2541-z) - van der Waals potential analysis (subfunction 6 of main function 20) - DOI:[10.1007/s00894-020-04577-0](https://pubmed.ncbi.nlm.nih.gov/33098007/) - Interaction region indicator (IRI) (subfunction 4 of main function 20) - DOI:[10.1002/cmtd.202100007](https://chemistry-europe.onlinelibrary.wiley.com/doi/10.1002/cmtd.202100007) - Independent gradient model based on Hirshfeld partition (IGMH) (subfunction 11 of main function 20) - DOI:[10.1002/jcc.26812](https://onlinelibrary.wiley.com/doi/abs/10.1002/jcc.26812) - ICSSZZ map (subfunction 4 in main function 200) - DOI:[10.1016/j.carbon.2020.04.099](https://www.sciencedirect.com/science/article/abs/pii/S000862232030436X) - MO-PDOS map (Option -2 in main function 10) - DOI:[10.1016/j.carbon.2020.05.023](https://www.sciencedirect.com/science/article/abs/pii/S0008622320304644) - Molecular polarity index (MPI) (outputted by main function 12) - DOI:[10.1016/j.carbon.2020.09.048](https://www.sciencedirect.com/science/article/abs/pii/S0008622320309076) - Analysis of valence electron and deformation density - DOI:[10.3866/PKU.WHXB201709252](http://www.whxb.pku.edu.cn/EN/10.3866/PKU.WHXB201709252) - Studying molecular planarity via MPP, SDP and coloring atoms according to ds values - DOI:[10.1007/s00894-021-04884-0](https://pubmed.ncbi.nlm.nih.gov/34435265/) - Energy decomposition analysis based on force field (EDA-FF) - DOI:[10.1016/j.mseb.2021.115425](https://www.sciencedirect.com/science/article/abs/pii/S0921510721003834)  <p></p>"},{"location":"chemistry/nwchem.html","title":"NWChem","text":"<p>This manual is work in progress, please check regularly for updates</p> <p></p>"},{"location":"chemistry/nwchem.html#nwchem-short-introduction","title":"NWChem short introduction","text":"<ol> <li> <p>Make nwchem.slurm batch script for parallel calculations:</p> Text Only<pre><code>#!/bin/bash\n\n#SBATCH --job-name=NWChem\n#SBATCH --mem-per-cpu=2GB\n#SBATCH --nodes=1\n#SBATCH --ntasks=6\n#SBATCH -t 1:00:00\n#SBATCH --partition=common\n\nmodule load rocky8-spack\nmodule load nwchem\n\n#Create scratch directory\nSCRATCH=/state/partition1/$SLURM_JOB_ID\nmkdir -p $SCRATCH\ncp  $SLURM_SUBMIT_DIR/*.nw $SCRATCH/\ncd $SCRATCH/\n\nnwchem  job.nw &gt;&gt; job.out\n\n#Copy files back to working directory\ncp $SCRATCH/* $SLURM_SUBMIT_DIR\n\n#Clean after yourself\nrm -rf  $SCRATCH\n</code></pre> </li> <li> <p>Copy job-input file job.nw.</p> </li> <li> <p>Submit the job on base:</p> Text Only<pre><code>sbatch nwchem.slurm\n</code></pre> </li> <li> <p>Check results using visualization software.</p> </li> </ol> <p> </p>"},{"location":"chemistry/nwchem.html#nwchem-long-version","title":"NWChem long version","text":"<p>The North West computational chemistry (NWChem) is an ab initio computational chemistry software package. NWChem offers various approaches: density functional (DFT), second-order M\u00f6ller\u2013Plesset perturbation theory (MP2), single- and multi-reference (MR), ground-and excited-state and linear-response (LR) coupled-cluster (CC), multi-configuration self-consistent field (MCSCF), selected and full configuration interaction (CI). A broad range of DFT response properties, ground and excited-state molecular dynamics (MD) using either AMBER or CHARMM force fields or methods of quantum mechanics (QM), nudged elastic band (NEB) method, linear-response (LR), and real-time (RT) time-dependent density functional theory (TDDFT) are available in NWChem. Through its modular design, the ab initio methods can be coupled with the classical MD to perform mixed quantum-mechanics and molecular-mechanics simulations (QM/MM). Various solvent models and relativistic approaches are also available. Additionally, python programs may be embedded into the NWChem input and used to control the execution of NWChem. More about the possibilities of NWChem can be found in this article - 10.1063/5.0004997.</p>  Some useful links:  - the tutorials site at FAccTs - [https://nwchemgit.github.io/Home.html](https://nwchemgit.github.io/Home.html) - DOI:[10.1016/j.cpc.2010.04.018](https://www.sciencedirect.com/science/article/abs/pii/S0010465510001438?via%3Dihub)  <p></p>"},{"location":"chemistry/nwchem.html#environment","title":"Environment","text":"<p>At HPC is installed 7.0.2 version of NWChem. To start working with NWChem an environment needed to be set up with the commands:</p> Text Only<pre><code>module load rocky8-spack\nmodule load nwche\n</code></pre>"},{"location":"chemistry/nwchem.html#input-file","title":"Input file","text":"NWChem input file consists from certain blocks: geometry, SCF, DFT, MP2, etc. NWChem also allows to combine several jobs into one input file. Bellow is given example of NWChem input file (job.nw) where:   1. water dimer will bi firstly optimized at BP86-D3BJ/def2-SVP level of theory 2. frequency calculations will be done at the same level of theort 3. single poin energy will be calculated using larger basis set (def2-TZVPP) and B3LYP functional.  <p>Additionally in example input file are shown inplementation of some useful keywords as <code>print</code> and <code>linopt</code>.</p> Text Only<pre><code>start water            # all intermediate files will have this name\ntitle \"Water dimer\"    # title of job\n\necho                   # input file will be printed in the beginning of the output file\n\nmemory total 3000 mb\n\ncharge 0\n\ngeometry units angstrom\nO    -0.093470    -1.154274     0.290542\nH     0.329461    -0.566865    -0.340362\nH    -0.864449    -1.335840    -0.238173\nO    -0.135461     1.136660    -0.233474\nH     0.636237     1.304331     0.298468\nH    -0.563123     0.545569     0.390712\nend\n\nbasis\n    * library Def2-SVP\nend\n\nscf\n    rhf                    # restricted Hartree-Fock\n    singlet                # multiplicity\n    maxiter 100            # maximum number of SCF iterations \n    print low              # will minimize output\nend\n\ndft\n    mult 1                 # multiplicity\n    xc becke88 perdew86    # functional BP86\n    disp vdw 4             # dispersion correction D3BJ\n    print low              # will minimize output\nend\n\ndriver\n    maxiter 100            # maximum number of iterations during optimization\n    linopt 0               # speed up calculations \nend\n\ntask dft optimize\ntask dft freq numerical\n\nbasis\n    * library Def2-TZVPP\nend\n\ndft\n    mult 1                 # multiplicity\n    xc b3lyp               # functional B3LYP\n    disp vdw 4             # dispersion correction D3BJ\n    print low              # will minimize output\nend\n\ntask dft energy\n</code></pre> <p>NWChem is well suited for large system calculations or molecular dynamics simulations with subsequent calculation of system properties. Example of an input (job_MD.nw) for MD sinumation with subsequent calculation of dipole moment every 10 steps.   </p> <p>More about NWChem input can be found at NWChem manual.</p>"},{"location":"chemistry/nwchem.html#running-nwchem-jobs","title":"Running NWChem jobs","text":"<p>NWChem input files are executed by the command <code>nwchem</code>. This command is usually placed in slurm script.</p>"},{"location":"chemistry/nwchem.html#single-core-parallel-calculations","title":"Single core &amp; parallel calculations","text":"<p>NWChem jobs can be calculated on one thread, in parallel on one node or using several nodes at once. Depending on the size of job, the corresponging parameters must be modified in slurm file:</p> Text Only<pre><code>#SBATCH --ntasks=6\n#SBATCH --nodes=1\n</code></pre> <p>Below is given an example of <code>slurm</code> script for NWChem parallel run on 1 node and 6 threads with allocated memory of 3 GB:</p> Text Only<pre><code>#!/bin/bash\n\n#SBATCH --job-name=NWChem\n#SBATCH --mem=3GB\n#SBATCH --nodes=1\n#SBATCH --ntasks=6\n#SBATCH -t 1:00:00\n#SBATCH --partition=common\n\nmodule load rocky8-spack\nmodule load nwchem\n\n#Create scratch directory\nSCRATCH=/state/partition1/$SLURM_JOB_ID\nmkdir -p $SCRATCH\ncp  $SLURM_SUBMIT_DIR/*.nw $SCRATCH/\ncd $SCRATCH/\n\nnwchem  job.nw &gt; job.out\n\n#Copy files back to working directory\ncp $SCRATCH/* $SLURM_SUBMIT_DIR\n\n#Clean after yourself\nrm -rf  $SCRATCH\n</code></pre> <p>NB! in example of <code>slurm</code> script calculations will be done on a single node, thus partition is <code>common</code>. If several nodes will be use than partition should be <code>green-ib</code>.</p> Text Only<pre><code>#SBATCH --nodes=2\n#SBATCH --ntasks=120\n#SBATCH --partition=green-ib\n</code></pre> <p>NB! to be able to restart calculations, they must be done in the <code>$HOME</code> catalog, and not in <code>$SCRATCH</code> directory.</p>"},{"location":"chemistry/nwchem.html#restarting-a-failedinterrupted-calculation","title":"Restarting a failed/interrupted calculation","text":"<p>NWChem does not give message about normal ternination. If calculation terminated normally, otput will have this end:</p> Text Only<pre><code>                                  AUTHORS\n                                  -------\n E. Apra, E. J. Bylaska, N. Govind, K. Kowalski, M. Valiev, W. A. de Jong,\n  T. P. Straatsma, H. J. J. van Dam, D. Wang, T. L. Windus, N. P. Bauman,\n   A. Panyala, J. Hammond, J. Autschbach, K. Bhaskaran-Nair, J. Brabec,\nK. Lopata, S. A. Fischer, S. Krishnamoorthy, M. Jacquelin, W. Ma, M. Klemm,\n   O. Villa, Y. Chen, V. Anisimov, F. Aquino, S. Hirata, M. T. Hackler,\n       Eric Hermes, L. Jensen, J. E. Moore, J. C. Becca, V. Konjkov,\n        D. Mejia-Rodriguez, T. Risthaus, M. Malagoli, A. Marenich,\nA. Otero-de-la-Roza, J. Mullin, P. Nichols, R. Peverati, J. Pittner, Y. Zhao,\n    P.-D. Fan, A. Fonari, M. J. Williamson, R. J. Harrison, J. R. Rehr,\n  M. Dupuis, D. Silverstein, D. M. A. Smith, J. Nieplocha, V. Tipparaju,\n  M. Krishnan, B. E. Van Kuiken, A. Vazquez-Mayagoitia, M. Swart, Q. Wu,\nT. Van Voorhis, A. A. Auer, M. Nooijen, L. D. Crosby, E. Brown, G. Cisneros,\n G. I. Fann, H. Fruchtl, J. Garza, K. Hirao, R. A. Kendall, J. A. Nichols,\n   K. Tsemekhman, K. Wolinski, J. Anchell, D. E. Bernholdt, P. Borowski,\n   T. Clark, D. Clerc, H. Dachsel, M. J. O. Deegan, K. Dyall, D. Elwood,\n  E. Glendening, M. Gutowski, A. C. Hess, J. Jaffe, B. G. Johnson, J. Ju,\n    R. Kobayashi, R. Kutteh, Z. Lin, R. Littlefield, X. Long, B. Meng,\n  T. Nakajima, S. Niu, L. Pollack, M. Rosing, K. Glaesemann, G. Sandrone,\n  M. Stave, H. Taylor, G. Thomas, J. H. van Lenthe, A. T. Wong, Z. Zhang.\n\nTotal times  cpu:       56.9s     wall:       57.2s\n</code></pre> <p>If job was not terminated normally, it can be restarted. However, to do this, calculations must be done in the <code>$HOME</code> catalog, and not in <code>$SCRATCH</code> directory.</p> <p>To restart calculation just change <code>start</code> command to <code>restart</code> in initial input file and run slurm script again. </p> <p>NB! we recommend to change the restart output file name so it was possible to compare progress in the end of calculations.</p>"},{"location":"chemistry/nwchem.html#memory","title":"Memory","text":"<p>At the beginning of the NWChem input file the amount of memory requested for the entire job must be specified. If amount of memory requested is insufficient, the job can crash. Memory usage in NWChem is controlled by the <code>memory total</code> keywords.</p> Text Only<pre><code>memory total 3000 mb\n</code></pre> <p>There is no golden rule for memory requests, since they are  basis set and calculation type dependant. Usually, 1-5 GB per 1 CPU is sufficient. Data from a <code>slurm-JOBID.stat</code> file can be useful to determine the amount of memory required for a computation. In <code>slurm-JOBID.stat</code> file the efficiency of memory utilization is shown. </p> <p>Bad example:</p> Text Only<pre><code>Memory Utilized: 3.08 GB \nMemory Efficiency: 11.83% of 26.00 GB\n</code></pre> <p>Good example:</p> Text Only<pre><code>Memory Utilized: 63.12 GB \nMemory Efficiency: 98.62% of 64.00 GB\n</code></pre>"},{"location":"chemistry/nwchem.html#time","title":"Time","text":"<p>Time limits depend on time partition used, see taltech user-guides. If the calculation time exceeds the time limit requested in the <code>slurm</code> script, then the job will be killed. Therefore, it is recommended to request more time than is usually needed for calculation. </p>"},{"location":"chemistry/nwchem.html#how-to-cite","title":"How to cite:","text":"<p>Please cite DOI:10.1063/5.0004997 when publishing results obtained with NWChem: </p> <p>And also look at the NWChem manual on the relevant topic, more detailed information on citing will be given there.   </p>"},{"location":"chemistry/orca.html","title":"ORCA","text":"<p>Important note: To run ORCA, user must registered individually and have agreed to the EULA at Orcaforum.</p> <p></p>"},{"location":"chemistry/orca.html#orca-short-introduction","title":"ORCA short introduction","text":"<ol> <li> <p>Make orca.slurm batch script for parallel calculations:</p> <p>#!/bin/bash    #SBATCH --job-name=ORCA-test    #SBATCH --mem=48GB    #SBATCH --nodes=1    #SBATCH --ntasks=24    #SBATCH --cpus-per-task=1    #SBATCH -t 1-00:00:00    #SBATCH --partition=common    #SBATCH  --no-requeue</p> Text Only<pre><code>module load rocky8/all\nmodule load orca/5.0.4\nexport orcadir=/gpfs/mariana/software/green/Orca/orca_5_0_4_openmpi_411/\n\n#Create scratch directory\nSCRATCH=/state/partition1/$SLURM_JOB_ID\nmkdir -p $SCRATCH\ncp  $SLURM_SUBMIT_DIR/* $SCRATCH/\ncd $SCRATCH/\n\n#Run calculations \n$orcadir/orca job.inp &gt; $SLURM_SUBMIT_DIR/job.log\n\n#Copy files back to working directory\ncp $SCRATCH/* $SLURM_SUBMIT_DIR\nrm *tmp*\n\n#Clean after yourself\nrm -rf  $SCRATCH\n</code></pre> <p>or orca-single-core.slurm batch script for single core calculations:  Click to expand  <p>#!/bin/bash    #SBATCH --job-name=Job_Name    #SBATCH --mem=2GB    #SBATCH --nodes=1     #SBATCH --ntasks=1    #SBATCH --cpus-per-task=1    #SBATCH -t 10:00:00    #SBATCH --partition=common    #SBATCH  --no-requeue</p> Text Only<pre><code>module load rocky8/all\nmodule load orca/5.0.4\nexport orcadir=/gpfs/mariana/software/green/Orca/orca_5_0_4_openmpi_411/\n\n#Create scratch directory\nSCRATCH=/state/partition1/$SLURM_JOB_ID\nmkdir -p $SCRATCH\ncp  $SLURM_SUBMIT_DIR/* $SCRATCH/\ncd $SCRATCH/\n\n#Run calculations \n$orcadir/orca job.inp &gt; $SLURM_SUBMIT_DIR/job.log\n\n#Copy files back to working directory\ncp $SCRATCH/* $SLURM_SUBMIT_DIR\nrm *tmp*\n\n#Clean after yourself\nrm -rf  $SCRATCH\n</code></pre> <p> </p> <li> <p>Copy job-input file job.inp (for single core run remove core specification block).</p> </li> <li> <p>Submit the job on base:</p> Text Only<pre><code>sbatch orca.slurm\n</code></pre> <p>NB! More cores does not mean faster!!! See benchmarks. NB! To ORCA parallel run full path name is needed. Single core calculations can be performed with just <code>orca</code> command.</p> </li> <li> <p>Check results using visualization software.</p> </li> <p> </p>"},{"location":"chemistry/orca.html#orca-long-version","title":"ORCA long version","text":""},{"location":"chemistry/orca.html#environment","title":"Environment","text":"<p>There are currently only latest ORCA 5.0.4 version is available. Environment is set up by the commands:</p> Text Only<pre><code>module load rocky8/all\nmodule load orca/5.0.4\n</code></pre> <p>The first time use, user has to agree to the licenses:</p> Text Only<pre><code>touch ~/.licenses/orca-accepted\n</code></pre> <p>if this is the first user license agreement, the following commands should be given:</p> Text Only<pre><code>mkdir .licenses\ntouch ~/.licenses/orca-accepted\n</code></pre> <p>NB! After agreeing to the license, user has to log out and log in again to be able run ORCA.  </p>"},{"location":"chemistry/orca.html#running-orca-jobs","title":"Running ORCA jobs","text":"<p>ORCA input files are executed by the command <code>orca</code>. This command is usually placed in <code>slurm</code> script.</p> <p>NB! To ORCA parallel run full path name is needed, but single core calculations can be performed with just <code>orca</code> command.</p> Text Only<pre><code>/gpfs/mariana/software/green/Orca/orca_5_0_4_openmpi_411/orca job.inp\n</code></pre>"},{"location":"chemistry/orca.html#single-core-calculations","title":"Single core calculations","text":"<p>ORCA by default execute jobs on only a single processor. Example of ORCA input:</p> Text Only<pre><code>! RI BP86 def2-SVP def2/J D4 printbasis Opt\n\n%maxcore 2000   #Use up to 2 GB of memory\n\n*xyz 0 1\nC          0.67650        0.42710        0.00022\nH          0.75477        1.52537        0.00197\nO          1.62208       -0.30498       -0.00037\nS         -1.01309       -0.16870        0.00021\nH         -1.58104        1.05112       -0.00371\n*\n</code></pre> <p>Example of an orca-single-core.slurm batch script for single core calculations.</p> <p>NB! If in <code>slurm</code>script is defined more processors, they will be reserved, but not utilized.</p>"},{"location":"chemistry/orca.html#parallel-jobs","title":"Parallel jobs","text":"<p>To run multiple processors/cores job a number of cores should be specified both in ORCA input file and in <code>slurm</code> script. In ORCA it is done with <code>PAL</code> keyword (e.g. PAL4) or as a block input.</p> <p>Example of ORCA input for 4 cores:</p> Text Only<pre><code>! RI BP86 def2-SVP def2/J D4 printbasis Opt\n\n%maxcore 2000   #Use up to 2 GB of memory\n\n%pal nprocs 4 end   #Use 4 cores\n\n*xyz 0 1\nC          0.67650        0.42710        0.00022\nH          0.75477        1.52537        0.00197\nO          1.62208       -0.30498       -0.00037\nS         -1.01309       -0.16870        0.00021\nH         -1.58104        1.05112       -0.00371\n*\n</code></pre> <p>Example of <code>slurm</code> script:</p> Text Only<pre><code>#!/bin/bash\n#SBATCH --job-name=ORCA-test        # Job name\n#SBATCH --mem=8GB               # Memory \n#SBATCH --nodes=1           # Number of nodes \n#SBATCH --ntasks=4          # Number of threads \n#SBATCH --cpus-per-task=1\n#SBATCH -t 2:00:00          # Time\n#SBATCH --partition=common      # Partition\n#SBATCH  --no-requeue           # Job will not be restarted by default\n\nmodule load rocky8/all\nmodule load orca/5.0.4\nexport orcadir=/gpfs/mariana/software/green/Orca/orca_5_0_4_openmpi_411/\n\n#Create scratch directory\nSCRATCH=/state/partition1/$SLURM_JOB_ID\nmkdir -p $SCRATCH\ncp  $SLURM_SUBMIT_DIR/* $SCRATCH/\ncd $SCRATCH/\n\n#Run calculations \n$orcadir/orca job.inp &gt; $SLURM_SUBMIT_DIR/job.log\n\ncp $SCRATCH/* $SLURM_SUBMIT_DIR\n\n#Clean after yourself\nrm -rf  $SCRATCH\n</code></pre> <p>NB! To ORCA parallel run full path name is needed. </p> <p>More about ORCA input can be found at ORCA Input Library, ORCA tutorials and ORCA forum.</p>"},{"location":"chemistry/orca.html#memory","title":"Memory","text":"<p>The default dynamic memory requested by ORCA is frequently too small for successful job termination. If amount of memory requested is insufficient, the job will be killed and in <code>slurm-JOBID.out</code> will appear _ \"... have been killed by the cgroup out-of-memory handler\". _ </p> <p>Memory usage in ORCA is controlled by the <code>%maxcore</code> keyword.</p> Text Only<pre><code>%maxcore 2000\n</code></pre> <p>There is no golden rule for memory requests, since they are  basis set and calculation type dependant. Usually, 2-8 GB per 1 CPU (thread) is sufficient. If Resolution of the identity (RI) approximation is used, the memory must be increased.  </p> <p>Data from a <code>slurm-JOBID.stat</code> file can be useful to determine the amount of memory required for a computation. In <code>slurm-JOBID.stat</code> file the efficiency of memory utilisation is shown. </p> <p>Bad example:</p> Text Only<pre><code>Memory Utilized: 3.08 GB \nMemory Efficiency: 11.83% of 26.00 GB\n</code></pre> <p>Good example:</p> Text Only<pre><code>Memory Utilized: 63.12 GB \nMemory Efficiency: 98.62% of 64.00 GB\n</code></pre>"},{"location":"chemistry/orca.html#time","title":"Time","text":"<p>Time limits depend on time partition used, see taltech user-guides. Therefore, it is recommended to request more time than is usually needed for calculation. </p> <p>If job was killed due to the time limit, this will be written in the end of  <code>slurm-JOBID.out</code> file \"error: *** JOB 317255 ON green23 CANCELLED AT 2023-08-11T22:28:01 DUE TO TIME LIMIT *** \"</p> <p>In this case some files including checkpoint file <code>gbw</code> will not be copied back to the working directory. To copy files user need to run interactive session to connect to the node where calculations were done. The node number is written in both <code>slurm-JOBID.stat</code> and <code>slurm-JOBID.out</code>. In example error message it was green23. </p> <p>Interactive session is started by the command:</p> Text Only<pre><code>srun -w greenXX --pty bash  # connect to green node\npwd             # see path to working directory\nls              # see JOBID from slurm\ncd /state/partition1/JOBID  # go to corresponding directory on green node\ncp job.gbw /gpfs/mariana/home/....../ # copy files needed to your working directory\n\nexit                # terminate interactive session\n</code></pre> <p>where <code>XX</code> - is the node number and <code>JOBID</code> - job serial number.</p> <p></p>"},{"location":"chemistry/orca.html#restarting-a-failedinterrupted-calculation","title":"Restarting a failed/interrupted calculation","text":"<p>All ORCA jobs are restart jobs as default. </p> <p>SCF calculations with input file name <code>job.inp</code> will automatically search for a file named <code>job.gbw</code> and will attempt to read in the old orbitals and continue the SCF from there.</p> <p><code>MOREAD</code> and <code>%moinp</code> keywords allows manually specify where to read the orbitals from. </p> Text Only<pre><code>! MORead\n%moinp \"job2.gbw\" \n# Note that if job2.gbw is the gbw file you read in then job2.inp can not be the name of the inputfile. \n*xyz 0 1\n</code></pre> <p>Geometry optimisation is recommended to be restarted using the last geometry (<code>job.xyz</code>).     </p> <p>Numerical frequency calculations also can be restarted if <code>.hess</code> files from the previous calculation are presented.</p> Text Only<pre><code>!\n%freq \nrestart true\nend\n</code></pre> <p>NB! Checkpoint files are very heavy and after successful completion of the calculation, it is recommended to delete these files. </p>"},{"location":"chemistry/orca.html#coping-files","title":"Coping files","text":"<p>During calculations ORCA creates many different additional files, by default, <code>slurm</code> copies all files to the user's directory. However, the user can choose which files to copy back to the working directory.</p> Text Only<pre><code>cp $SCRATCH/*.gbw  $SLURM_SUBMIT_DIR/\ncp $SCRATCH/*.engrad  $SLURM_SUBMIT_DIR/\ncp $SCRATCH/*.xyz  $SLURM_SUBMIT_DIR/\ncp $tdir/*.log  $SLURM_SUBMIT_DIR/\ncp $tdir/*.hess  $SLURM_SUBMIT_DIR/\n</code></pre>"},{"location":"chemistry/orca.html#how-to-cite","title":"How to cite:","text":"<ul> <li>Neese, F. (**2012) The ORCA program system, Wiley Interdiscip. Rev.: Comput. Mol. Sci., 2, 73-78.</li> <li>Neese, F. (2017) Software update: the ORCA program system, version 4.0, Wiley Interdiscip. Rev.: Comput. Mol. Sci., 8, e1327.</li> <li>Neese, F. (2022) Software update: The ORCA program system\u2014Version 5.0, WIREs Computational Molecular Science, 12, e1606.</li> </ul>"},{"location":"chemistry/orca.html#additional-information","title":"Additional Information","text":"<ul> <li>Official ORCA website</li> <li>ORCA Input Library</li> <li>ORCA compound scripts repository</li> <li>ORCA Tutorials by FAccTs</li> </ul>"},{"location":"chemistry/orca.html#benchmarks-for-parallel-jobs","title":"Benchmarks for parallel jobs","text":""},{"location":"chemistry/turbomole.html","title":"Turbomole","text":"<p>not changed to rocky yet</p> <p>\ufeff</p>"},{"location":"chemistry/turbomole.html#turbomole","title":"TURBOMOLE","text":"<p>** This manual is work in progress, please check regularly for updates **</p> <p>Important note: To run TURBOMOLE, user must be a member of the TURBOMOLE user group or have purchased TURBOMOLE licenses. </p> <p></p>"},{"location":"chemistry/turbomole.html#turbomole-short-introduction","title":"TURBOMOLE short introduction","text":"<ol> <li> <p>Set up environment and generate TURBOMOLE coordinate file:</p> Text Only<pre><code>module load turbomole7.0\nx2t inputfile.xyz &gt; start-coord\n</code></pre> </li> <li> <p>Run define to create the input files needed.</p> </li> <li> <p>Download TURBOMOLE.slurm batch script:</p> Text Only<pre><code>#!/bin/bash\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=1\n#SBATCH --job-name=test\n#SBATCH --mem-per-cpu=1GB\n#SBATCH -t 1:00:00\n#SBATCH --partition=common\n\nmodule load turbomole7.0\n\n#Directory where you run the script\nCALCULATIONDIR=`pwd`\n\n#Create scratch directory\nSCRATCHDIRNAME=/state/partition1/$SLURM_JOBID\nmkdir $SCRATCHDIRNAME\ncp * $SCRATCHDIRNAME\ncd $SCRATCHDIRNAME\n\n#Run calculations \n jobex -ri &gt; jobex.out 2&gt;jobex.err  # TURBOMOLE commands\n t2x -c &gt; final.xyz\n\n#Clean after yourself\nmv $SCRATCHDIRNAME/* $CALCULATIONDIR \nrm -rf $SCRATCHDIRNAME\n</code></pre> </li> <li> <p>Submit the job:</p> Text Only<pre><code>   sbatch TURBOMOLE.slurm\n</code></pre> </li> </ol> <p>NB! More cores does not mean faster!!! See Benchmarks.</p> <p></p>"},{"location":"chemistry/turbomole.html#turbomole-long-version","title":"TURBOMOLE long version","text":""},{"location":"chemistry/turbomole.html#environment","title":"Environment","text":"<p>There are currently several versions of TURBOMOLE (6.3 - 7.0) are available on HPC, and most of them can be run as parallel jobs. Environment is set up by the command:</p> Text Only<pre><code>module load turbomole7.0\nmodule load turbomole7.0-mpi    # for parallel run\n</code></pre>"},{"location":"chemistry/turbomole.html#running-turbomole-jobs","title":"Running TURBOMOLE jobs","text":"<p>TURBOMOLE uses its own coordinate file <code>coord</code>, which can be generated from .xyz file by TURBOMOLE command (when some TURBOMOLE version is already loaded):</p> Text Only<pre><code>x2t inputfile.xyz &gt; start-coord\n</code></pre> <p>Example of TURBOMOLE coordinate file:</p> Text Only<pre><code> $coord\n 1.27839972889714      0.80710203135546      0.00041573974923       c\n 1.42630859331810      2.88253155131977      0.00372276048178       h\n 3.06528696563114     -0.57632867600746     -0.00069919866917       o\n-1.91446264796512     -0.31879679861781      0.00039684248791       s\n-2.98773260513752      1.98632893279876     -0.00701088395301       h\n $end\n</code></pre> <p>In addition to coordinate file, TURBOMOLE uses a special interactive program <code>define</code> to create the input files, which determines  molecules' parameters, levels of theory used and calculation types.  </p> Text Only<pre><code>define\n</code></pre> <p>The answers to the <code>define</code>'s questions can be presented as a separate file. More about <code>define</code> can be read in \u2018Quick and Dirty\u2019 Tutorial and TURBOMOLE tutorial. Some examples of define files can be found here.</p> <p>To include solvent effects into calculations interactive program <code>cosmoprep</code> should be run after <code>define</code>.</p> Text Only<pre><code>cosmoprep\n</code></pre> <p>TURBOMOLE includes the Conductor-like Screening Model (COSMO), where the solvent is described as dielectric continuum with permittivity \u03b5. An example of cosmoprep file with acetonitrile as a solvent.</p> <p>After input files are created TURBOMOLE calculations are executed by one of the following commands: <code>dscf</code>, <code>ridft</code>, <code>jobex</code>, <code>aoforce</code>, <code>NumForce</code>, <code>escf</code>, <code>egrad</code>, <code>mpshift</code>, <code>raman</code>, <code>ricc2</code> etc. For example, </p> Text Only<pre><code>dscf           # for Hartree-Fock energy calculation (single point calculation)\njobex -ri      # geometry optimization using RI-approximation\naoforce        # analytical force constant calculations\nNumForce -ri   # numerical force constant calculations using RI-approximation\n</code></pre> <p>More about TURBOMOLE commands used can be found in TURBOMOLE tutorial.</p>"},{"location":"chemistry/turbomole.html#single-core-calculations","title":"Single core calculations","text":"<p>Example of Slurm script for single point HF calculation performed on a single core:</p> Text Only<pre><code>#!/bin/bash\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=1\n#SBATCH --job-name=test\n#SBATCH --mem-per-cpu=1GB\n#SBATCH -t 1:00:00\n#SBATCH --partition=common\n\nmodule load turbomole7.0\n\n#Directory where you run the script\nCALCULATIONDIR=`pwd`\n\n#Create scratch directory\nSCRATCHDIRNAME=/state/partition1/$SLURM_JOBID\nmkdir $SCRATCHDIRNAME\n\ncp * $SCRATCHDIRNAME\ncd $SCRATCHDIRNAME\n\n#Run calculations \ndscf &gt; JOB.out 2&gt;JOB.err\n\n#Clean after yourself\nmv $SCRATCHDIRNAME/* $CALCULATIONDIR \nrm -rf $SCRATCHDIRNAME\n</code></pre>"},{"location":"chemistry/turbomole.html#parallel-jobs-smp","title":"Parallel jobs SMP","text":"<p>Example of Slurm script for geometry optimization using RI-approximation performed by SMP run:</p> Text Only<pre><code>#!/bin/bash\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=24\n#SBATCH --job-name=test\n#SBATCH --mem-per-cpu=1GB\n#SBATCH -t 1:00:00\n#SBATCH --partition=common\n\nmodule load turbomole7.0-mpi\n\n#Directory where you run the script\nCALCULATIONDIR=`pwd`\n\n#Create scratch directory\nSCRATCHE=/state/partition1/$SLURM_JOBID\nmkdir $SCRATCH\n\ncp * $SCRATCH\ncd $SCRATCH\n\nexport PARA_ARCH=SMP\nexport PATH=$TURBODIR/bin/`sysname`:$PATH \nexport PARNODES=$SLURM_NTASKS \nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n\n#Run calculations \njobex -ri -c 600 &gt; jobex.out 2&gt;jobex.err \nt2x -c &gt; final.xyz\n\n#Clean after yourself\nmv $SCRATCH/* $CALCULATIONDIR \nrm -rf $SCRATC\n</code></pre>"},{"location":"chemistry/turbomole.html#memory","title":"Memory","text":"<p>For common calculations (e.g. optimization, frequency etc.) it is enough 1 GB per 1 CPU. However, some calculations can require more memory (e.g TD-DFT, large SCF calculations, etc.). Data from a <code>slurm-JOBID.stat</code> file can be useful to determine the amount of memory required for a computation. In <code>slurm-JOBID.stat</code> file the efficiency of memory utilization is shown. </p> <p>Bad example:</p> Text Only<pre><code>Memory Utilized: 3.08 GB \nMemory Efficiency: 11.83% of 26.00 GB\n</code></pre> <p>Good example:</p> Text Only<pre><code>Memory Utilized: 63.12 GB \nMemory Efficiency: 98.62% of 64.00 GB\n</code></pre>"},{"location":"chemistry/turbomole.html#time","title":"Time","text":"<p>Time limits depend on time partition used taltech user-guides. If the calculation time exceeds the time limit requested in the Slurm script, then the job will be killed. Therefore, it is recommended to request a little more than is usually needed for calculation. </p>"},{"location":"chemistry/turbomole.html#restarting-a-failedinterrupted-calculation","title":"Restarting a failed/interrupted calculation","text":"<p>All TURBOMOLE jobs are restart jobs as default.</p>"},{"location":"chemistry/turbomole.html#benchmarks-for-parallel-jobs","title":"Benchmarks for parallel jobs","text":""},{"location":"chemistry/visualization.html","title":"Visualization","text":"<p>not changed to rocky yet</p>"},{"location":"chemistry/visualization.html#visualization-software-for-computational-chemistry-biology-and-physics","title":"Visualization software for computational chemistry, biology and physics","text":"<p>Visualization mostly should be done on viz (Molden, Avogadro, JMol or VMD), but also short-time geometry checks are possible on base (Molden, Avogadro or JMol). </p> <p> </p>"},{"location":"chemistry/visualization.html#short-introduction","title":"Short introduction","text":"<ol> <li> <p>Accesse viz by remote access programs (more preferable) or by ssh protocol (less preferable):</p> Text Only<pre><code>ssh -X -Y -J  UNI-ID@base.hpc.taltech.ee UNI-ID@viz\n</code></pre> </li> <li> <p>Load enviroment (gray or gray/spack/):</p> Text Only<pre><code>module load viz-spack\nmodule load jmol\n</code></pre> </li> <li> <p>Run visualization program (<code>jmol</code>, <code>molden</code>, <code>avogadro</code>, <code>vmd</code> or  <code>rasmol</code>):</p> Text Only<pre><code>jmol job.xyz\n</code></pre> </li> </ol>"},{"location":"chemistry/visualization.html#visualization-long-version","title":"Visualization long version","text":""},{"location":"chemistry/visualization.html#molden","title":"Molden","text":"<p>Molden is a nice program for visualization the results of quantum chemical calculations. </p>  Molden is a quite universal:  - reads various input and output formats,  - can be used as a Z-matrix editor, - shows optimization paths, - animates reaction paths and molecular vibrations, - displays molecular orbitals and electron densities.  <p></p>"},{"location":"chemistry/visualization.html#environment","title":"Environment","text":"<p>On viz environment is set up by the commands:</p> Text Only<pre><code>module load viz-spack\nmodule load molden\n</code></pre> <p>On base environment is set up by the commands:</p> Text Only<pre><code>module load green/all\nmodule load molden\n</code></pre>"},{"location":"chemistry/visualization.html#running-molden","title":"Running Molden","text":"<p>Molden is executed by command <code>molden</code> and reads <code>.xyz</code>, Gaussian and ORCA outputs, etc.  </p> Text Only<pre><code>molden job.out\n</code></pre>  Some useful links:  - [Molden manual](https://www.theochem.ru.nl/molden/) - Building molecule [video](https://www.youtube.com/watch?v=t-Lf2ti6fxE) - Calculation results visualization [video](https://www.youtube.com/watch?v=sRvuJno1zW4) (includes showing optimization path and molecular vibrations)  <p></p> <p></p>"},{"location":"chemistry/visualization.html#how-to-cite","title":"How to cite:","text":"Molden 6.7:  - [DOI:10.1007/s10822-017-0042-5](https://link.springer.com/article/10.1007/s10822-017-0042-5) - [DOI:10.1023/A:1008193805436](https://link.springer.com/article/10.1023/A:1008193805436)"},{"location":"chemistry/visualization.html#avogadro","title":"Avogadro","text":"<p>Avogadro is an advanced molecule editor and visualizer designed for computational chemistry, molecular modeling, bioinformatics, materials science, and related areas. </p>  Avogado has many useful options:  - construction of molecules from fragments, - Z-matrix generation, - geometry optimization, - measurements, - performing a conformational search, - reading various input and output formats, - showing of molecular orbitals and electron density, - animation of reaction paths and molecular vibrations, - construction of IR spectra.  <p></p>"},{"location":"chemistry/visualization.html#environment_1","title":"Environment","text":"<p>On viz there is a native install of Avogadro (no modules needed).</p> <p>On base environment is set up by the commands:</p> Text Only<pre><code>module load green/all\nmodule load Avogadro\n</code></pre>"},{"location":"chemistry/visualization.html#running-avogadro","title":"Running Avogadro","text":"<p>Avogadro is executed by command <code>avogadro</code> and reads various input and output formats.</p> Text Only<pre><code> avogadro job.log\n</code></pre>  Some useful links:  - [Avogadro manual](avogadro.pdf) - Building molecule [video I](https://www.youtube.com/watch?v=LOZswm07j0U), [video II](https://www.youtube.com/watch?v=UWR4hPbtrHE), [video III](https://www.youtube.com/watch?v=fA3odPmkV-U), [changing molecule](https://www.youtube.com/watch?v=6O4DEoETAnU) (including optimization) - Orbitals visualization [video](https://www.youtube.com/watch?v=WpyqqDwspoo) - Molecular vibrations &amp; IR spectra [video](https://www.youtube.com/watch?v=AScn1OuyBoY)  <p> </p> <p> </p>"},{"location":"chemistry/visualization.html#how-to-cite_1","title":"How to cite:","text":"<ul> <li>Avogadro 1.2.0 available at http://avogadro.cc/</li> <li>DOI: 10.1186/1758-2946-4-17</li> </ul>"},{"location":"chemistry/visualization.html#jmol","title":"JMol","text":"<p>JMol is a free, open source viewer of molecular structures that supports a wide range of chemical file formats.</p>  JMol has following possibilities:  - visualization of animation, - visualization of vibration, - visualization of surfaces, - visualization of orbitals, - schematic shapes for secondary structures in biomolecules, - measurements.  <p></p>"},{"location":"chemistry/visualization.html#environment_2","title":"Environment","text":"<p>On viz environment is set up by the commands:</p> Text Only<pre><code>module load green-spack\nmodule load jmol\n</code></pre> <p>On base by the commands:</p> Text Only<pre><code>module load green-spack \nmodule load jmol\n</code></pre>"},{"location":"chemistry/visualization.html#running-jmol","title":"Running JMol","text":"<p>JMol is executed by command <code>jmol</code> and reads <code>.xyz</code>, <code>.pdb</code>, <code>.mol</code> formats, etc.  </p> Text Only<pre><code>jmol job.pdb\n</code></pre> <p></p>"},{"location":"chemistry/visualization.html#how-to-cite_2","title":"How to cite:","text":"<p>Jmol 14.31.0: an open-source Java viewer for chemical structures in 3D. Available at http://www.jmol.org/.</p> <p> </p>"},{"location":"chemistry/visualization.html#vmd","title":"VMD","text":"<p>Visual Molecular Dynamics (VMD) is a molecular modelling and visualization program designed of biological systems. It supports over 60 file formats and has user-extensible graphical and text-based interfaces, as well as built-on standard Tcl/Tk and Python scripting languages. VMD provides a wide range of methods for visualizing and coloring molecules or atom subsets and an extensive selection syntax for subsets of atoms and has no limits on the number of molecules, atoms, residues or trajectory frames. </p>  VMD has the following features:  - animation of MD trajectories, - analysis of MD trajectories, - analysis of sequences and structures of proteins and nucleic acids, - ability to export graphics to files that can be processed by ray tracing and image rendering packages, - ability to write molecular analysis programs in the Tcl language.  <p></p>  Some useful links:  - [VMD manual](vmd.pdf) - [Many tutorials](http://www.ks.uiuc.edu/Research/vmd/current/docs.html#tutorials) - Short introduction [video](https://www.youtube.com/watch?v=0KxdmnbodNw) - Long introduction [video](https://www.youtube.com/watch?v=_skmrS6X4Ys)  <p></p>"},{"location":"chemistry/visualization.html#environment_3","title":"Environment","text":"<p>The first time use, the user has to read the license at https://www.ks.uiuc.edu/Research/vmd/current/LICENSE.html the software can only be used if the license is accepted! If you agree to the license, do: </p> Text Only<pre><code>touch ~/.licenses/vmd-accepted\n</code></pre> <p>after that it is needed to unload module and load it again by commands:</p> Text Only<pre><code>module unload VMD/1.9.3-text\nmodule load VMD\n</code></pre> <p>if this is the first time you accept a license agreement, the following commands should be given:</p> Text Only<pre><code>mkdir .licenses\ntouch ~/.licenses/vmd-accepted \nmodule unload VMD/1.9.3-text\nmodule load VMD\n</code></pre> <p>On viz environment is set up by the commands:</p> Text Only<pre><code>module load viz\nmodule load VMD\n</code></pre> <p>On base environment is set up by the commands:</p> Text Only<pre><code>module load green\nmodule load VMD\n</code></pre> <p>User also needs to agree with the licenses, as described above.    </p>"},{"location":"chemistry/visualization.html#running-vmd","title":"Running VMD","text":"<p>VMD is executed by command <code>vmd</code> and reads various input and output formats.  </p> Text Only<pre><code>vmd job.mol\n</code></pre> <p></p>"},{"location":"chemistry/visualization.html#how-to-cite_3","title":"How to cite:","text":"<ul> <li>VMD 1.9.4 available at http://www.ks.uiuc.edu/Research/vmd/</li> <li>DOI:10.1016/0263-7855(96)00018-5 </li> </ul> In addition, the following articles should be cited depending on the functionalities used:  - Interactive Molecular Dynamics - DOI:[10.1145/364338.364398](https://dl.acm.org/doi/10.1145/364338.364398) - Multiple Alignment Plugin - DOI:[10.1093/bioinformatics/bti825](https://academic.oup.com/bioinformatics/article/22/4/504/184006) - Tachyon ray tracing library - John Stone [\"An Efficient Library for Parallel Ray Tracing and Animation\"](https://scholarsmine.mst.edu/cgi/viewcontent.cgi?article=2746&amp;context=masters_theses), _Computer Science Department, University of Missouri-Rolla,_ **1998** - STRIDE Secondary Structure Prediction - DOI:[10.1002/prot.340230412](https://onlinelibrary.wiley.com/doi/abs/10.1002/prot.340230412?casa_token=RH2f-1gBiPAAAAAA:dB4V2-CCPt_VIL_cv5Hi9U5S_blWaykCF69Mu5hfXNhKBd8r1PXeUD-2GHRsKFYgRI8ZLqNiG_2b0uwXtw) - SURF solvent accessible surface calculator - DOI:[10.1109/38.310720](https://ieeexplore.ieee.org/document/310720) - MSMS solvent excluded surface calculator - DOI:[doi/10.1145/220279.220324](https://dl.acm.org/doi/10.1145/220279.220324) - Speech and gesture recognition - DOI:[10.1109/38.824531](https://ieeexplore.ieee.org/document/824531)"},{"location":"chemistry/visualization.html#rasmol","title":"RasMol","text":"<p>RasMol is a molecular graphics program for visualisation of proteins, nucleic acids and small molecules.  RasMol provides  a variety of colour schemes and molecule representations. In RasMol, different parts of the molecule may be represented and coloured independently of the rest of the molecule or displayed in several representations simultaneously, and atoms may also be labelled with arbitrary text strings. In addition, RasMol can read a prepared list of commands from a 'script' file. Supported input file formats are <code>.pdb</code>, <code>.mol2</code>, <code>.mdl</code>, <code>.msc</code>, <code>.xyz</code>, <code>.xmol</code>, CHARMm and CIF formats files.  Finally, the rendered image may be written in a variety of formats such as GIF, PPM, BMP, PICT, Sun rasterfile or as a MolScript input script or Kinemage.</p> <p>RasMol is available on viz just by common <code>rasmol</code>.</p> Text Only<pre><code>rasmol job.mol\n</code></pre>  Some useful links:  - [RasMol manual](http://www.rasmol.org/software/RasMol_Latest_Manual.html) - tutorial for beginners [part 1](https://www.youtube.com/watch?v=cq5lpR2Hqgw), [part 2](https://www.youtube.com/watch?v=HXIkwW539jU)  <p></p>"},{"location":"chemistry/visualization.html#how-to-cite_4","title":"How to cite:","text":"<p>Herbert J. Bernstein, 2009. RasMol, available at: http://www.rasmol.org/.  </p>"},{"location":"chemistry/xtb.html","title":"xTB","text":""},{"location":"chemistry/xtb.html#xtb-short-introduction","title":"xTB short introduction","text":"<ol> <li> <p>Make xtb.slurm batch script for parallel calculations:</p> <p>#!/bin/bash    #SBATCH --job-name=xTB-test    #SBATCH --mem=2GB    #SBATCH --nodes=1    #SBATCH --ntasks=1    #SBATCH --cpus-per-task=24    #SBATCH -t 1-00:00:00    #SBATCH --partition=common</p> <p>module load rocky8/all    module load xtb-crest/6.7.0-crest3.0</p> <p>#Run calculations     xtb struc.xyz --opt tight --cycles 50 --charge -0 --alpb toluene --gfn 2 -P 4 &gt; final.out</p> </li> <li> <p>Copy job-input file struc.xyz</p> </li> <li> <p>Submit the job on base:</p> <p>sbatch xtb.slurm</p> </li> </ol> <p></p>"},{"location":"chemistry/xtb.html#xtb-long-version","title":"xTB long version","text":"<p>Extended tight binding - xTB program developed by Grimme's group for solutions of common chemical problems. The workhorses of xTB are the GFN methods, both semi-empirical and force-field. The program contains several implicit solvent models: GBSA, ALPB. xTB functionality covers single-point energy calculations, geometry optimization, frequency calculations, reaction path methods. Also allows to perform molecular dynamics, meta-dynamics, and ONIOM calculations. More about xTB on HPC can be found here.</p>"},{"location":"chemistry/xtb.html#environment","title":"Environment","text":"<p>Environment is set up by the commands:</p> Text Only<pre><code>module load rocky8/all\nmodule load xtb-crest/6.7.0-crest3.0\n</code></pre>"},{"location":"chemistry/xtb.html#running-xtb-jobs","title":"Running xTB jobs","text":"<p>Input file should be in <code>.xyz</code> format and is executed by the command <code>xtb</code>. This command is usually placed in <code>slurm</code> script. </p> Text Only<pre><code>xtb struc.xyz --opt tight --cycles 50 --charge -0 --alpb toluene --gfn 2 -P 4 &gt; final.out\n</code></pre> <p>In xTB calculation options are specified as command line arguments. <code>-P</code> is number of processors used, <code>--gfn 2</code> -- calculation method (here GFN2-xTB), <code>--alpb toluene</code> -- ALPB implicit solvation model for toluene. More about command line arguments.</p>"},{"location":"chemistry/xtb.html#time-memory","title":"Time &amp; Memory","text":"<p>Calculation time depends on size of molecule and methods used, and can only be determined empirically.</p> <p>1 GB per 2 or even more cores should be sufficient. For more detailed information look <code>slurm-XXX-.stat</code> file after a test run. The lines \"CPU Efficiency:\" and \"Memory Efficiency:\" will give an idea of \u200b\u200bhow efficiently the resources were used.</p>"},{"location":"chemistry/xtb.html#how-to-cite","title":"How to cite:","text":"<p>The main publication for the xTb program - DOI: 10.1002/wcms.1493.</p>"},{"location":"data-analysis/jupyter.html","title":"JupyterLab","text":"<p>JupyterLab is an interactive notebook environment (with a web-browser interface) for Julia, Python, Octave and R.</p> <p></p>"},{"location":"data-analysis/jupyter.html#jupyterlab-short-introduction","title":"JupyterLab Short introduction","text":"<ol> <li> <p>Download jupyterlab.slurm or  jupyterlab-gpu.slurm batch script:</p> Text Only<pre><code>#!/bin/bash\n#SBATCH --nodes 1\n#SBATCH --ntasks 1\n#SBATCH --time 01:30:00\n# #SBATCH -p gpu                #uncomment if needed\n# #SBATCH --gres=gpu:1          #uncomment if needed\n\n#Load Python modules\nmodule load rocky8-spack/\nmodule load r\nmodule load julia\nmodule load octave\nmodule load py-jupyterlab\nmodule load py-pip\n# module load cuda              #uncomment if needed\n\nmodule list\necho $SLURM_JOB_NODELIST\n\n#Create Jupyter conf\ncat &lt;&lt; EOF &gt; JupyterLab.conf.py\n# Set ip to '*' to bind on all interfaces\nc.NotebookApp.ip = '*'\nc.NotebookApp.open_browser = False\n\n# It is a good idea to set a known, fixed port for server access\nc.NotebookApp.port = 9900\nEOF\n\n#jupyter-lab --no-browser\njupyter-lab --no-browser --config=JupyterLab.conf.py\n\n#Remove jupyter conf\nrm JupyterLab.conf.py\n</code></pre> </li> <li> <p>Submit the job on base</p> <p>ssh uni-ID@base.hpc.taltech.ee    sbatch jupyterlab.slurm</p> </li> <li> <p>Check the <code>slurm-JOBID.out</code> file for the URL, host and port to connect to. NB! Text in <code>slurm-JOBID.out</code> can appear with some delay. </p> </li> <li> <p>Close terminal and establish a new connection to base with port forwarding</p> <p>ssh uni-ID@base.hpc.taltech.ee -L 99YY:greenX:99YY</p> </li> <li> <p>Point your webbrowser to the URL from the <code>slurm-JOBID.out</code>, e.g.</p> <p>http://green1:9900/lab?token=6ddef678a64245352e74fc66987d378ce075f390d89a2c7f</p> </li> </ol> <p>NB: Node name (green1 in the above example), port (here 9900) and token will be different for each job you submit! More detais can be found in Accessing Jupyter webinterface part.</p> <p></p>"},{"location":"data-analysis/jupyter.html#jupyterlab-on-taltech-hpc-long-version","title":"JupyterLab on Taltech HPC (long version)","text":""},{"location":"data-analysis/jupyter.html#network-access-to-taltech-hpc","title":"Network access to TalTech HPC","text":"<p>In order to access the HPC head-node base.hpc.taltech.ee you have to use some Estonia network or VPN.  TalTech OpenVPN can be used for access outside of Estonia network. </p>"},{"location":"data-analysis/jupyter.html#copy-the-slurm-start-script-to-hpc-head-node","title":"Copy the SLURM start script to HPC head node","text":""},{"location":"data-analysis/jupyter.html#1-command-line-solution","title":"1. Command line solution","text":"<p>Open command prompt. On Linux and mac open any terminal app, On Windows open PowerShell app (preferred) or Command Prompt app. Log into HPC head-node with SSH client (UniID - your UniID, usually six lower letter string):</p> Text Only<pre><code>ssh uni-ID@base.hpc.taltech.ee\nmkdir Jupyter  #Create folder for your Jupyter\nexit\n</code></pre> <p>Copy the sbatch script jupyterlab.slurm (for SLURM queuing system) to your newly created Jupyter folder in HPC.  NB! The file jupyterlab.slurm must be in the same folder where you execute the copy command Copy with sftp:</p> Text Only<pre><code>sftp uni-ID@base.hpc.taltech.ee:Jupyter/\nput local/path/from/where/to/copy/jupyterlab.sh\n</code></pre> <p>Copy with scp:</p> Text Only<pre><code>scp local/path/from/where/to/copy/jupyterlab.sh uni-ID@base.hpc.taltech.ee:Jupyter/\n</code></pre>"},{"location":"data-analysis/jupyter.html#2-gui-solution","title":"2. GUI solution","text":"<p>Download sftp capable file transfer software. For example, WinSCP (Windows) or FileZilla Client (any platform). Install sftp client, connect to base.hpc.taltech.ee, make your work directory and copy thejupyterlab.slurm file. More details about using WinSCP and FileZilla Client can be found here.</p>"},{"location":"data-analysis/jupyter.html#starting-jupyterlab-as-batch-job-in-hpc","title":"Starting JupyterLab as batch job in HPC","text":"<p>If necessary start VPN connection. Open terminal and ssh to HPC:</p> Text Only<pre><code>ssh uni-ID@base.hpc.taltech.ee\n</code></pre> <p>Change directory to Jupyter project folder and start JupyterLab as a batch job:</p> Text Only<pre><code>cd Jupyter\nsbatch jupyterlab.slurm\n</code></pre> <p>Then the system will say: Submitted batch job JOBID  were JOBID is unique number of a submitted job.</p> <p>You can check whether the job runs, waits in the queue or has finished with a command:  <code>squeue -u $uni-ID</code>. It gives you information about the job status, on which machine it runs, how long has it worked etc. </p> <p>As a default behavior submitted job standard output will be written into file slurm-JOBID.out.</p> <p>If you need to stop and remove the job this can be done with command <code>scancel JOBID</code>.</p>"},{"location":"data-analysis/jupyter.html#values-in-jupyterlabsh-slurm-batch-job-script","title":"Values in jupyterlab.sh SLURM batch job script","text":"<p>Values concerning queuing system SLURM:</p> Text Only<pre><code>#SBATCH --time 01:30:00 #Max runtime of the job, int this exampe 1 hour 30 minutes\n#SBATCH --ntasks 1  #Number of cores\n#SBATCH --nodes 1 #job must run on one node\n</code></pre> <p>Jupyter specific:</p> Text Only<pre><code>c.NotebookApp.port = 9900 # The port where Jupyter can be accessed\n</code></pre> <p>If the specified port is occupied then the Jupyter takes next available port. The actual port number is shown in slurm output file.</p>"},{"location":"data-analysis/jupyter.html#accessing-jupyter-webinterface","title":"Accessing Jupyter webinterface","text":"<p>You have to create ssh tunnel. Enter into terminal:</p> Text Only<pre><code>ssh uni-ID@base.hpc.taltech.ee -L 99YY:greenX:99YY\n</code></pre> <p>Where \"99YY\" is port number of the Jupyter web server and \"greenX\" is the name of the node where the Jupyter job runs.  The port YY and compute node X name must be replaced with the actual values.  These values are in the SLURM out file (unique to the running job).  To check the content of SLURM out file more or less command can be used:</p> Text Only<pre><code>less slurm-JOBID.out\n</code></pre> <p>NB! To quit <code>less</code> press <code>q</code>.</p> <p>In the end part of the file looks something like:</p> Text Only<pre><code>To access the server, open this file in a browser:\n    file:///gpfs/mariana/home/.......\nOr copy and paste one of these URLs:\n    http://green27:9900/lab?token=6ddef678a64245352e74fc66987d378ce075f390d89a2c7f\n or http://127.0.0.1:9900/lab?token=6ddef678a64245352e74fc66987d378ce075f390d89a2c7f\n</code></pre> <p>In this example port number is 9901 and compute name is green27. So you have to replace these values in the tunneling command presented above. The command will look now:</p> Text Only<pre><code>ssh uni-ID@base.hpc.taltech.ee -L 9901:green27:9901\n</code></pre> <p>If your tunnel is running (ssh connection is active) then the JupyterLab can be accessed from the address:</p> Text Only<pre><code>http://127.0.0.1:9901/lab?token=4046f45de18c9523525ed8d972d48618ee333c6417e640f6\n</code></pre> <p>Open the above presented address in your browser.  NB! These values are unique and different during each run. </p> <p></p>"},{"location":"data-analysis/jupyter.html#with-the-new-spack-based-modules","title":"with the new SPACK-based modules","text":"<p>The following modules need to be loaded to activate Julia, Octave and Python 3.8.12 in JupyterLab</p> Text Only<pre><code>    module load rocky8-spack/\n    module load r\n    module load julia\n    module load octave\n    module load py-jupyterlab\n    module load py-pip\n</code></pre>"},{"location":"data-analysis/jupyter.html#first-time-only-prepare-non-python-kernels","title":"First time only, prepare non-python kernels","text":"<p>Do the following in the command-line on base.</p> <p>For julia (start julia and add package):</p> Text Only<pre><code>julia\n]\nadd IJulia\n</code></pre> <p>For Octave:</p> Text Only<pre><code>python -m pip install --user octave_kernel\n</code></pre> <p>then proceed as above (in the non-SPACK case, but change the module lines in the .slurm script)</p> <p></p> <p></p>"},{"location":"data-analysis/jupyter.html#jupyter-notebooks-as-non-interactive-jobs","title":"Jupyter-notebooks as non-interactive jobs","text":"<p>Jupyter notebooks can be run non-interactive from the command-line using </p> <ul> <li> <p>the buildin <code>nbconvert</code></p> Text Only<pre><code> jupyter nbconvert --to notebook --inplace --execute mynotebook.ipynb -ExecutePreprocessor.timeout=None\n</code></pre> <p>you can also create a PDF from the notebook, by running</p> Text Only<pre><code> jupyter nbconvert --to PDF --execute mynotebook.ipynb -ExecutePreprocessor.timeout=None\n</code></pre> </li> <li> <p>the <code>papermill</code> package</p> Text Only<pre><code> papermill mynotebook.ipynb mynotebook_output.ipyn [args...]\n</code></pre> <p>papermill makes it possible to pass parameters, e.g. setting start and end variables</p> Text Only<pre><code> papermill mynotebook.ipynb mynotebook_output.ipyn -p start \"2017-11-01\" -p end \"2017-11-30\"\n</code></pre> </li> </ul>"},{"location":"data-analysis/octave.html","title":"Octave","text":"<p>not changed to rocky yet</p>"},{"location":"data-analysis/octave.html#octavematlab","title":"Octave/Matlab","text":"<p>Octave is a free alternative to Matlab and if you do not use Matlab's toolboxes, your code should work without many changes. For many of Matlab's toolboxes (partial) implementations exist for Octave as well. we have setup a module for Octave:</p> Text Only<pre><code>module load octave\n</code></pre> <p></p>"},{"location":"data-analysis/octave.html#octave-example-of-a-non-interactive-batch-job-single-process","title":"Octave: Example of a non-interactive batch job (single process)","text":"<p>SLURM batch script <code>octave-script.slurm</code></p> Text Only<pre><code>#!/bin/bash\n#SBATCH --partition=common\n#SBATCH -t 2:10:00\n#SBATCH -J octave\nmodule load octave\noctave script.m\n</code></pre> <p>The commands that Octave should calculate are in <code>script.m</code></p> Text Only<pre><code>n = 1000; \nA = normrnd(0,1, n, n); \nX = A'*A; Y = inv(X); \na = mean( diag(Y*X) ); \n## should output 1.0: \na\n</code></pre> <p></p>"},{"location":"data-analysis/octave.html#octave-netcdf-toolbox","title":"Octave netcdf toolbox","text":"<p>To use netcdf in octave the toolbox octcdf has to be installed from octave forge. Note that octcdf is a NetCDF toolbox for Octave which aims to be compatible with the \u201eoriginal\u201c matlab toolbox.</p> <p>To install the toolbox do following steps in the frontend and later the package is available in all nodes for your user.</p> Text Only<pre><code>module load green-spack\nmodule load octave\noctave\n</code></pre> <p>Inside octave do:</p> Text Only<pre><code>pkg install -forge -verbose octcdf\n</code></pre> <p>You may need to scroll down when necessary and it should complete the compilation successfully. Then you can start using Octcdf in your octave scripts by adding the line</p> Text Only<pre><code>pkg load octcdf\n</code></pre> <p></p>"},{"location":"data-analysis/octave.html#matlab","title":"Matlab","text":"<p>Matlab is available on the cluster through a campus license. Use</p> Text Only<pre><code>module avail\n</code></pre> <p>to see which version is installed. If a newer is needed, contact us via hpcsupport. Start using it by loading the module</p> Text Only<pre><code>module load green\nmodule load Matlab/R2018a\n</code></pre> <p>Matlab can be used non-interactive (like Octave in the above example).</p>"},{"location":"engineering/cad-mesh.html","title":"CAD &amp; Mesh-Tools","text":"<p>CAD and meshing software installed on viz, amp and green cluster nodes.</p> <p> </p>"},{"location":"engineering/cad-mesh.html#freecad","title":"FreeCAD","text":"<p>FreeCAD is an easy to use open-source CAD software, that can use Gmsh or Netgen for meshing. It can also serve as a frontend for CalculiX and ElmerFEM, thus providing similar functionality as SolidWorks. </p> Text Only<pre><code>module load rocky8\nmodule load freecad\nfreecad\n</code></pre> <p>(Please don't run simulations on viz) FreeCAD is best used  within a VNC session.</p>"},{"location":"engineering/cad-mesh.html#how-to-cite","title":"How to cite:","text":"<p>FreeCAD available from: https://www.freecadweb.org.</p> <p> </p>"},{"location":"engineering/cad-mesh.html#salome","title":"Salome","text":"<p>Salome is a multi-platform environment, allowing the realization of physics simulations. Salome is suitable for various stages of a study: from creation of the CAD model and the mesh to the post-processing and visualization of the results. Other functionalities such as uncertainty treatment, data assimilation are also implemented. Salome does not contain a physics solver but it provides the computing environment necessary for their integration.</p> Text Only<pre><code>module load rocky8\nmodule load salome/9.13.0\n</code></pre> <p>Salome has a python interface, so the meshing can be done as a batch-job on the cluster nodes.</p> <p>The current state can be dumped into a script from the GUI by <code>CTRL+D</code>, the script can then be executed on the command-line by: </p> Text Only<pre><code>salome -t scriptname.py\n</code></pre> <p>It is therefore possible to prepare the geometry on your workstation or an OnDemand desktop session, dump the script, add the meshing command and run the script on the cluster in batch-mode.</p> <p>See separate page on visualization.</p>"},{"location":"engineering/cad-mesh.html#how-to-cite_1","title":"How to cite:","text":"<p>Salome can be cited as DOI:10.13140/RG.2.2.12107.08485.</p> <p> </p>"},{"location":"engineering/cad-mesh.html#brl-cad","title":"BRL-CAD","text":"<p>BRL-CAD is a CAD software that has been in development since 1979 and is open-source since 2004. It is based on CSG modeling. BRL-CAD does not provide volume meshing, however, the CSG geometry can be exported to BREP (boundary representation, like STL, OBJ, STEP, IGES, PLY), the <code>g-*</code> tols are for this, while the <code>*-g</code> tools are for importing. An introduction can be found in this PDF.</p> <p> </p>"},{"location":"engineering/cad-mesh.html#gmsh","title":"Gmsh","text":"<p>Gmsh meshes can be used with ElmerFEM and OpenFOAM. For OpenFOAM, make sure it is saved as version 2 and ASCII format. </p> <p>The GUI can be used within a OnDemand desktop session, large meshes can be done as batch jobs on cluster nodes (use srun or sbatch), on the command-line run:</p> Text Only<pre><code>gmsh -3 -format msh2 -o outfilename.msh infilename.geo\n</code></pre> <p>this creates a volume mesh and saves as version 2 format suitable for OpenFOAM.</p> <p>Use the SPACK module:</p> Text Only<pre><code>module load rocky8-spack\nmodule load gmsh\n</code></pre>"},{"location":"engineering/cad-mesh.html#how-to-cite_2","title":"How to cite:","text":"<p>If you use Gmsh please cite C. Geuzaine and J.-F. Remacle. Gmsh: a three-dimensional finite element mesh generator with built-in pre- and post-processing facilities. Int. J. Numer. Methods Eng., 79(11), pp. 1309-1331, 2009.  You can also cite additional references for specific features and algorithms.</p> <p> </p>"},{"location":"engineering/cad-mesh.html#netgen-ngsolve","title":"Netgen (NGsolve)","text":"<p>Netgen is a part of the NGsolve suite. Netgen is a  automatic 3d tetrahedral mesh generator containing modules for mesh optimization and hierarchical mesh refinement. It accepts input from constructive solid geometry <code>.csg</code> or boundary representation (BRep) from <code>.stl</code> files, but also handles <code>.brep</code>, <code>.step</code> and <code>.iges</code> formats. Those meshes generated can be exported in several formats (e.g. neutral, Abaqus, Fluent, ElmerFEM Gmsh and OpenFOAM). Netgen has a GUI (e.g. use a X2GO session on viz), but can also be used through its Python interface.</p> <p>A Python example using the OpenCASCADE kernel:</p> Text Only<pre><code>from netgen.NgOCC import *\ngeo = LoadOCCGeometry('screw.step')\ngeo.Heal()\nmesh = geo.GenerateMesh()\n# mesh.Save('screw.vol')\nmesh.Export(\"export.msh\",\"Gmsh Format\")\n\nfrom ngsolve import *\nDraw(Mesh(mesh))\n</code></pre> <p>A Python example using the netgen STL:</p> Text Only<pre><code> import netgen.stl as stl\n geo2 = stl.LoadSTLGeometry(\"input.stl\")\n m2 = geo2.GenerateMesh (maxh=0.05)\n m2.Export(\"export.msh\",\"Gmsh2 Format\")\n</code></pre> <p>You can get a list of export formats from the GUI.</p>"},{"location":"engineering/cad-mesh.html#how-to-cite_3","title":"How to cite:","text":"<p>Netgen - J. Sch\u00f6berl. NETGEN An advancing front 2D/3D-mesh generator based on abstract rules. Comput. Vis. Sci., 1(1):41\u201352, 1997.</p> <p>Netgen/NGSolve is open source and available at www.ngsolve.org.</p> <p> </p>"},{"location":"engineering/elmerfem.html","title":"ElmerFEM","text":"<p>This page is work in progress</p> <p></p>"},{"location":"engineering/elmerfem.html#elmerfem_1","title":"ElmerFEM","text":"<p>Elmer is a multi-physics simulation software developed by CSC. It can perform coupled mechanical, thermal, fluid, electro-magnetic simulations and can be extended by own equations.</p>  Some useful links:   - Elmer homepage: [http://www.elmerfem.org/blog/](http://www.elmerfem.org/blog/)  - Elmer manuals and tutorials: [https://www.nic.funet.fi/pub/sci/physics/elmer/doc/](https://www.nic.funet.fi/pub/sci/physics/elmer/doc/)  <p></p>"},{"location":"engineering/elmerfem.html#how-to-cite","title":"How to cite:","text":"<p>CSC \u2013 IT CENTER FOR SCIENCE LTD., 2020. Elmer, Available at: https://www.csc.fi/web/elmer/.</p> <p> </p>"},{"location":"engineering/elmerfem.html#loading-the-module","title":"Loading the module","text":"<p>To use ElmerFEM the module needs to be loaded</p> Text Only<pre><code>module load rocky8-spack    \nmodule load elmerfem/9.0-gcc-10.3.0-netlib-lapack-qjdi\n</code></pre> <p>This makes the following main commands <code>ElmerGrid</code>, <code>ElmerSolver</code> available and <code>ElmerGUI</code> can be used with X11 forwarding or in an OnDemand desktop session to setup the case file. The use of ElmerGUI to run simulations is not recommended.</p> <p> </p>"},{"location":"engineering/elmerfem.html#running-a-tutorial-case-quick-start-for-the-impatient","title":"Running a tutorial case (quick-start for the impatient)","text":"<ol> <li> <p>Copy the tutorial directory (here the linear elastic beam) and go into it:</p> Text Only<pre><code>cp -r /share/apps/HPC2/ElmerFEM/tutorials-CL-files/ElasticEigenValues/ linear-elastic-beam-tutorial\ncd linear-elastic-beam-tutorial\n</code></pre> </li> <li> <p>Start an interactive session on a node</p> Text Only<pre><code>srun -t 2:00:00 --pty bash\n</code></pre> </li> <li> <p>Create the mesh</p> Text Only<pre><code>ElmerGrid d 7 2 mesh.FDNEUT\n</code></pre> </li> <li> <p>Run the solver</p> Text Only<pre><code>ElmerSolver\n</code></pre> </li> <li> <p>Postprocessing would be visualizing the <code>eigen_values.vtu</code> file in <code>paraview</code>.</p> </li> </ol> <p> </p>"},{"location":"engineering/elmerfem.html#setting-up-a-simulation-for-new-users","title":"Setting up a simulation (for new users)","text":"<p>The follwing steps are needed to configure a simulation case (mostly on base).   </p> <ol> <li> <p>Create geometry in Gmsh, group and name physical volumes and surfaces (can be done on viz). </p> </li> <li> <p>Create mesh in Gmsh (large meshes can be created from the CLI in a batch job: </p> Text Only<pre><code>gmsh -3 geometry.geo\n</code></pre> </li> <li> <p>Convert the mesh to elmer's format using ElmerGrid, including scaling if needed: </p> Text Only<pre><code>ElmerGrid 14 2 geometry.msh -scale 0.001 0.001 0.001\n</code></pre> </li> <li> <p> Create a new project in ElmerGUI (can be done on viz).  <ul> <li>create project  </li> <li>load Elmer mesh (point to the created mesh directory)   </li> <li>add equation(s)  </li> <li>add material(s)  </li> <li>add boundary conditions  </li> <li>create sif   </li> <li>edit &amp; save sif  </li> </ul> <li> <p>Edit the <code>case.sif</code> file (mesh directory, some other parameters [e.g. calculate PrincipalStresses] can only be added in the sif file, not in the GUI)</p> </li> <li> <p>Run simulation: </p> Text Only<pre><code>srun ElmerSolver\n</code></pre> <p>or create batch file and submit using sbatch.</p> </li> <li> <p>Postprocessing in ParaView.</p> </li> <p> </p>"},{"location":"engineering/elmerfem.html#an-example-simulation-case-from-start-to-finish","title":"An example simulation case from start to finish","text":""},{"location":"engineering/openfoam.html","title":"OpenFOAM","text":""},{"location":"engineering/openfoam.html#quick-start-example-use-of-openfoam-on-base-cluster","title":"Quick-start: Example use of OpenFOAM on BASE cluster","text":"<p>For the example we will use one of the tutorial cases.</p> <ol> <li> <p>Load environment:</p> Text Only<pre><code>module load rocky8-spack\nmodule load openfoam\n</code></pre> </li> <li> <p>First time users need to create their <code>$WM_PROJECT_USER_DIR</code>:</p> Text Only<pre><code>export WM_PROJECT_USER_DIR=$HOME/OpenFOAM/$USER-$WM_PROJECT_VERSION\nmkdir $WM_PROJECT_USER_DIR --parent\n</code></pre> </li> <li> <p>Copy the damBreak tutorial case into the <code>$WM_PROJECT_USER_DIR</code> and go into the folder damBreak:</p> Text Only<pre><code>cp -r $FOAM_TUTORIALS/multiphase/interFoam/laminar/damBreak/damBreak $WM_PROJECT_USER_DIR/\ncd $WM_PROJECT_USER_DIR/damBreak\npwd\n</code></pre> </li> <li> <p>Now we can run the OpenFOAM case step-by-step or as a batch job.</p> Text Only<pre><code>srun --partition=common -t 2:10:00 -\u2212pty bash\nblockMesh\nsetFields\ninterFoam\n</code></pre> </li> </ol> <p>NB: Do not use the <code>Allrun</code> script(s) of the tutorials, as these may try to launch parallel jobs without requesting resources.</p> <ol> <li> <p>Visualize the results (create <code>case.foam</code> file to load in ParaView):</p> Text Only<pre><code>touch case.foam\nparaview\n</code></pre> </li> <li> <p>Open <code>case.foam</code> in ParaView.</p> </li> </ol>"},{"location":"engineering/openfoam.html#interactive-single-process","title":"Interactive single process","text":"<p>For a non-parallel run of the tutorial case, the <code>decomposeParDict</code> needs to be removed from <code>system</code> directory:</p> Text Only<pre><code>mv system/decomposeParDict system/decomposeParDict-save\n</code></pre> <p>Running the damBreak case step-by-step interactively:</p> Text Only<pre><code>module load rocky8-spack\nmodule load openfoam\n\nsrun --partition=common -t 2:10:00 -\u2212pty bash \nblockMesh\nsetFields\ninterFoam\n</code></pre>"},{"location":"engineering/openfoam.html#batch-job-non-interactive-parallel-job","title":"Batch-job (non-interactive) parallel job","text":"<p>Alternatively, we can run the job in parallel as a batch job (if you removed/renamed the <code>decomposeParDict</code> before, copy  it back by command): </p> Text Only<pre><code>cp system/decomposeParDict-save system/decomposeParDict\n</code></pre> <p>The <code>openfoam.slurm</code> script:</p> Text Only<pre><code>#!/bin/bash -l\n\n#SBATCH -n 4\n#SBATCH -t 00:10:00  \n#SBATCH -J openfoam-damBreak\n#SBATCH --partition=green-ib\n\n#the following 2 lines are only needed if not done manually in command-line\n#before submitting the job\nmodule load rocky8-spack\nmodule load openfoam\n\nblockMesh\ndecomposePar\nsetFields\nsrun interFoam -parallel\nreconstructPar\n</code></pre> <p> </p>"},{"location":"engineering/openfoam.html#pre-processing-geometry-and-mesh-generation","title":"Pre-processing (geometry and mesh generation)","text":"<p>The geometry and mesh can be either hand-coded using blockMesh or with Gmsh, FreeCAD or Salome. When using Gmsh, be sure to save the mesh in v2 ASCII format (see separate page on CAD-mesh. This creates a volume mesh.</p> <p>To convert a Gmsh volume <code>.msh</code> file for OpenFOAM, use</p> Text Only<pre><code>gmshToFoam meshfile.msh\n</code></pre> <p>Another possibility is to use CAD for a surface mesh and use the snappyHexMesh utility to adapt a blockMesh volume mesh to the surface (see OpenFOAM motorcycle tutorial).</p> <p> </p>"},{"location":"engineering/openfoam.html#visualizing-the-results-post-processing","title":"Visualizing the results (post-processing)","text":"<ol> <li>Login to viz (manual can be found here).</li> <li>Change to the case directory.</li> <li> <p>Create an empty <code>.foam</code> file for the case:</p> Text Only<pre><code>touch damBreak.foam\n</code></pre> </li> <li> <p>then use the regular ParaView:</p> Text Only<pre><code>paraview\n</code></pre> </li> <li> <p>Open the <code>.foam</code> file from the menu.</p> </li> </ol> <p> </p>"},{"location":"engineering/openfoam.html#comparison-of-the-execution-time","title":"Comparison of the execution time","text":"<p>It is educational to check the runtime of the code using the <code>time</code> command, e.g. for the single-thread</p> Text Only<pre><code>time interFoam\n</code></pre> <p>and for the parallel run (in the <code>openfoam.slurm</code> script)</p> Text Only<pre><code>time mpirun -n $SLURM_NTASKS interFoam -parallel\n</code></pre> <p>As the damBreak case is quite small, it is likely that the parallel run is not faster than the sequential, due to the communication overhead.</p> <p>In a testrun, the resuls have been as follows:</p> time type sequential parallel real 0m8.319s 0m39.463s user 0m6.927s 1m1.755s sys 0m0.432s 0m2.922s <p>Lesson to be learned: Parallel computation is only useful for sufficiently large jobs.</p> <p>NOTE: Parallel does not (necessarily) mean faster!!! Parallel execution introduces overhead (starting threads, communication)! For optimal execution time and optimal use of resources one needs to test and find the sweet spot.</p> <p></p> <p>The division into the areas is a combined decision taking into account \"real\" (wall clock) and \"user\" (summed time of all threads) time (from the <code>time</code> command). \"Wall clock\" (real) time is the time one needs to wait till the job is finished, \"Summed thread time\" (user) is the sum of the times that all individual threads needed, it should be roughly user = numtreads x real. For parallel programs, one can expect that \"user\" time of the parallel run is larger than for the sequential, due to communication overhead, if it is smaller, that probably means the individual threads could make better use of cache.</p> area why explanation sweet spot minimal \"user\" time = minimal heat production, optimal use of resources good range linear speedup for \"real\", with constant or slightly increasing \"user\" OK range slightly less than linear speedup for \"real\", and slightly increasing \"user\" avoid ascending slope in the diagram for \"real\" and \"user\" one actually needs to wait longer compared to the case with fewer cores <p>Recommended in this case would be to request 8 threads <code>-n 8 --ntasks-per-node 8</code> but use <code>mpirun -n 4</code>. OpenFOAM does not seem to benefit from hyperthreading.</p> <p> </p>"},{"location":"engineering/openfoam.html#some-errors-and-how-to-solve-them","title":"Some errors and how to solve them","text":"<ul> <li>\"slurmstepd: error: Detected 1oom-kill event(s) in \": this is a SLURM out-of-memory error: solve by increasing the memory request <code>--mem=xxGB</code> where xx is something larger than before</li> <li>a \"Bus error\" means the software tries to access non-existing memory, this is actually a SLURM out-of-memory error: solve by increasing the memory request <code>--mem=xxGB</code> where xx is something larger than before</li> <li>infiniband error: wrong partition, the nodelist contains non-infiniband nodes; or wrong openmpi module</li> <li></li> </ul>"},{"location":"engineering/swan.html","title":"Swan","text":"<p>not changed to rocky yet</p>"},{"location":"engineering/swan.html#swan","title":"Swan","text":"<p>SWAN is a third-generation wave model for obtaining realistic estimates of wave parameters in coastal areas, lakes and estuaries from given wind, bottom and current conditions. The model is based on the wave action balance equation with sources and sinks. SWAN allows to use two types of grids (structured and unstructured) and nesting approach. SWAN  is not recommended to use in on ocean scales.</p> <p>Manual can be found here and more about SWAN settings can be found here. </p>"},{"location":"engineering/swan.html#quickstart","title":"Quickstart","text":"<ol> <li> <p>Load the module</p> Text Only<pre><code>module load green/all\nmodule load Swan\n</code></pre> </li> <li> <p>Copy the examples</p> Text Only<pre><code>cp -r $SWANDIR/../Examples Swan-examples\n</code></pre> </li> <li> <p>Unpack one example</p> Text Only<pre><code>cd Swan-examples\ntar xf refrac.tar.gz\ncd refrac\n</code></pre> </li> <li> <p>Download the swan.slurm script</p> Text Only<pre><code>curl https://docs.hpc.taltech.ee/engineering/swan.slurm\n</code></pre> </li> <li> <p>Adjust <code>ntasks</code>, <code>nodes</code> and <code>cpus-per-task</code> submit it:</p> Text Only<pre><code>sbatch swan.slurm\n</code></pre> <p>Contents of the <code>swan.slurm</code>:</p> <p>#!/bin/bash    #SBATCH --partition=green-ib  ### Partition    #SBATCH --job-name=Swan   ### Job Name           -J    #SBATCH --time=00:10:00       ### WallTime           -t    #SBATCH --nodes=1             ### Number of Nodes    -N     #SBATCH --ntasks-per-node=2   ### Number of tasks (MPI processes)    #SBATCH --cpus-per-task=4     ### Number of threads per task (OMP threads)    #SBATCH --mem-per-cpu=100     ### Min RAM required in MB</p> <p>#set up enviroment      module load green/all    module load Swan</p> <p>export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK          MACHINEFILE=\"machinefile\"</p> <p># Generate Machinefile for mpi such that hosts are in the same order as if run via srun    srun -l /bin/hostname | sort -n | awk '{print $2}' &gt; $MACHINEFILE</p> <p># do the swan simulation    swanrun -input a11refr.swn -mpi $SLURM_NTASKS -omp=$SLURM_CPUS_PER_TASK</p> <p>#rm $MACHINEFILE   </p> </li> </ol>"},{"location":"engineering/wam.html","title":"Wam","text":"<p>not changed to rocky yet</p>"},{"location":"engineering/wam.html#wam-cycle-6","title":"WAM Cycle 6","text":"<p>WAM is a third-generation wave model that describes the evolution of the wave spectrum by solving the wave energy transfer equation. WAM predicts wave direction spectra and properties and can be linked to a number of other models.</p> <p>A repository of the source code with modifications for Taltech HPC, can be found here.</p> <p> How to cite  </p> <p>The WAM Model\u2014A Third Generation Ocean Wave Prediction Model DOI: https://doi.org/10.1175/1520-0485(1988)018&lt;1775:TWMTGO&gt;2.0.CO;2 </p>"},{"location":"engineering/wam.html#quickstart","title":"Quickstart","text":""},{"location":"engineering/wam.html#short-jobs-one-core-jobs","title":"Short jobs &amp; one core jobs","text":"<ol> <li> <p>To run your first calculations, start a session on a node:</p> Text Only<pre><code>srun -t 2:0:0 --pty bash\n</code></pre> </li> <li> <p>Enter the following commands to set up environment and working directory:</p> Text Only<pre><code>module load green/all\nmodule load WAM\n\nexport WORK=$HOME/newwamtest\n\nmkdir --parent ${WORK}/tempsg\n\ncd ${WORK}/tempsg\ncp ${WAMDIR}/const/TOPOCAT.DAT .\ncp ${WAMDIR}/const/Coarse_Grid/ARD/Preproc_User .\npreproc\n\ncp ${WAMDIR}/const/Coarse_Grid/ARD/WAM_User .\ncp ${WAMDIR}/const/WIND_INPUT.DAT .\n</code></pre> </li> <li> <p>Run WAM</p> Text Only<pre><code>mpirun wam\n\ncp ${WAMDIR}/const/Coarse_Grid/ARD/Grid_User .\npgrid\n\ncp ${WAMDIR}/const/Coarse_Grid/ARD/Time_User .\nptime\n\ncp ${WAMDIR}/const/Coarse_Grid/ARD/Spectra_User .\npspec\n\ncp ${WAMDIR}/const/Coarse_Grid/ARD/Time_User_S .\nptime_S\n\ncp ${WAMDIR}/const/Coarse_Grid/ARD/nlnetcdf NETCDF_User\nmpirun pnetcdf\n</code></pre> </li> <li> <p>Adapt the WORKDIR, LOGDIR and output directories to your needs!</p> </li> <li> <p>If calculations are going normally, you should have the following files in your <code>$WORK</code> directory:</p> Text Only<pre><code>BLS19780907060000  Grid_Prot              OUT19780907060000  Time_Prot_S\nBLS19780908060000  Grid_User              OUT19780908060000  Time_User\nC0119780906060000  Grid_info_COARSE_GRID  Preproc_Prot       Time_User_S\nC0119780907060000  MAP19780906060000      Preproc_User       WAM_Prot\nC0119780908060000  MAP19780907060000      Spectra_Prot       WAM_User\nC0219780906060000  MAP19780908060000      Spectra_User       WAVE1978090606.nc\nC0219780907060000  NETCDF_User            TOPOCAT.DAT        WIND_INPUT.DAT\nC0219780908060000  OUT19780906060000      Time_Prot          pnetcdf_prot\n</code></pre> </li> <li> <p>To visualise results you can open the <code>WAVE*.nc</code> file for example in Octave or Matlab.</p> Text Only<pre><code>pkg load netcdf\nnetcdf_open('WAVE1978090606.nc')\nncdisp('WAVE1978090606.nc')\nhmax = ncread(\"WAVE1978090606.nc\",'hmax_st')\n%plot field at timestep 12\npcolor(hmax(:,:,12))\n\nhs_swell = ncread(\"WAVE1978090606.nc\",'hs_swell')\n%plot timeseries at position 20 20\nplot(hs_swell(20,20,:))\n</code></pre> </li> </ol>"},{"location":"engineering/wam.html#long-parallel-jobs","title":"Long &amp; parallel jobs","text":"<p>Longer running and parallel jobs are better submitted as batch jobs using an sbatch script wam.slurm:</p> Click to expand      #!/bin/bash     #SBATCH --job-name=WAM-testrun     #SBATCH --mem-per-cpu=1GB     #SBATCH --nodes=1     #SBATCH --ntasks=2     #SBATCH --cpus-per-task=1     #SBATCH -t 0-01:0:00     #SBATCH --partition=green-ib     #SBATCH --no-requeue      module load green     module load WAM      export WORK=$HOME/newwamtest      mkdir --parent ${WORK}/tempsg     mkdir --parent ${WORK}/work      cd ${WORK}/tempsg     cp ${WAMDIR}/const/TOPOCAT.DAT .     cp ${WAMDIR}/const/Coarse_Grid/ARD/Preproc_User .     preproc      cp ${WAMDIR}/const/Coarse_Grid/ARD/WAM_User .     cp ${WAMDIR}/const/WIND_INPUT.DAT .      mpirun wam      cp ${WAMDIR}/const/Coarse_Grid/ARD/Grid_User .     pgrid      cp ${WAMDIR}/const/Coarse_Grid/ARD/Time_User .     ptime      cp ${WAMDIR}/const/Coarse_Grid/ARD/Spectra_User .     pspec      cp ${WAMDIR}/const/Coarse_Grid/ARD/Time_User_S .     ptime_S      cp ${WAMDIR}/const/Coarse_Grid/ARD/nlnetcdf NETCDF_User     mpirun pnetcdf  <p></p>"},{"location":"engineering/wam.html#wam-long-version","title":"WAM long version","text":""},{"location":"engineering/wam.html#starting-calculations","title":"Starting calculations","text":"<p>If job is small it can be run as an interactive session:</p> Text Only<pre><code>srun -t 2:0:0 --pty bash\n</code></pre> <p>If calculation is long or needs several cores, it is better to gather all needed commands in one wam.slurm batch script and submit it by command:</p> Text Only<pre><code>sbatch wam.slurm\n</code></pre>"},{"location":"engineering/wam.html#preparation","title":"Preparation","text":"<ol> <li> <p>Firstly, user needs to load proper environment by commands:</p> Text Only<pre><code>module load green\nmodule load WAM\n</code></pre> </li> <li> <p>After it is needed to determine working directory and go into it </p> Text Only<pre><code>export WORK=$HOME/newwamtest\nmkdir --parent ${WORK}/tempsg\ncd ${WORK}/tempsg\n</code></pre> </li> <li> <p>Copy into working directory all needed data, for example:</p> Text Only<pre><code>cp ${WAMDIR}/const/TOPOCAT.DAT .\ncp ${WAMDIR}/const/Coarse_Grid/ARD/Preproc_User .\npreproc\n\ncp ${WAMDIR}/const/Coarse_Grid/ARD/WAM_User .\ncp ${WAMDIR}/const/WIND_INPUT.DAT .\n</code></pre> </li> </ol>"},{"location":"engineering/wam.html#running-wam","title":"Running WAM","text":"<p>WAM calculations can be started by command <code>WAM</code>.</p> Text Only<pre><code>mpirun wam\n</code></pre> <p>To run calculations normally, such parameters as grid (<code>pgrid</code>), time (<code>ptime</code>), spectra (<code>pspec</code>) time step (<code>ptime_S</code>) and (<code>pnetcdf</code>) should be defined or copy from example:</p> Text Only<pre><code>cp ${WAMDIR}/const/Coarse_Grid/ARD/Grid_User .\npgrid\n\ncp ${WAMDIR}/const/Coarse_Grid/ARD/Time_User .\nptime\n\ncp ${WAMDIR}/const/Coarse_Grid/ARD/Spectra_User .\npspec\n\ncp ${WAMDIR}/const/Coarse_Grid/ARD/Time_User_S .\nptime_S\n\ncp ${WAMDIR}/const/Coarse_Grid/ARD/nlnetcdf NETCDF_User\nmpirun pnetcdf\n</code></pre> <p>If calculations are going normally, you should have the following files in your <code>$WORK</code> directory:</p> Text Only<pre><code>BLS19780907060000  Grid_Prot              OUT19780907060000  Time_Prot_S\nBLS19780908060000  Grid_User              OUT19780908060000  Time_User\nC0119780906060000  Grid_info_COARSE_GRID  Preproc_Prot       Time_User_S\nC0119780907060000  MAP19780906060000      Preproc_User       WAM_Prot\nC0119780908060000  MAP19780907060000      Spectra_Prot       WAM_User\nC0219780906060000  MAP19780908060000      Spectra_User       WAVE1978090606.nc\nC0219780907060000  NETCDF_User            TOPOCAT.DAT        WIND_INPUT.DAT\nC0219780908060000  OUT19780906060000      Time_Prot          pnetcdf_prot\n</code></pre>"},{"location":"engineering/wam.html#visualisation","title":"Visualisation","text":"<p>To visualise results you can open the <code>WAVE*.nc</code> file for example in Octave or Matlab</p> Text Only<pre><code>    pkg load netcdf\n    netcdf_open('WAVE1978090606.nc')\n    ncdisp('WAVE1978090606.nc')\n    hmax = ncread(\"WAVE1978090606.nc\",'hmax_st')\n    %plot field at timestep 12\n    pcolor(hmax(:,:,12))\n\n    hs_swell = ncread(\"WAVE1978090606.nc\",'hs_swell')\n    %plot timeseries at position 20 20\n    plot(hs_swell(20,20,:))\n</code></pre>"},{"location":"lumi/examples.html","title":"Slurm scripts","text":"<p>Like at HPC, at LUMI, computing resources are allocated to the user by the resource manager Slurm. More about Slurm scripts at LUMI can be found here.</p> <ul> <li> <p>At LUMI partitions can be allocated by node or by resources.</p> </li> <li> <p>      User always has to specifying the account.            It is mandatory!</p> <p>Account specification can be done by adding into Slurm script <code>#SBATCH --   account=project_XXX</code> line or by adding the following two lines into <code>.bashrc</code> file by command:</p> Text Only<pre><code>cat  &lt;&lt;EOT &gt; .bashrc\nexport SBATCH_ACCOUNT=project_XXX\nexport SALLOC_ACCOUNT=project_XXX\nEOT\n</code></pre> <p>where <code>XXX</code> is a project number which can be found in ETAIS as <code>Effective ID</code>.</p> </li> <li> <p>By default,       upon node failure job will be automatically resubmitted to the queue            with the same job ID and that will truncate the previous output. To avoid this add the following two lines into <code>.bashrc</code> file by command:</p> Text Only<pre><code>cat  &lt;&lt;EOT &gt; .bashrc\nSBATCH_NO_REQUEUE=1 \nSBATCH_OPEN_MODE=append\nEOT\n</code></pre> </li> </ul> <p>More about Slurm options can be found in LUMI manuals.</p>  Slurm script examples provided by LUMI:  - [GPU jobs](https://docs.lumi-supercomputer.eu/runjobs/scheduled-jobs/lumig-job/) - [CPU jobs](https://docs.lumi-supercomputer.eu/runjobs/scheduled-jobs/lumic-job/) - [Job array](https://docs.lumi-supercomputer.eu/runjobs/scheduled-jobs/throughput/)  <p> </p>"},{"location":"lumi/examples.html#multi-node-multi-gpu-pytorch-training","title":"Multi Node Multi GPU PyTorch Training","text":"<p>This PyTorch script simulates training a ResNet model across multiple gpus and nodes.</p>"},{"location":"lumi/examples.html#quick-guide","title":"Quick Guide","text":"<ol> <li> <p> Download: <ul> <li>environment setup script - env.sh </li> <li>bash script setup singularity - setup.sh</li> <li>PyTorch script - min_dist.py</li> <li>slurm script - dist_run.slurm </li> </ul> <li> <p>Setup environment by command: </p> Text Only<pre><code>. env.sh project_XXX\n</code></pre> <p>where <code>XXX</code> is a project number.</p> </li> <li> <p>Setup singularity:</p> Text Only<pre><code>./setup.sh\n</code></pre> </li> <li> <p>Run PyTorch script:</p> Text Only<pre><code>sbatch -N 2 dist_run.slurm min_dist.py\n</code></pre> </li> <li> <p>You should get an output file <code>slurm_job-number</code> with content like:</p> Text Only<pre><code>8 GPU processes in total\nBatch size = 128\nDataset size = 50000\nEpochs = 5\n\nEpoch 0  done in 232.64820963301463s\nEpoch 1  done in 31.191600811027456s\nEpoch 2  done in 31.244039460027125s\nEpoch 3  done in 31.384101407951675s\nEpoch 4  done in 31.143528194981627s\n</code></pre> </li> <p></p>"},{"location":"lumi/examples.html#detailed-guide","title":"Detailed Guide","text":"Download:  - environment setup script - [env.sh](env.sh)  - bash script setup singularity - [setup.sh](setup.sh) - PyTorch script - [min_dist.py](min_dist.py) - slurm script - [dist_run.slurm](dist_run.slurm)"},{"location":"lumi/examples.html#setup","title":"Setup","text":"<p>This commands will setup environment and singularity</p> Text Only<pre><code>. env.sh project_XXX\n./setup.sh\n</code></pre> <p>where <code>XXX</code> is a project number that should be changed according user's project number.</p>"},{"location":"lumi/examples.html#running","title":"Running","text":"<p>Job can be submitted into queue by command </p> Text Only<pre><code>sbatch -N 2 dist_run.slurm min_dist.py\n</code></pre> <p>Where  <code>dist_run.slurm</code> is a resource manager, <code>min_dist.py</code> is a PyTorch script and <code>-N</code> in a number of nodes used. </p> Bash<pre><code>#!/bin/bash\n#SBATCH --job-name=DIST\n#SBATCH --time=10:00\n#SBATCH --mem 64G\n#SBATCH --cpus-per-task 32\n#SBATCH --partition small-g\n#SBATCH --gpus-per-node 4\n\nexport NCCL_SOCKET_IFNAME=hsn\nexport NCCL_NET_GDR_LEVEL=3\n\nexport CXI_FORK_SAFE=1\nexport CXI_FORK_SAFE_HP=1\nexport FI_CXI_DISABLE_CQ_HUGETLB=1\n\nexport MIOPEN_USER_DB_PATH=/tmp/${USER}-miopen-cache-${SLURM_JOB_ID}\nexport MIOPEN_CUSTOM_CACHE_DIR=${MIOPEN_USER_DB_PATH}\n\nexport MASTER_ADDR=$(scontrol show hostname $SLURM_NODELIST | head -n1)\n\nexport OMP_NUM_THREADS=8\n\nsrun singularity exec --rocm \\\n    $SCRATCH/pytorch_latest.sif \\\n    torchrun --nnodes=$SLURM_NNODES \\\n    --nproc_per_node=$SLURM_GPUS_PER_NODE \\\n    --rdzv_id=$SLURM_JOB_ID \\\n    --rdzv_backend=c10d \\\n    --rdzv_endpoint=$MASTER_ADDR $@\n</code></pre> <p>The environment variables containing <code>NCCL</code> and <code>CXI</code> are used by RCCL for communication over Slingshot.</p> <p>The ones containing <code>MIOPEN</code> are for MIOpen to create its caches in the <code>/tmp</code> (which is local to each node and in memory). If this is not set then MIOpen will create its cache in the user home directory (the default) which is a problem since each node needs its own cache.</p>"},{"location":"lumi/software.html","title":"Programs on LUMI","text":"LUMI offers a wide range of programs in different fields:  - Machine learning on top of PyTorch and Tensorflow - Image classification - ResNet - Object recognition and XFMR translation - SSD, XFMR - Scientific software suites - Gromacs (molecular dynamics), CP2K (quantum chemistry) and ICON (climate science) - Weather prediction  application - GridTools allowing  measures stencil-based - etcetera  <p>More information about installed software and how to install software yourself can be found in the LUMI documentation.</p> <p>The list of available programs can be found in LUMI Software Library. There can be also found license information - whether the program is free to use, requires pre-registration, or the user must provide own license. </p> <p> </p>"},{"location":"lumi/software.html#first-time-use","title":"First time use","text":"<p>To be able to use a program user has first to install it. Installation can be done by spack or by EasyBuild. The list of available programs in EasyBuild can be found in LUMI Software Library as well as installation instructions. List of programs which can be installed by spack can be looked by <code>spack list</code> command. The same program can be installed by both spack and EasyBuild.</p> <p>There are two possible places where programs can be installed - user's <code>$HOME</code> or project directory. The latest is recommended, sines other user of the project will be able to use installed programs as well. Moreover <code>$HOME</code> size is limited by 20GB. More about data storage at LUMI and storage billing.</p> <p>Project <code>XXX</code> number can be found in ETAIS as <code>Effective ID</code>.</p> <p></p> <p></p> <p>It is a good practice to add place, where programs will be installed into your .profile or .bashrc file. To do this give a command:</p> Text Only<pre><code>echo  \"export EBU_USER_PREFIX=/project/project_XXX\" &gt;&gt; .bashrc\n</code></pre> <p>where <code>XXX</code> is a project number.</p> <p>To install programs into use the following commands:</p> Text Only<pre><code>export EASYBUILD PREFIX=$HOME/easybuild \nexport EASYBUILD BUILDPATH=/tmp/$USER\n</code></pre> <p> </p>"},{"location":"lumi/software.html#program-installation","title":"Program installation","text":""},{"location":"lumi/software.html#installation-by-spack","title":"Installation by Spack","text":"<ol> <li> <p>Initialize spack:</p> Text Only<pre><code>export SPACK_USER_PREFIX=/project/project_XXX/spack \nmodule list\n</code></pre> <p>where project <code>XXX</code> number can be found in ETAIS as <code>Effective ID</code>. </p> <p>after user should load the following modules:</p> Text Only<pre><code>module load LUMI/YYY  partition/ZZZ \nmodule load spack/RRR\n</code></pre> <p>where <code>YYY</code>  - version of LUMI, will appear in module list. Partition <code>ZZZ</code> is determined depending on CPUs (partition/C) or GPUs (partition/G) will be used. <code>RRR</code> - version of spack, will appear in the module list. </p> <p></p> </li> <li> <p>The entire list of programs available for installation by spack, can be viewed by command:</p> Text Only<pre><code>spack list\n</code></pre> <p>The list will be too long, so better to search for certain program by command:</p> Text Only<pre><code>spack list program_name\n</code></pre> <p>where the whole name or part of it is given.</p> <p>NB! spack is insensitive to caps.</p> </li> <li> <p>Check what flags should be added: </p> Text Only<pre><code>spack info program_name\n</code></pre> <p></p> </li> <li> <p>Program installation is made by command:</p> Text Only<pre><code>spack install program_name@version%compiler@version +flag(s) ^forced_dependencies\n</code></pre> <p>where <code>flag</code> is an installation options taken from variants of spack info see above. It is recommended to try the <code>cce</code> (Cray Compiler Edition) and for MPI dependent software to force the <code>cray-mpich</code> dependency.</p> <p>for example:</p> Text Only<pre><code>spack install nwchem@7.0.2%cce@15.0.1 +openmp ^cray-mpich@8.1.25\n</code></pre> <p>or </p> Text Only<pre><code>spack install kokkos+rocm+debug_bounds_check amdgpu_target=gfx90a %gcc@11.2.0\n\nRefresh the module list\n\n        spack module tcl refresh -y\n</code></pre> <p>For more details see LUMI guide.</p> <p>NB! Program installation will require time up to hours.</p> </li> <li> <p>When program is already installed user should load it before use by commands:</p> Text Only<pre><code>module load program_name\n</code></pre> </li> </ol> <p></p>"},{"location":"lumi/software.html#installation-by-easybuild","title":"Installation by EasyBuild","text":"<ol> <li> <p>To install program with EasyBuild, initialise it by following commands:</p> Text Only<pre><code>module use /projappl/project_XXXX/easybuild/modules/all\nmodule list\n</code></pre> <p>where project <code>XXX</code> number can be found in ETAIS as <code>Effective ID</code>. </p> <p>after user should load the following modules:</p> Text Only<pre><code>module load LUMI/YYY  partition/ZZZ \nmodule load EasyBuild-user\n</code></pre> <p>where <code>YYY</code> - version of LUMI that can be found at program's page in LUMI Software Library. Sometimes partition <code>ZZZ</code> is determined in the description of the program in LUMI Software Library. In case it is not, partition <code>ZZZ</code> is used depending on CPUs (partition/C) or GPUs (partition/G) will be used.</p> <p></p> </li> <li> <p>After EasyBuild is loaded user can install the program needed by command <code>eb</code>.</p> Text Only<pre><code>eb `program_eb_file`\n</code></pre> <p>NB! The full name of <code>program_eb_file</code> as well as some additional flags needed for installation can be found at program's page in LUMI Software Library.</p> <p>NB! Program installation will require time up to an hour.</p> </li> <li> <p>When program is already installed user should load it before use by commands:</p> Text Only<pre><code>module load program_name\n</code></pre> </li> </ol> <p> </p>"},{"location":"lumi/software.html#loading-program-adding-modules-into-slurm","title":"Loading program &amp; adding modules into slurm","text":"<p>When program is already installed, user should load it before use or add into slurm script. If program was installed by spack the following commands should be given:</p> Text Only<pre><code>export SPACK_USER_PREFIX=/project/project_XXX/spack\nmodule load LUMI/YYY  partition/ZZZ \nmodule load spack/YYY\nmodule load program_name/VVV\n</code></pre> <p>where <code>XXX</code> is a project number, and can be found in ETAIS as <code>Effective ID</code>. <code>YYY</code>  - version of LUMI, will appear in module list. Partition <code>ZZZ</code> is determined depending on CPUs (partition/C) or GPUs (partition/G) will be used. <code>RRR</code> - version of spack, will appear in the module list. <code>VVV</code> - version of program, will appear in the module list.</p> <p>if programs was installed by EasyBuild the following commands should be given:</p> Text Only<pre><code>module use /projappl/project_465000338/easybuild/modules/all\nmodule load LUMI/YYY  partition/ZZZ \nmodule load EasyBuild/\nmodule load program_name/VVV\n</code></pre> <p>where <code>XXX</code> is a project number, and can be found in ETAIS as <code>Effective ID</code>. <code>YYY</code>  - version of LUMI, will appear in module list. Partition <code>ZZZ</code> is determined depending on CPUs (partition/C) or GPUs (partition/G) will be used. <code>RRR</code> - version of spack, will appear in the module list. <code>VVV</code> - version of program, will appear in the module list.</p> <p>Examples of slurm scripts can be found here.</p> <p> </p>"},{"location":"lumi/software.html#uninstalling-programs","title":"Uninstalling programs","text":""},{"location":"lumi/start.html","title":"LUMI","text":""},{"location":"lumi/start.html#getting-started-on-lumi","title":"Getting Started on LUMI","text":"<p>If you face any problems during registration to LUMI give us know by email \"hpcsupport@taltech.ee\" or in Teams in  \"HPC Support Chat\". </p> <ol> <li> <p>Login to minu.etais.ee with MyAccessID.</p> <p> <p></p> <li> <p>Choose your affiliation (ttu.ee)</p> <p> <p></p> <li> <p>Identify yourself with Uni-ID (six letters taken from the user\u2019s full name), but for longtime employees it could be name.surname. </p> <p> <p></p> <li> <p>Agree with all propositions and press button to continue.  </p> <p> <p></p> <li> <p>After notification of successful account creation you need to open a new tab and go to mms.myaccessid.org.  </p> <p></p> <p>After filling in fields needed and agreement with use policy and terms of use, press button \"Submit\".</p> </li> <li> <p>Register your SSH key in MyAccessID. To do this click to <code>Manage SSH key</code>. </p> <p></p> <p>Then add your SSH key into corresponding field. In Linux and Mac public SSH key can be found in <code>.ssh/id_rsa.pub</code> file. Windows by default saves public SSH key at <code>C:\\Users\\your_username/.ssh/id_rsa.pub</code>. How to generate SSH keys can be found here.</p> <p></p> <p>It is important to add your SSH key immediately after account creation, since this SSH key will be automatic transfer to LUMI and used for user authentication during first and subsequent connection to LUMI.</p> </li> <li> <p>If you are a project leader -- Contact to Resource Allocator (<code>hpcsupport@taltech.ee</code>). HPC Centre will add LUMI resources to your account. If name of the project is already known add it as well.   If you are a team member, -- contact your project leader (or course teacher) to be added to a project.</p> <p>NB! Just adding HPC-LUMI resources to an existing project will not work.</p> </li> <li> <p>After you receive an answer from HPC Centre, login to minu.etais.ee account.</p> </li> <li> <p>The corresponding project appears in your ETAIS account.  </p> <p></p> </li> <li> <p>In short time you will receive a letter from <code>info-noreply@csc.fi</code> where you will be given your user name for LUMI.</p> </li> <li> <p>Try to connect to LUMI with received user name by the command:</p> Text Only<pre><code>ssh LUMI-user-name@@lumi.csc.fi\n</code></pre> <p>NB! Synchronization may take some time, so if you have problems with connection with SSH key, it should wait longer and try again.</p> </li> <li> <p>Read the documentation on the LUMI homepage https://docs.lumi-supercomputer.eu/firststeps/getstarted/. Especially check the different filesystems and their prices, since LUMI charges TB/hour, as well as the guide for containerization of python environments (pip, conda).</p> </li>"},{"location":"visualization/VirtualGL.html","title":"VirtualGL","text":"<p>not changed to rocky yet</p>"},{"location":"visualization/VirtualGL.html#remote-visualization-using-virtualgl","title":"Remote visualization using VirtualGL","text":"The client (your desktop) computer needs X11 and the VirtualGL software:  -   Linux:      -   X11 is default, if you have a graphical desktop, you have X11     -   download your VirtualGl package from  -   Windows:     -   Cygwin      -   use the Cygwin installer to install Cygwin/x and VirtualGL -   Mac:      -   XQuarts needs to be installed      -   download your VirtualGl package from  <p>Any recent VGL client version should work (vis-node has 2.5.2). If there is no native package for your Linux distribution, you can download the .deb and unpack it using <code>dpkg -x virtualgl...deb vgl</code>. The programs you need are in <code>vgl/opt/VirtualGL/bin/</code>.</p> <p>Connect to the visualization node using <code>vglconnect</code> from the VirtualGL package</p> Text Only<pre><code>vglconnect -s uni-ID@viz.hpc.taltech.ee\n</code></pre> <p>and run the visualization application you want to use, e.g. ParaView or VisIt:</p> Text Only<pre><code>vglrun paraview\n</code></pre>"},{"location":"visualization/vnc.html","title":"Vnc","text":"<p>not changed to rocky yet</p>"},{"location":"visualization/vnc.html#remote-visualization-using-vnc","title":"Remote visualization using VNC","text":""},{"location":"visualization/vnc.html#short-guide","title":"Short guide","text":"<ol> <li> <p>Connect to viz:</p> Text Only<pre><code>ssh uni-ID@viz.hpc.taltech.ee\n</code></pre> </li> <li> <p>Start VNC on viz by command:</p> Text Only<pre><code>vncserver -geometry 1265x980\n</code></pre> </li> <li> <p>Open a second connection to viz:</p> Text Only<pre><code>ssh -L 59XX:localhost:50XX uni-ID@viz.hpc.taltech.ee\n</code></pre> <p>where <code>XX</code> is the display number that appear after giving <code>vncserver</code> command.  NB! <code>XX</code> is always the number of two digits (e.g. <code>01</code> for <code>:1</code>)</p> </li> <li> <p>On your desktop start a vncviewer</p> Text Only<pre><code>vncviewer :XX\n</code></pre> <p>where <code>XX</code> is the display number</p> </li> <li> <p>Stop the vncserver!!! on viz by command:</p> Text Only<pre><code>vncserver -kill :XX\n</code></pre> </li> </ol> <p> </p>"},{"location":"visualization/vnc.html#get-started","title":"Get started","text":""},{"location":"visualization/vnc.html#software-recommended-to-use","title":"Software recommended to use","text":"<p>Virtual Network Computing (VNC) is a graphical desktop-sharing system to remotely control another computer.</p>  The client (your desktop) computer needs a vncviewer  -   **Linux:** [xtigervncviewer](https://command-not-found.com/xtigervncviewer) -   **Windows:** TigerVNCviewer: [vncviewer64-1.12.0.exe](https://sourceforge.net/projects/tigervnc/files/stable/1.12.0/vncviewer64-1.12.0.exe/download) -   **Mac:** [VNC Viewer](https://www.realvnc.com/en/connect/download/viewer/)  <p></p>"},{"location":"visualization/vnc.html#first-time-use","title":"First time use","text":"<p>On the first start, VNC asks to specify a password to connect to the server, choose a secure one, which does not match your HPC/UniID password because VNC connections are not encrypted!</p>    ![vnc-host-0](vnc-host-0.png)   <p> </p>"},{"location":"visualization/vnc.html#vnc-long-version","title":"VNC Long version","text":"<p>VNC should be run firstly at viz node of HPC and after at user's computer.</p> <ol> <li> <p>Connect to viz by command:</p> <p>ssh uni-ID@viz.hpc.taltech.ee</p> <p>if this command does not work try to connect through jump host:</p> Text Only<pre><code>ssh -J uni-ID@base.hpc.taltech.ee uni-ID@viz\n</code></pre> <p>NB! Connection to viz can be done only with SSH keys. SSH key generation guide is here.</p> <p>NB! To use viz the SSH key must be added to the base node.</p> <p>On Mac and Linux this can be done by command:</p> Text Only<pre><code>ssh-copy-id Uni-ID@base.hpc.taltech.ee\n</code></pre> <p>After about an hour, when the automatic script has synced the files, and you can use viz.</p> </li> <li> <p>On viz start the VNC server. Depending on which VNC client user has, one of those commands should be given:</p> Text Only<pre><code>vncserver -geometry 1265x980\n</code></pre> <p>and for Tiger VNC:</p> Text Only<pre><code>tigervncserver -geometry 1280x1024\n</code></pre> <p>It is recommended to specify window size as well by <code>-geometry</code> flag, since changing the resolution of the remote desktop (= window size) at runtime can have undesired effects. </p> </li> <li> <p>The output in the terminal will show on which display VNC is running.</p> <p> <p></p> <p>see second line <code>desktop at :8</code>, where <code>:8</code> is the display number -- further <code>XX</code>.</p> <li> <p>Open a second connection to viz (in new terminal) and give the command:</p> Text Only<pre><code>ssh -L 59XX:localhost:50XX uni-ID@viz.hpc.taltech.ee\n</code></pre> <p>where <code>XX</code> is the display number as two digits (e.g. <code>01</code> for <code>:1</code>)</p> <p>NB! If you were connected through jump host this command should be given:</p> Text Only<pre><code>ssh -J Uni-ID@base.hpc.taltech.ee -L 59XX:127.0.0.1:59XX Uni-ID@viz\n</code></pre> </li> <li> <p>On your desktop start a VNC viewer. If you do it from terminal -- give one of these command depending on which VNC viewer you have:</p> Text Only<pre><code>vncviewer :XX\n</code></pre> <p>or</p> Text Only<pre><code>xtigervncviewer localhost:XX\n</code></pre> <p>where <code>XX</code> is the number from above. On Windows (depending on the VNC-client) the address to connect to could be <code>localhost::50XX</code> (again, the <code>XX</code> stands for the display/port as specified before).</p> </li> <p>If you use graphical interface - specify localhost - in the corresponding field (line in the top) and click bottom \"Continue\". </p>    ![vnc-1](vnc-1.png)   <p>Type password.</p>    ![vnc-2](vnc-2.png)   <p>If you see monochromic field and can not start a session, it mean that you to set up your VNC session: Setting up VNC config.</p> <p>If you see terminal - then everything is done correctly and you can start working. Within the session window, you can start any program from the terminal or using the menus of the window manager.</p> <p>Viz has module system. Most of the modules are needed to be loaded unless the manual says they are native.</p> <p>Before loading modules, the source must be specified:</p> Text Only<pre><code>source /usr/share/lmod/6.6/init/bash\nmodule use /gpfs/mariana/modules/system\n</code></pre> <p>followed by two commands to load the modules. The first one loads viz-spack or viz module, depending on program installation type, and the second command loads the program itself. For example:</p> Text Only<pre><code> module load viz-spack\n module load jmol\n</code></pre>    ![vnc-3](vnc-3.png)   <p>In case of native program only the command that calls this program is needed.</p> Text Only<pre><code>rasmol\n</code></pre> <p>or </p> Text Only<pre><code>paraview\n</code></pre> <p> </p>"},{"location":"visualization/vnc.html#correct-termination","title":"Correct termination","text":"<p>In is very important to finish session correctly! Since if you do not do it - session continue to run even if you close session on your computer. </p> <p>To stop VNC session give on viz one of these commands:</p> Text Only<pre><code>vncserver -kill :XX\n</code></pre> <p>or</p> Text Only<pre><code>tigervncserver -kill :XX\n</code></pre> <p>where <code>XX</code> is the display number.</p> <p>Running sessions can be checked by command:</p> Text Only<pre><code>vncserver -list\n</code></pre>    ![vnc-4](vnc-4.png)   <p> </p>"},{"location":"visualization/vnc.html#setting-up-vnc-session","title":"Setting up VNC session","text":"<p>It is impossible to work with VNC without setting it. To do this give the following commands from home catalogue on base or viz:</p> Text Only<pre><code>cat  &lt;&lt;EOT &gt; .xsession\nxterm &amp;\nfvwm2\nEOT\n</code></pre> <p>This will configure automatic startup of  <code>xterm</code> and  <code>fvwm2</code> window manager. Alternatively, user can use other window managers: more desktop-like -- <code>fluxbox</code>, <code>awesome</code> or <code>jwm</code> or tiling -- <code>i3</code>, <code>xmonad</code> or <code>tritium</code>. To do this, the corresponding line must be added to <code>.xsession</code> file by command:</p> Text Only<pre><code>echo  \"fluxbox\" &gt;&gt; .xsession\n</code></pre> <p>The same way <code>.vnc/xstartup</code> can be configured in case user wants to apply special settings exactly to VNC visualisation.</p>"},{"location":"visualization/x2go.html","title":"X2go","text":"<p>not changed to rocky yet</p>"},{"location":"visualization/x2go.html#remote-visualization-using-x2go-easiest-to-get-working","title":"Remote visualization using X2GO (easiest to get working)","text":"<p>The client (your desktop) computer needs the X2GO-client</p> <p>NB! X2GO uses ssh-key. Ssh-key generation guide is here.</p> <p>NB! To use viz the ssh-key must be added to the base node.</p> <p>On Mac and Linux this can be done by command:</p> Text Only<pre><code>ssh-copy-id Uni-ID@base.hpc.taltech.ee\n</code></pre> <p>After about an hour, when the automatic script has synced the files, and you can use viz.</p> <p> </p>"},{"location":"visualization/x2go.html#configuring-the-client","title":"Configuring the client:","text":"<p>During first use X2GO-client needed to be configured as it is shown at the picture. To configure, select in the upper left corner \"Session\" tab. The setting of the ssh-key is only necessary, if you use a non-standard name or not the default key. </p> <p></p> <p>Large memory consuming Desktop environments like MATE, KDE, GNOME are not available, use window managers like <code>blackbox</code> <code>fluxbox</code> <code>jwm</code> <code>fvwm2</code> <code>awesome</code> <code>lwm</code> <code>fvwm-crystal</code> (last setting on the screen).</p> <p>If you select <code>Terminal</code> as \"Session type\" (use \"fvwm2\" as Command), you will get a \"rootless\" xterm and you can use that to start other software which will appear as individual windows on your regular desktop (not within a remote desktop window).</p> <p>It is also recommended to configure the display settings, for example, as done in the example below or in some other suitable way, since changing the resolution of the remote desktop (= window size) at runtime is not possible (resizing the window would be the equivalent of stretching your physical monitor) or can have other undesired effects. </p> <p></p> <p> </p>"},{"location":"visualization/x2go.html#configuring-the-server-side","title":"Configuring the server-side:","text":"<p>A couple of config files need to be present:</p> <ul> <li><code>$HOME/.fvwm/.fvwm2rc</code> .fvwm2rc </li> <li><code>$HOME/.xsessionrc-x2go</code> .xsessionrc-x2go (can be a link to .xsessionrc, .xsession, or .vnc/xstartup) </li> </ul> <p>If the files are not present, just copy them from <code>/etc/skel/</code> or run</p> Text Only<pre><code>curl https://docs.hpc.taltech.ee/visualization/fvwm2rc.fvwm2rc --create-dirs -o $HOME/.fvwm/.fvwm2rc\ncurl https://docs.hpc.taltech.ee/visualization/xsessionrc-x2go.xsessionrc-x2go --create-dirs -o $HOME/.xsessionrc-x2go\n</code></pre> <p>to copy/save the example configs.</p> <p> </p>"},{"location":"visualization/x2go.html#x2go-run-use","title":"X2GO run &amp; use","text":"<p><code>$HOME</code> on base  coincide with <code>$HOME</code> on viz.</p> <p>When session is configured, press <code>enter</code> to run session. Press left bottom of the mouse to call menu and choose XTerm.</p> <p></p> <p>Will appear terminal where user can call the desired visualization program. We do not maintain the list of software in the menues of window managers or desktop environments, that means even with a graphical frontend, you still need to use the command-line to start your programs! You can configure the menues yourself, e.g. in the <code>$HOME/.fvwm/.fvwm2rc</code> file for the fvwm window manager.</p> <p>Viz has module system. Most of the modules are needed to be loaded unless the manual says they are native.</p> <p>Before loading modules, the source must be specified the source:</p> Text Only<pre><code>source /usr/share/lmod/6.6/init/bash\nmodule use /gpfs/mariana/modules/system\n</code></pre> <p>followed by module load commands, for example:</p> Text Only<pre><code> module load viz-spack\n module load jmol\n</code></pre> <p></p> <p>In case of native program only the command that calls this program is needed.</p> Text Only<pre><code>rasmol\n</code></pre> <p>or </p> Text Only<pre><code>paraview\n</code></pre> <p>NB! ParaView and maybe other software using GLX needs to be started using VirtualGL: <code>vglrun paraview</code></p> <p></p> <p> </p>"},{"location":"visualization/x2go.html#terminate-x2go-run","title":"Terminate X2GO run","text":"<p>It is extremely important to end session in proper way! To do this  1. Print <code>exit</code> in your terminal 2. Click left mouth buttom, call menu and choose <code>Exit fvwm</code>.</p> <p></p> <p></p>"},{"location":"visualization/xpra.html","title":"Xpra","text":"<p>not changed to rocky yet</p>"},{"location":"visualization/xpra.html#remote-visualization-using-xpra","title":"Remote visualization using Xpra","text":"The client (your desktop) computer needs X11 and Xpra:  -   Linux:     -   X11 is default, if you have a graphical desktop, you have X11     -   Xpra should be available in the package manager -   Windows:     -   VcXsrv or Xming or Cygwin/X     -   Xpra client -   Mac:      -   XQuarts needs to be installed ([https://www.xquartz.org/](https://www.xquartz.org/))     -   Xpra client  <p>Unlike VNC, these applications are \"rootless\". They appear as individual windows inside your window manager rather than being contained within a single window.</p> Text Only<pre><code>xpra start ssh://uni-ID@viz.hpc.taltech.ee/ --start-child=xterm\n</code></pre> <p>specifying an ssh-key to use and a jump-host:</p> Text Only<pre><code>xpra start ssh://viz/ --ssh=\"ssh -J base.hpc.taltech.ee\" --start-child=xterm\n</code></pre> <p>re-attach from a different computer:</p> Text Only<pre><code>xpra attach ssh:viz.hpc.taltech.ee\n</code></pre> <p>To stop Xpra, run on the server:</p> Text Only<pre><code>xpra stop\n</code></pre>"}]}