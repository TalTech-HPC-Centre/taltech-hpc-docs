{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to TalTech HPC User Guides page!","text":"<p>The use of the resources of the TalTech HPC Centre requires an active Uni-ID account (an application form for non-employees/non-students can be found here). Further, the user needs to be added to the HPC-USERS group. Please ask hpcsupport@taltech.ee to activate HPC access from your TalTech email and provide your UniID (six letters taken from the user's full name). In the case of using licensed programs, the user must also be added to the appropriate group. More information about available programs and licenses can be found here.</p> <p>TalTech HPC Centre includes cluster, cloud and is also responsible for providing access to the resources of the LUMI supercomputer.</p> <p>The cloud provides users the ability to create virtual machines where the user has full admin rights and can install all the necessary software themselves. VMs can be connected from outside and can be used for providing web services. Accessible through the ETAIS website.</p> <p>The cluster has a Linux operating system (based on CentOS; Debian or Ubuntu on special-purpose nodes) and uses SLURM as a batch scheduler and resource manager. Linux is the dominant operating system used for scientific computing and is the only operating system present in the Top500 list (a list of the 500 most powerful computers in the world). Linux command-line knowledge is essential for using the cluster. Resources on learning Linux can be found in our guide, including introductory lectures in Moodle. However, some graphical interface is available for data visualization, copy, and transfer.</p> <p>LUMI supercomputer is the fastest supercomputer in Europe, the fifth fastest globally and the seventh greenest supercomputer on the planet. Specifications of LUMI can be found here.</p>"},{"location":"#hardware-specification","title":"Hardware Specification","text":"<p>TalTech ETAIS Cloud:</p> <ul> <li>5-node OpenStack cloud</li> <li>5 compute (nova) nodes with 768 GB of RAM and 80 threads each</li> <li>65 TB CephFS storage (net capacity)</li> <li>accessible through the ETAIS website</li> </ul> <p>TalTech cluster base (base.hpc.taltech.ee):</p> <ul> <li>SLURM v23 scheduler, a live load diagram</li> <li>1.5 PB storage, with a 0.5 TB/user quota on $HOME and 2 TB/user quota on SMBHOME</li> <li>32 green nodes, 2 x Intel Xeon Gold 6148 20C 2.40 GHz (40 cores, 80 threads per node), 96 GB DDR4-2666 R ECC RAM (green[1-32]), 25 Gbit Ethernet, 18 of these FDR InfiniBand (green-ib partition)</li> <li>1 mem1tb large memory node, 1 TB RAM, 4x Intel Xeon CPU E5-4640 (together 32 cores, 64 threads)</li> <li>2 ada GPU nodes, 2xNvidia L40/48GB, 2x 32core AMD EPYC 9354 Zen4 (together 64 cores, 128 threads), 1.5 TB RAM</li> <li>amp GPU nodes (specific guide for amp and amp1):</li> <li>amp: 8xNvidia A100/40GB, 2x 64core AMD EPYC 7742 Zen (together 128 cores, 256 threads), 1 TB RAM</li> <li>amp2: 8xNvidia A100/80GB, 2x 64core AMD EPYC 7713 zen3 (together 128 cores, 256 threads), 2 TB RAM</li> <li>Visualization node viz (accessible within University network and FortiVPN, guide for viz): 2xNvidia Tesla K20Xm graphic cards (on displays :0.0 and :0.1), CPU Intel(R) Xeon(R) CPU E5-2630L v2@2.40GHz (24 threads), 64 GB RAM, HDD 2 TB storage.</li> </ul>"},{"location":"#billing","title":"Billing","text":""},{"location":"#virtual-server-hosting","title":"Virtual server hosting","text":"What Unit TalTech internal External CPU CPU*hour 0.002 EUR 0.003 EUR Memory RAM*hour 0.001 EUR 0.0013 EUR Storage TB*year 20 EUR 80 EUR"},{"location":"#taltech-cluster","title":"TalTech cluster","text":"What Unit TalTech internal External CPUcore &amp; &lt; 6 GB RAM CPUcore*hour 0.006 EUR 0.012 EUR CPUcore &amp; &gt; 6 GB RAM 6 GB RAM*hour 0.006 EUR 0.012 EUR GPU GPU*hour 0.20 EUR 0.50 EUR Storage 1 TB*Year 20 EUR 80 EUR <p>More details on how to calculate computational costs for TalTech cluster can be found in the Monitoring resources part of the Quickstart page.</p>"},{"location":"#lumi-cluster-for-users-from-estonia","title":"LUMI cluster for users from Estonia","text":"What Unit Price for TalTech CPUcore CPUcore*hour 0.008 EUR GPU GPU*hour 0.35 EUR User home directory 20 GB free Project storage (persistent and scratch) TB*hour 0.0106 EUR Flash based scratch storage TB*hour 10 x 0.0106 EUR <p>A more detailed guide on how to calculate computational costs for LUMI can be found in the LUMI billing policy.</p>"},{"location":"#slurm-partitions","title":"SLURM partitions","text":"partition default time time limit default memory nodes short 10 min 4 hours 1 GB/thread green, ada, amp common 10 min 8 days 1 GB/thread green green-ib 10 min 8 days 1 GB/thread green long 10 min 22 days 1 GB/thread green gpu 10 min 5 days 1 GB/thread amp, ada bigmem 10 min 8 days 1 GB/thread ada, amp, mem1tb"},{"location":"acknowledgement/","title":"Acknowledgement","text":"<p>When publishing results obtained by using the systems of the HPC Centre, an acknowledgement would be appreciated. It helps emphasize the usefulness and importance of our services and to acquire funding for new systems. Please include a sentence along the lines of:</p> <p>\"The simulations were carried out in the High Performance Computing Centre of TalTech.\"</p>"},{"location":"access/cloud/","title":"TalTech HPC Cloud","text":"<p>With access to the TalTech HPC Cloud, you can create and manage virtual machines tailored to your research or project needs. The cloud environment allows you to request virtual machines for computing resources, where you can install necessary software, and run simulations or analyses.</p>"},{"location":"access/cloud/#access-to-hpc-cloud-short-introduction","title":"Access to HPC Cloud (short introduction)","text":"<p>To gain access to the HPC Centre Cloud, follow these steps:</p> <ol> <li>Log in to ETAIS and authenticate with your TalTech UniID credentials via MyAccessID.</li> <li>Upload your SSH Public Key to the ETAIS Portal.</li> <li>Contact us by email (hpcsupport@taltech.ee), in Teams (HPC Support Chat), or through Helpdesk to be added to a project.</li> <li>Get familiar with the ETAIS documentation here.</li> </ol>"},{"location":"access/cloud/#access-to-hpc-cloud-long-version","title":"Access to HPC Cloud (long version)","text":"<p>The HPC Centre runs an OpenStack-based Cloud. To gain access to it, follow these steps:</p>"},{"location":"access/cloud/#log-in-to-etais-and-authenticate-with-your-taltech-uniid-credentials-via-myaccessid","title":"Log in to ETAIS and authenticate with your TalTech UniID credentials via MyAccessID","text":"<p>Go to ETAIS ETAIS and choose \"sign in with MyAccessID\":</p> <p></p> <p>Choose ttu.ee as your affiliation:</p> <p></p> <p>Log in using Uni-ID (six letters taken from the user\u2019s full name), but for long time employees, it could be name.surname:</p> <p></p> <p>Confirm your data and press the button to continue:</p> <p></p> <p>Fill in the required fields, agree to the terms of use, and press the submit button:</p> <p></p>"},{"location":"access/cloud/#upload-your-ssh-public-key-to-the-etais-portal","title":"Upload your SSH Public Key to the ETAIS Portal","text":"<p>Go to the SSH page:</p> <p></p> <p>Add your public key (<code>id_rsa.pub</code>) to the corresponding field:</p> <p></p> <p>How to get SSH keys can be read here.</p>"},{"location":"access/cloud/#contact-us-to-be-added-to-a-project","title":"Contact us to be added to a project","text":"<p>There are three ways to contact us:</p> <ul> <li>Email (hpcsupport@taltech.ee)</li> <li>Teams (HPC Support Chat)</li> <li>Helpdesk</li> </ul>"},{"location":"access/cloud/#learn-how-to-use-the-cloud","title":"Learn how to use the cloud","text":"<p>You can read detailed usage instructions here.</p>"},{"location":"access/cluster-gpu/","title":"GPU-servers","text":"<p>Warning</p> <p>This page has not been completely updated for Rocky 8 yet!</p> <p>Tip</p> <p>Job submission is done from \"base\" now, direct login not possible!</p> <p>Danger</p> <p>The AI-lab \"Illukas\" modules will NOT work on the cluster due to different OS!</p>"},{"location":"access/cluster-gpu/#hardware","title":"Hardware","text":"<p>amp1:</p> <ul> <li>CPU: 2x AMD EPYC 7742 64-core (2nd gen EPYC, Zen2)</li> <li>RAM: 1 TB</li> <li>GPUs: 8x A100 Nvidia 40GB</li> <li>OS: Rocky8</li> </ul> <p>amp2:</p> <ul> <li>CPU: 2x AMD EPYC 7713 64-core (3rd gen EPYC, Zen3)</li> <li>RAM: 2 TB</li> <li>GPUs: 8x A100 Nvidia 80GB</li> <li>OS: Rocky8</li> </ul> <p>ada[1,2]</p> <ul> <li>CPU: 2x AMD EPYC 9354 32-core (4th gen EPYC, Zen4)</li> <li>RAM: 1.5 TB</li> <li>GPUs: 2x L40 Nvidia 48GB</li> <li>avx512</li> <li>OS: Rocky8</li> </ul>"},{"location":"access/cluster-gpu/#login-and-local-storage","title":"Login and local storage","text":"<p>No direct login, jobs are submitted from \"base\", use <code>srun -p gpu --gres=gpu:L40 --pty bash</code>.</p> <p>amp[1,2] have <code>/localstorage</code>, a 10 TB NVMe partition for fast data access. Data in this directory has a longer storage duration than data in the 4 TB <code>/tmp</code> (<code>/state/partition1</code> is the same as <code>/tmp</code>).</p>"},{"location":"access/cluster-gpu/#running-jobs","title":"Running jobs","text":"<p>Jobs need to be submitted using <code>srun</code> or <code>sbatch</code>; do not run jobs outside the batch system.</p> <p>Interactive jobs are started using <code>srun</code>:</p> Bash<pre><code>`srun -p gpu -t 1:00:00 --pty bash`\n</code></pre> <p>GPUs have to be reserved/requested with:</p> Bash<pre><code>`srun -p gpu --gres=gpu:A100:1 -t 1:00:00 --pty bash`\n</code></pre> <p>All nodes with GPUs are in the same partition (<code>-p gpu</code>, but also in <code>short</code>, which has higher priority, but shorter time-limit) so jobs that do not have specific requirements can run on any of the nodes. If you need a specific type, e.g. for testing performance or because of memory requirements: - It is possible to request the feature \"A100-40\" (for the 40GB A100s), \"A100-80\" (for the 80GB A100s): <code>--gres=gpu:A100:1 --constraint=A100-80</code> or <code>--gres=gpu:1 --constraint=A100-40</code>. - It is also possible to request the \"compute capability\", e.g. nvcc80 (for A100) or nvcc89 (for L40) using <code>--gres=gpu:1 --constraint=nvcc89</code> = <code>--gres=gpu:L40:1</code>. - Another option is to request the job to run on a specific node, using the <code>-w</code> switch (e.g. <code>srun -p gpu -w amp1 --gres=gpu:A100:1 ...</code>)</p> <p>You can see which GPUs have been assigned to your job using <code>echo $CUDA_VISIBLE_DEVICES</code>, the CUDA-deviceID in your programs always start with \"0\" (no matter which physical GPU was assigned to you by SLURM).</p>"},{"location":"access/cluster-gpu/#software-and-modules","title":"Software and modules","text":"<p>Same modules as on all nodes, i.e., the rocky8 and rocky8-spack modules.</p>"},{"location":"access/cluster-gpu/#from-ai-lab","title":"From AI lab","text":"<p>Will not work due to different OS</p>"},{"location":"access/cluster-gpu/#software-that-supports-gpus","title":"Software that supports GPUs","text":"<ul> <li>JupyterLab, see page on JupyterLab</li> <li>Gaussian, see page on Gaussian</li> <li>cp2k</li> <li>StarCCM+</li> <li>Julia</li> <li>Chapel</li> <li>Singularity (Apptainer), see page on Singularity</li> </ul>"},{"location":"access/cluster-gpu/#gpu-libraries-and-tools","title":"GPU libraries and tools","text":"<p>The GPUs installed are Nvidia A100 with compute capability 80, compatible with CUDA 11. However, when developing own software, be aware of vendor lockin, CUDA is only available for Nvidia GPUs and does not work on AMD GPUs. Some new supercomputers (LUMI (CSC), El Capitan (LLNL), Frontier (ORNL)) are using AMD, and some plan the Intel \"Ponte Vecchio\" GPU (Aurora (ANL), SuperMUC-NG (LRZ)). To be future-proof, portable methods like OpenACC/OpenMP are recommended.</p> <p>Porting to AMD/HIP for LUMI: https://www.lumi-supercomputer.eu/preparing-codes-for-lumi-converting-cuda-applications-to-hip/</p>"},{"location":"access/cluster-gpu/#nvidia-cuda-11","title":"Nvidia CUDA 11","text":"<p>Again, beware of the vendor lock-in.</p> <p>To compile CUDA code, use the Nvidia compiler wrapper:</p> Bash<pre><code>nvcc\n</code></pre>"},{"location":"access/cluster-gpu/#offloading-compilers","title":"Offloading Compilers","text":"<ul> <li>PGI (Nvidia HPC-SDK) supports OpenACC and OpenMP offloading to Nvidia GPUs</li> <li>GCC-10.3.0</li> <li>GCC-11.2.0 with NVPTX supports GPU-offloading using OpenMP and OpenACC pragmas</li> <li>LLVM-13.0.0 (Clang/Flang) with NVPTX supports GPU-offloading using OpenMP pragmas.</li> </ul> <p>See also: https://lumi-supercomputer.eu/offloading-code-with-compiler-directives/</p>"},{"location":"access/cluster-gpu/#openmp-offloading","title":"OpenMP offloading","text":"<p>Since version 4.0, it supports offloading to accelerators. It can be utilized by GCC, LLVM (C/Flang), and Nvidia HPC-SDK (former PGI compilers).</p> <ul> <li>GCC-10.3.0</li> <li>GCC-11.2.0 with NVPTX supports GPU-offloading using OpenMP and OpenACC pragmas</li> <li>LLVM-13.0.0 (Clang/Flang) with NVPTX supports GPU-offloading using OpenMP pragmas</li> <li>AOMP</li> </ul> <p>List of compiler support for OpenMP: https://www.openmp.org/resources/openmp-compilers-tools/</p> <p>Current recommendation: use Clang or GCC or AOMP</p>"},{"location":"access/cluster-gpu/#nvidia-hpc-sdk","title":"Nvidia HPC SDK","text":"<p>Compile option <code>-mp</code> for CPU-OpenMP or <code>-mp=gpu</code> for GPU-OpenMP-offloading.</p> <p>The table below summarizes useful compiler flags to compile your OpenMP code with offloading.</p> NVC/NVFortran Clang/Cray/AMD GCC/GFortran OpenMP flag -mp -fopenmp -fopenmp -foffload= Offload flag -mp=gpu -fopenmp-targets= -foffload= Target NVIDIA default nvptx64-nvidia-cuda nvptx-none Target AMD n/a amdgcn-amd-amdhsa amdgcn-amdhsa GPU Architecture -gpu= -Xopenmp-target -march= -foffload=\"-march=\""},{"location":"access/cluster-gpu/#openacc-offloading","title":"OpenACC offloading","text":"<p>OpenACC is a portable compiler directive based approach to GPU computing. It can be utilized by GCC, (LLVM (C/Flang)) and Nvidia HPC-SDK (former PGI compilers).</p> <p>Current recommendation: use HPC-SDK</p>"},{"location":"access/cluster-gpu/#nvidia-hpc-sdk_1","title":"Nvidia HPC SDK","text":"<p>Installed are versions 21.2, 21.5, and 21.9 (2021). These come with module files; to use them, enable the directory:</p> Bash<pre><code>module load rocky8-spack\n</code></pre> <p>Then load the module you want to use, e.g.</p> Bash<pre><code>module load nvhpc\n</code></pre> <p>The HPC SDK also comes with a profiler, to identify regions that would benefit most from GPU acceleration.</p> <p>OpenACC is based on compiler pragmas enabling an incremental approach to parallelism (you never break the sequential program), it can be used for CPUs (multicore) and GPUs (tesla). </p> <p>Compiling an OpenACC program with the Nvidia compiler: Get accelerator information</p> Bash<pre><code>pgaccelinfo\n</code></pre> <p>Compile for multicore (C and Fortran commands)</p> Bash<pre><code>pgcc -fast -ta=multicore -Minfo=accel -I/opt/nvidia/hpc_sdk/Linux_x86_64/21.5/cuda/11.3/targets/x86_64-linux/include/ -o laplace jacobi.c laplace2d.c\npgfortran -fast -ta=multicore -Minfo=accel -I/opt/nvidia/hpc_sdk/Linux_x86_64/21.5/cuda/11.3/targets/x86_64-linux/include/ -o laplace_multicore laplace2d.f90 jacobi.f90\n</code></pre> <p>Compile for GPU (C and Fortran commands)</p> Bash<pre><code>pgcc -fast -ta=tesla -Minfo=accel -I/opt/nvidia/hpc_sdk/Linux_x86_64/21.5/cuda/11.3/targets/x86_64-linux/include/ -o laplace_gpu jacobi.c laplace2d.c\npgfortran -fast -ta=tesla,managed -Minfo=accel -I/opt/nvidia/hpc_sdk/Linux_x86_64/21.5/cuda/11.3/targets/x86_64-linux/include/ -o laplace_gpu laplace2d.f90 jacobi.f90\n</code></pre> <p>Profiling:</p> Bash<pre><code>nsys profile -t nvtx --stats=true --force-overwrite true -o laplace ./laplace\nnsys profile -t openacc --stats=true --force-overwrite true -o laplace_data_clauses ./laplace_data_clauses 1024 1024\n</code></pre> <p>Analysing the profile using CLI:</p> Bash<pre><code>nsys stat s laplace.qdrep\n</code></pre> <p>Using the GUI:</p> Bash<pre><code>nsys-ui\n</code></pre> <p>Then load the <code>.qdrep</code> file.</p>"},{"location":"access/cluster-gpu/#gcc-needs-testing","title":"GCC (needs testing)","text":"<ul> <li>GCC-10.3.0</li> <li>GCC-11.2.0 with NVPTX supports GPU-offloading using OpenMP and OpenACC pragmas</li> </ul>"},{"location":"access/cluster-gpu/#hip-upcoming","title":"HIP (upcoming)","text":"<p>For porting code to AMD-Instinct based LUMI, the AMD HIP SDK will be installed.</p>"},{"location":"access/cluster/","title":"TalTech HPC Cluster","text":"<p>Using the queueing system, it is possible to run programs and scripts to utilize cluster resources such as CPU, RAM, and GPU. This allows for efficient execution of computational tasks by distributing your workloads across the cluster's multiple nodes.</p>"},{"location":"access/cluster/#accessing-the-cluster","title":"Accessing the cluster","text":"<p>Info</p> <p>NB! To access the cluster, the user must have an active Uni-ID account. For people who are neither students nor employees of Taltech, a Uni-ID non-contractual account should be created by the head of a structural unit.</p> <p>To get access to HPC, contact us by email (hpcsupport@taltech.ee) or Taltech portal. We need the following information: Uni-ID, department, and project that covers costs.</p> <p>The login node of the cluster can be reached by SSH. SSH (the Secure SHell) is available using the command <code>ssh</code> in Linux/Unix, Mac, and Windows-10. A guide for Windows users using PuTTY (an alternative SSH using a graphical user interface (GUI)) is here.</p> <p>To access the cluster base.hpc.taltech.ee, use the command:</p> Bash<pre><code>ssh uni-ID@base.hpc.taltech.ee\n</code></pre> <p>where uni-ID should be changed to the user's Uni-ID.</p> <p>The cluster is accessible from inside the university and from major Estonian network providers. If you are traveling (or not on one of the major networks), access requires FortiVPN (with the previously shown command) or a two-step login using a jump host:</p> Bash<pre><code>ssh -l uni-ID@intra.ttu.ee uni-ID@proksi.intra.ttu.ee\nssh uni-ID@base.hpc.taltech.ee\n</code></pre> <p>where all Uni-ID should be changed to the user's Uni-ID.</p> <p>For using graphical applications, add the <code>-X</code> switch to the SSH command, and for GLX (X Window System) forwarding, additionally, the <code>-Y</code> switch. To start a GUI program that uses GLX, the connection command would be:</p> Bash<pre><code>ssh -X -Y uni-ID@base.hpc.taltech.ee\n</code></pre> <p>NB! The login node is for some light interactive analysis. For heavy computations, request an (interactive) session on a compute node with the resource manager SLURM or submit a job for execution by a SLURM sbatch script!</p> <p>We strongly recommend using SSH keys for logging into the cluster.</p>"},{"location":"access/cluster/#ssh-fingerprints-of-host-keys","title":"SSH fingerprints of host keys","text":"<p>An SSH key fingerprint is a security feature for easy identification/verification of the host the user is connecting to. This option allows connecting to the server without a password. On the first connect, the user is shown a fingerprint of a host key and asked if it should be added to the list of known hosts.</p> <p>Please compare the fingerprint to the ones below. If one matches, the host can be added. If the fingerprint does not match, then there is a problem (e.g., man-in-the-middle attack).</p> <p>SSH host keys of our servers:</p> <p>base.hpc.taltech.ee:</p> Text Only<pre><code>ECDSA SHA256:OEfQiOB/eIG8hYoQ25sQk9T5tx9EtQbhi6sNM4C8mME\nED25519 SHA256:t0CSTU0AnSsJThzuM68tucrcfnn2wLKabjSnuRKX8Yc\nRSA SHA256:qYrmOw/YN7wf640yBHADX3wnAOPu0OOXlcu4LKBxzG8\n</code></pre> <p>viz.hpc.taltech.ee:</p> Text Only<pre><code>ECDSA SHA256:z2/bxleZ3T3vErkg4C7kvDPKKEU0qaoR8bL29EgMfGA\nED25519 SHA256:9zRBmS3dxD7BNISZKwg6l/2+6p4HeqlOhA4OMBjD9mk\nRSA SHA256:Q6NDm88foRVTKtEAEexcRqPqMQNGUzf3rQdetBympPg\n</code></pre> <p>How to get SSH keys.</p>"},{"location":"access/cluster/#structure-and-file-tree","title":"Structure and file tree","text":"<p>By accessing the cluster, the user gets into their home directory or <code>$HOME</code> (<code>/gpfs/mariana/home/$USER/</code>).</p> <p>In the home directory, the user can create, delete, and overwrite files and perform calculations (if the slurm script does not force the program to use the <code>$SCRATCH</code> directory). The home directory is limited in size to 500 GB, and backups are performed once per week.</p> <p>The home directory can be accessed from the console or by GUI programs, but it cannot be mounted. For mounting, special <code>smbhome</code> and <code>smbgroup</code> folders were created (<code>/gpfs/mariana/smbhome/$USER/</code> and <code>/gpfs/mariana/smbgroup/</code>, respectively). More about <code>smb</code> folders can be found here.</p> <p>Some programs and scripts suppose that files will be transferred to the <code>$SCRATCH</code> directory at the compute node and calculations will be done there. If the job is killed, for example, due to the time limit, back transfer will not occur. In this case, the user needs to know which node this job was running on (see <code>slurm-$job_id.stat</code>), to connect to exactly this node (in this example, it is green11). The <code>$SCRATCH</code> directory will be in <code>/state/partition1/</code> and corresponds to the job ID number.</p> Bash<pre><code>srun -w green11 --pty bash\ncd /state/partition1/\n</code></pre> <p>Please note that the scratch is not shared between nodes, so parallel MPI jobs that span multiple nodes cannot access each other's scratch files.</p>"},{"location":"access/cluster/#running-jobs-with-slurm","title":"Running jobs with SLURM","text":"<p>SLURM is a management and job scheduling system for Linux clusters. A SLURM quick reference can be found here.</p> <p>Examples of slurm scripts are usually given on the program's page with some recommendations for optimal use of resources for this particular program. A list of the programs installed at HPC is given on our software page. On the software page or program's page, information about licenses can also be found, since programs installed at HPC have varying license agreements. To use some licensed programs (for example, Gaussian), the user must be added to the appropriate group. For this, contact us by email (hpcsupport@taltech.ee) or Taltech portal.</p> <p>The most often used SLURM commands are:</p> <ul> <li><code>srun</code> - to start a session or an application (in real time)</li> <li><code>sbatch</code> - to start a computation using a batch file (submit for later execution)</li> <li><code>squeue</code> - to check the load of the cluster and the status of own jobs</li> <li><code>sinfo</code> - to check the state of the cluster and partitions</li> <li><code>scancel</code> - to delete a submitted job (or stop a running one).</li> </ul> <p>For more parameters, see the man pages (manual) of the commands <code>srun</code>, <code>sbatch</code>, <code>sinfo</code>, and <code>squeue</code>. For this, use the command <code>man</code> followed by the program name whose manual you want to see, e.g.:</p> Bash<pre><code>man srun\n</code></pre> <p>Requesting resources with SLURM can be done either with parameters to <code>srun</code> or in a batch script invoked by <code>sbatch</code>.</p> <p>The following defaults are used if not otherwise specified:</p> <ul> <li>default memory -- is 1 GB/thread (for larger jobs, request more memory)</li> <li>short partition -- default time limit is 10 min and max time limit is 4 hours (longer jobs need to be submitted to partitions common, green-ib, or gpu partitions)</li> <li>common partition -- default time is 10 min and max time limit is 8 days.</li> <li>long partition -- default time is 10 min and time limit is 15 days.</li> <li>green-ib partition -- default time is 10 min and max time limit is 8 days</li> <li>bigmem partition -- default time is 10 min and max time limit is 8 days</li> <li>gpu partition -- default time is 10 min and max time limit is 5 days</li> </ul> <p>Running an interactive session longer than the default 10 min. (here 1 hour):</p> Bash<pre><code>srun -t 01:00:00 --pty bash\n</code></pre> <p>This logs you into one of the compute nodes. There you can load modules and run interactive applications, compile your code, etc.</p> <p>With <code>srun</code>, it is recommended to use CLI (command-line interface) instead of GUI (Graphical User Interface) programs if possible. For example, use octave-CLI or octave instead of octave-GUI.</p> <p>Running a simple non-interactive single-process job that lasts longer than the default 4 hours (here 5 hours):</p> Bash<pre><code>srun --partition=common -t 05:00:00 -n 1 ./a.out\n</code></pre> <p>NB! Environment variables for OpenMP are not set automatically, e.g.</p> Bash<pre><code>srun  -N 1 --cpus-per-task=28 ./a.out\n</code></pre> <p>would not set <code>OMP_NUM_THREADS</code> to 28; this has to be done manually. So usually, for parallel jobs, it is recommended to use scripts for <code>sbatch</code>.</p> <p>Below is an example of a batch slurm script (filename: <code>myjob.slurm</code>) with an explanation of the commands.</p> Bash<pre><code>#!/bin/bash\n#SBATCH --partition=common    ### Partition\n#SBATCH --job-name=HelloOMP   ### Job Name           -J\n#SBATCH --time=00:10:00       ### WallTime           -t\n#SBATCH --nodes=4             ### Number of Nodes    -N \n#SBATCH --ntasks-per-node=7   ### Number of tasks (MPI processes)\n#SBATCH --cpus-per-task=4     ### Number of threads per task (OMP threads)\n#SBATCH --account=hpcrcf      ### In case of several accounts, specifies account used for job submission\n#SBATCH --mem-per-cpu=100     ### Min RAM required in MB\n#SBATCH --array=13-18         ### Array tasks for parameter sweep\n\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK     ### setup environment\nmodule load gcc                     ### setup environment\n./hello_omp $SLURM_ARRAY_TASK_ID            ### only for arrays, setup output files with system information\nmpirun -n 28 ./hello_mpi                ### run program\n</code></pre> <p>In this example, some of the more common submission parameters are listed. There are many more possible job-submission options; moreover, some of the options listed above are not useful to apply together. An explanation of the variables used inside SLURM/SBATCH can be found here. In contrast to, e.g., GridEngine, SLURM allows fine-grained resource requests, using parameters like <code>--ntasks-per-core</code> or <code>--ntasks-per-node</code>.</p> <p>An example script for submitting:</p> <ul> <li>a single process job</li> <li>an OpenMP parallel job</li> <li>an MPI parallel job (OpenFOAM)</li> <li>an array (parameter sweep) job</li> <li>a GPU job</li> <li>a job using the scratch partition (sequential or OpenMP parallel)</li> </ul> <p>The job is then submitted to SLURM by</p> Bash<pre><code>sbatch myjob.slurm\n</code></pre> <p>and will be executed when the requested resources become available.</p> <p>Output of applications and error messages are by default written to a <code>slurm-$job_id.out</code> file. More about SLURM finished job statistics can be found here.</p> <p>Some useful online resources:</p> <ul> <li>SLURM scheduler workload manager</li> <li>Victor Eijkhout: Introduction to High-Performance Scientific Computing </li> <li>Charles Severance, Kevin Dowd: High Performance Computing</li> <li>OpenMP standard</li> <li>MPI standard</li> <li>SLURM Quick Reference (Cheat Sheet)</li> </ul>"},{"location":"access/cluster/#slurm-accounts","title":"SLURM accounts","text":"<p>In SLURM, there are accounts for billing; these are different from the login account!</p> <p>Each user has their own personal SLURM account, which will have a monthly limit and at least one project account for larger calculations.</p> <p>SLURM user accounts start with <code>user_</code> and project accounts with <code>project_</code> and course accounts with <code>course_</code>, followed by UniID/projectID/courseID. You can check which SLURM accounts you belong to by <code>sacctmgr show associations format=account%30,user%30 | grep uniID</code>. Currently, (almost) all users belong to the SLURM account \"vaikimisi\" (default). It is possible to submit jobs under this account, especially if no <code>user_</code> or project account has been created for you yet; however, \"vaikimisi\" will be discontinued in the near future.</p> <p>When submitting a job, it is important to use the correct SLURM account <code>--account=SLURM-ACCOUNT</code>, as this is connected to the financial source.</p>"},{"location":"access/cluster/#monitoring-jobs-resources","title":"Monitoring jobs &amp; resources","text":""},{"location":"access/cluster/#monitoring-a-job-on-the-node","title":"Monitoring a job on the node","text":""},{"location":"access/cluster/#status-of-a-job","title":"Status of a job","text":"<p>Users can check the status of their jobs (whether they are running or not, and on which node) by the command:</p> Bash<pre><code>squeue -u $USER\n</code></pre> <p></p>"},{"location":"access/cluster/#load-of-the-node","title":"Load of the node","text":"<p>Users can check the load of the node their job runs on, the status, and the configuration of this node by the command</p> Bash<pre><code>scontrol show node &lt;nodename&gt;\n</code></pre> <p>The load should not exceed the number of hyperthreads (CPUs in SLURM notation) of the node.</p> <p></p> <p>In the case of MPI parallel runs, statistics of several nodes can be monitored by specifying node names. For example:</p> Bash<pre><code>scontrol show node=green[25-26]\n</code></pre> <p>Node features for node selection using <code>--constraint=</code>:</p> feature what it is A100-40 has A100 GPU with 40GB A100-80 has A100 GPU with 80GB L40 has L40 GPU with 48GB nvcc80 GPU has compute capability 8.0 nvcc89 GPU has compute capability 8.9 nvcc35 GPU has compute capability 3.5 zen2 AMD Zen CPU architecture 2nd gen zen3 AMD Zen CPU architecture 3rd gen zen4 AMD Zen CPU architecture 4th gen avx512 CPU has avx512 skylake Intel SkyLake CPU architecture sandybridge Intel SandyBridge CPU architecture"},{"location":"access/cluster/#monitoring-with-interactive-job","title":"Monitoring with interactive job","text":"<p>It is possible to submit a second interactive job to the node where the main job is running. Check with <code>squeue</code> where your job is running, then submit:</p> Bash<pre><code>srun -w &lt;nodename&gt; --pty htop\n</code></pre> <p>Note that there must be free slots on the machine, so you cannot use <code>-n 80</code> or <code>--exclusive</code> for your main job (use <code>-n 79</code>).</p> <p></p> <p>Press <code>q</code> to exit.</p> <p>You can also add a column that shows the CPU number of the program (for more details click here).</p> <p>For Linux, F1-F10 keys should be used; for Mac, just click on the corresponding buttons.</p> <p></p> <p>A new column will appear, showing the CPU number of the program.</p> <p></p>"},{"location":"access/cluster/#monitoring-jobs-using-gpus","title":"Monitoring jobs using GPUs","text":"<p>Log in to amp or amp2. The command</p> Bash<pre><code>echo ${SLURM_STEP_GPUS:-$SLURM_JOB_GPUS}\n</code></pre> <p>shows the GPU IDs allocated to your job.</p> <p>GPU load can be checked by the command:</p> Bash<pre><code>nvidia-smi\n</code></pre> <p></p> <p>Press <code>control+c</code> to exit.</p> <p>Another option is to log in to amp or amp2, check which GPUs are allocated to your job, and give the command:</p> Bash<pre><code>nvtop\n</code></pre> <p></p> <p>Press <code>q</code> to exit.</p> <p>An alternative method on Linux computers, if you have X11, is to log in to base/amp with the <code>--X</code> key:</p> Bash<pre><code>ssh --X UniID@base.hpc.taltech.ee\n</code></pre> <p>Then submit your main interactive job:</p> Bash<pre><code>srun --x11 -n &lt;numtasks&gt; --cpus-per-task=&lt;numthreads&gt; --pty bash\n</code></pre> <p>and start an <code>xterm -e htop &amp;</code> in the session.</p> <p>In <code>sbatch</code>, the option <code>--x11=batch</code> can be used. Note that the SSH session to base needs to stay open!</p>"},{"location":"access/cluster/#monitoring-resource-usage","title":"Monitoring resource usage","text":"<p>The default disk quota for <code>home</code> (which is backed up weekly) is 500 GB, and for <code>smbhome</code> (which is not backed up) it is 2 TB per user. For <code>smbgroup</code>, there are no limits and no backup.</p> <p>The easiest way to check your current disk usage is to look at the table that appears when you log in to HPC.</p> <p></p> <p>You can also monitor your resource usage with the <code>taltech-lsquota.bash</code> script and the <code>sreport</code> command.</p> <p>Current disk usage:</p> Bash<pre><code>taltech-lsquota.bash\n</code></pre> <p></p> <p>CPU usage during the last day:</p> Bash<pre><code>sreport -t Hours cluster UserUtilizationByAccount Users=$USER\n</code></pre> <p></p> <p>CPU usage in a specific period (e.g., since the beginning of this year):</p> Bash<pre><code>sreport -t Hours cluster UserUtilizationByAccount Users=$USER start=2024-01-01T00:00:00 end=2024-12-31T23:59:59\n</code></pre> <p>Where <code>start=</code> and <code>end=</code> can be changed depending on the desired period of time.</p> <p></p> <p>For convenience, a tool <code>taltech-history</code> was created. By default, it shows the jobs of the current month. Use <code>taltech-history -a</code> to get a summary of the usage hours and costs of the current month.</p>"},{"location":"access/cluster/#copying-data-tofrom-the-clusters","title":"Copying data to/from the clusters","text":"<p>Since HPC disk quota is limited, it is recommended to have your own copy of important calculations and results. Data from HPC can be transferred by several commands: <code>scp</code>, <code>sftp</code>, <code>sshfs</code>, or <code>rsync</code>.</p> <ol> <li> <p><code>scp</code> is available on all Linux systems, Mac, and Windows10 PowerShell. There are also GUI versions available for different OS (like PuTTY).</p> <p>Copying to the cluster with <code>scp</code>:</p> Bash<pre><code>scp local_path_from_where_copy/file uni-id@base.hpc.taltech.ee:path_where_to_save\n</code></pre> <p></p> <p>Copying from the cluster with <code>scp</code>:</p> Bash<pre><code>scp uni-id@base.hpc.taltech.ee:path_from_where_copy/file local_path_where_to_save\n</code></pre> <p></p> <p>The path to the file at HPC can be checked by the <code>pwd</code> command.</p> </li> <li> <p><code>sftp</code> is the secure version of the <code>ftp</code> protocol available on Linux, Mac, and Windows10 PowerShell. This command starts a session in which files can be transmitted in both directions using the <code>get</code> and <code>put</code> commands. File transfer can be done in \"binary\" or \"ascii\" mode; conversion of line endings (see below) is automatic in \"ascii\" mode. There are also GUI versions available for different OS (FileZilla, gFTP, and WinSCP (Windows)).</p> Bash<pre><code>sftp uni-id@base.hpc.taltech.ee\n</code></pre> <p></p> </li> <li> <p><code>sshfs</code> can be used to temporarily mount remote filesystems for data transfer or analysis. Available in Linux. The data is tunneled through an SSH connection. Be aware that this is usually not performant and can create high load on the login node due to SSH encryption.</p> Bash<pre><code>sshfs uni-id@base.hpc.taltech.ee:remote_dir/ /path_to_local_mount_point/\n</code></pre> </li> <li> <p><code>rsync</code> can update files if previous versions exist without having to transfer the whole file. However, its use is recommended for the advanced user only since one has to be careful with the syntax.</p> </li> </ol>"},{"location":"access/cluster/#smbcifs-exported-filesystems","title":"SMB/CIFS exported filesystems","text":"<p>One of the simple and convenient ways to control and process data based on HPC is mounting. Mounting means that the user attaches their directory placed at HPC to a directory on their computer and can process files as if they were on this computer. These can be accessed from within the university or from EduVPN.</p> <p>Each user automatically has a directory within <code>smbhome</code>. It does not match with the <code>$HOME</code> directory, so calculations should be initially done in the <code>smbhome</code> directory to prevent copying, or files needed should be copied from the <code>home</code> directory to the <code>smbhome</code> directory by commands:</p> Bash<pre><code>pwd  ### look path to the file \ncp path_to_your_file/your_file /gpfs/mariana/smbhome/$USER/  ### copying\n</code></pre> <p>To get a directory for group access, please contact us (a group and a directory need to be created).</p> <p>The HPC center exports two filesystems as Windows network shares:</p> local path on cluster Linux network URL Windows network URL /gpfs/mariana/smbhome/$USER smb://smb.hpc.taltech.ee/smbhome \\\\smb.hpc.taltech.ee\\smbhome /gpfs/mariana/smbgroup smb://smb.hpc.taltech.ee/smbgroup \\\\smb.hpc.taltech.ee\\smbgroup /gpfs/mariana/home/$USER not exported not exported <p>This is the quick-access guide; for more details, see here</p>"},{"location":"access/cluster/#windows-access","title":"Windows access","text":"<p>The shares can be found using the Explorer \"Map Network Drive\".</p> Bash<pre><code>server &gt;&gt;&gt; \\\\smb.hpc.taltech.ee\\smbhome\nusername &gt;&gt;&gt; INTRA\\&lt;uni-id&gt;\n</code></pre> <p>From PowerShell:</p> Bash<pre><code>net use \\\\smb.hpc.taltech.ee\\smbhome /user:INTRA\\uni-id\nget-smbconnection\n</code></pre>"},{"location":"access/cluster/#linux-access","title":"Linux access","text":"<p>On Linux with GUI Desktop, the shares can be accessed with the Nautilus browser.</p> <p>From the command line, the shares can be mounted as follows:</p> Bash<pre><code>dbus-run-session bash\ngio mount smb://smb.hpc.taltech.ee/smbhome/\n</code></pre> <p>You will be asked for \"User\" (which is your UniID), \"Domain\" (which is \"INTRA\"), and your password.</p> <p>To disconnect from the share, unmount with</p> Bash<pre><code>gio mount -u smb://smb.hpc.taltech.ee/smbhome/\n</code></pre>"},{"location":"access/cluster/#special-considerations-for-copying-windows-linux","title":"Special considerations for copying Windows - Linux","text":"<p>Microsoft Windows uses a different line ending in text files (ASCII/UTF8 files) than Linux/Unix/Mac: CRLF vs. LF. When copying files between Windows and Linux, this needs to be taken into account. The FTP (File Transfer Protocol) has ASCII and BINARY modes; in ASCII mode, the line-end conversion is automatic.</p> <p>There are tools for conversion of the line ending, in case the file was copied without line conversion: <code>dos2unix</code>, <code>unix2dos</code>, <code>todos</code>, <code>fromdos</code>, and the stream editor <code>sed</code> can also be used.</p>"},{"location":"access/cluster/#backup","title":"Backup","text":"<p>There are two major directories where users can store data:</p> <ul> <li><code>/gpfs/mariana/home/</code> default home directory, which is limited to 500GB and is backed up, excluding specific directories: <code>[*/envs/, */.cache/, */pkgs/]</code>.</li> <li><code>/gpfs/mariana/smbhome/</code> has a limit of 2TB and is not backed up.</li> </ul> <p>The home directory is meant for critical data like configurations and scripts, whereas smbhome is meant for data.</p> <p>The backup will run weekly. If the home directory is larger than 500GB [usage is displayed upon login to the cluster], it will not be backed up.</p> <p>If your home directory is larger than 500GB, please move the data to smbhome.</p> <p>At HPC, programs with varying license agreements are installed. To use some licensed programs (for example, Gaussian), the user must be added to the appropriate group. For this, contact us by email (hpcsupport@taltech.ee) or Taltech portal. More about available programs and licenses can be found on the software page.</p>"},{"location":"access/hardware/","title":"Hardware Specification","text":"<p>Warning</p> <p>This page has not been completely updated for Rocky 8 yet!</p> <p>The use of the resources of the TalTech HPC Centre requires an active Uni-ID account. A procedure for non-employees/non-students can be found here (in Estonian).</p> <p>TalTech operates the following infrastructure:</p> <ul> <li>base.hpc.taltech.ee is the new cluster environment where all nodes from HPC1 and HPC2 will be migrated.</li> <li>A live diagram of the cluster load is available here.</li> <li>Uni-ID login (please contact hpcsupport@taltech.ee to activate access).</li> <li>SLURM v20 scheduler.</li> <li>CentOS 7.</li> <li>Home directories' location: <code>/gpfs/mariana/home/uni-ID</code>.</li> <li>HPC1 home directories accessible under <code>/beegfs/home/uni-ID</code>.</li> <li>Home directory file system has 1.5 PB storage, with a 2 TB/user quota.</li> <li>Green nodes (former hpc2.ttu.ee nodes):</li> <li>32 nodes with 2 x Intel Xeon Gold 6148 20C 2.40 GHz, 96 GB DDR4-2666 R ECC RAM (green[1-32]).</li> <li>25 Gbit Ethernet.</li> <li>18 of these nodes are connected with low latency interconnect FDR InfiniBand (green-ib partition).</li> <li>Gray nodes (former hpc.ttu.ee nodes, migration in progress):</li> <li>48 nodes with 2 x Intel Xeon E5-2630L 6C with 64 GB RAM and 1 TB local drive connected with low latency interconnect QDR InfiniBand (gray-ib partition).</li> <li>60 nodes with 2 x Intel Xeon E5-2630L 6C with 48 GB RAM and 1 TB local drive (memory upgrade to 64 GB in progress).</li> <li>1 Gbit Ethernet.</li> <li>mem1tb large memory node:</li> <li>1 node with 1 TB of operating RAM.</li> <li>4x Intel Xeon CPU E5-4640 (together 32 cores, 64 threads).</li> <li>amp GPU node, specific guide for amp (amp.hpc.taltech.ee):</li> <li>1 node with 8x Nvidia A100.</li> <li>2x 64-core AMD EPYC 7742 (together 128 cores, 256 threads).</li> <li>1 TB RAM.</li> <li>OpenACC, OpenMP, OpenCL, CUDA.</li> <li>Ubuntu 20.04.</li> <li>viz.hpc.taltech.ee Visualization node (accessible within University network and FortiVPN):</li> <li>This service is intended mainly for post-processing, especially of large datasets; it is not intended as a remote desktop service; job submission is not possible from here.</li> <li>1 node with 2 Nvidia Tesla K20Xm graphic cards (on displays :0.0 and :0.1).</li> <li>This node is not for computation on GPUs.</li> <li>Debian 10.</li> <li>Uni-ID login with ssh-keys (no password login; previous login on base.hpc.taltech.ee required to create account).</li> <li>Same home directories as base, HPC1 homes accessible under <code>/beegfs/home/uni-ID</code>.</li> <li>ParaView, VisIt, and COVISE available for remote visualization.</li> <li>VirtualGL, Xpra, VNC available for remote execution of programs (can be used with e.g., Mayavi, ParaView, VisIt, COVISE, OpenDX).</li> <li>There is no backup; please take care of backups yourself!</li> <li>Partitions:</li> <li>short: (default) time limit 4 hours, default time 10 min, default mem 1 GB/thread, green nodes.</li> <li>common: time limit 8 days, default time 10 min, default mem 1 GB/thread, green nodes.</li> <li>long: time limit 22 days, default time 10 min, default mem 1 GB/thread, green nodes.</li> <li>green-ib: time limit 8 days, default time 10 min, default mem 1 GB/thread, green InfiniBand nodes.</li> <li>gray-ib: time limit 8 days, default time 10 min, default mem 1 GB/thread, gray InfiniBand nodes.</li> <li>gpu: amp GPU node, time limit 5 days, default time 10 min, default mem 1 GB/thread.</li> <li> <p>mem1tb: mem1tb node.</p> </li> <li> <p>training.hpc.taltech.ee virtual training cluster:</p> </li> <li>Used for testing.</li> <li> <p>Running on the TalTech ETAIS OpenStack cloud service.</p> </li> <li> <p>TalTech ETAIS Cloud: 4 node OpenStack cloud:</p> </li> <li>5 compute (nova) nodes with 282 GB or 768 GB of RAM and 80 threads each.</li> <li>65 TB CephFS storage (net capacity).</li> <li>Accessible through the ETAIS website: https://etais.ee/using/.</li> </ul>"},{"location":"access/lumi-examples/","title":"LUMI Slurm Usage Examples","text":"<p>Like at HPC, at LUMI, computing resources are allocated to the user by the resource manager Slurm. More about Slurm scripts at LUMI can be found here.</p> <ul> <li> <p>At LUMI, partitions can be allocated by node or by resources.</p> </li> <li> <p>The user always has to specify the account. It is mandatory!</p> <p>Account specification can be done by adding into the Slurm script <code>#SBATCH --account=project_XXX</code> line or by adding the following two lines into the <code>.bashrc</code> file by command:</p> Bash<pre><code>cat &lt;&lt;EOT &gt; .bashrc\nexport SBATCH_ACCOUNT=project_XXX\nexport SALLOC_ACCOUNT=project_XXX\nEOT\n</code></pre> <p>where <code>XXX</code> is a project number which can be found in ETAIS as <code>Effective ID</code>.</p> </li> <li> <p>By default, upon node failure, the job will be automatically resubmitted to the queue with the same job ID and that will truncate the previous output. To avoid this, add the following two lines into the <code>.bashrc</code> file by command:</p> Bash<pre><code>cat &lt;&lt;EOT &gt; .bashrc\nSBATCH_NO_REQUEUE=1 \nSBATCH_OPEN_MODE=append\nEOT\n</code></pre> </li> </ul> <p>More about Slurm options can be found in LUMI manuals.</p> <p>Slurm script examples provided by LUMI:</p> <ul> <li>GPU jobs</li> <li>CPU jobs</li> <li>Job array</li> </ul>"},{"location":"access/lumi-examples/#multi-node-multi-gpu-pytorch-training","title":"Multi-Node Multi-GPU PyTorch Training","text":"<p>This PyTorch script simulates training a ResNet model across multiple GPUs and nodes.</p>"},{"location":"access/lumi-examples/#quick-guide","title":"Quick Guide","text":"<ol> <li> <p>Download:</p> <ul> <li>environment setup script - env.sh</li> <li>bash script to set up Singularity - setup.sh</li> <li>PyTorch script - min_dist.py</li> <li>Slurm script - dist_run.slurm</li> </ul> </li> <li> <p>Set up the environment by command:</p> Bash<pre><code>. env.sh project_XXX \n</code></pre> <p>where <code>XXX</code> is a project number.</p> </li> <li> <p>Set up Singularity:</p> Bash<pre><code>./setup.sh \n</code></pre> </li> <li> <p>Run the PyTorch script:</p> Bash<pre><code>sbatch -N 2 dist_run.slurm min_dist.py\n</code></pre> </li> <li> <p>You should get an output file <code>slurm_job-number</code> with content like:</p> Text Only<pre><code>8 GPU processes in total\nBatch size = 128\nDataset size = 50000\nEpochs = 5\n\nEpoch 0  done in 232.64820963301463s\nEpoch 1  done in 31.191600811027456s\nEpoch 2  done in 31.244039460027125s\nEpoch 3  done in 31.384101407951675s\nEpoch 4  done in 31.143528194981627s\n</code></pre> </li> </ol>"},{"location":"access/lumi-examples/#detailed-guide","title":"Detailed Guide","text":"<p>Download:</p> <ul> <li>environment setup script - env.sh</li> <li>bash script to set up Singularity - setup.sh</li> <li>PyTorch script - min_dist.py</li> <li>Slurm script - dist_run.slurm</li> </ul>"},{"location":"access/lumi-examples/#setup","title":"Setup","text":"<p>These commands will set up the environment and Singularity:</p> Bash<pre><code>. env.sh project_XXX\n./setup.sh\n</code></pre> <p>where <code>XXX</code> is a project number that should be changed according to the user's project number.</p>"},{"location":"access/lumi-examples/#running","title":"Running","text":"<p>The job can be submitted into the queue by command:</p> Bash<pre><code>sbatch -N 2 dist_run.slurm min_dist.py\n</code></pre> <p>Where <code>dist_run.slurm</code> is a resource manager, <code>min_dist.py</code> is a PyTorch script, and <code>-N</code> is the number of nodes used.</p> Bash<pre><code>#!/bin/bash\n#SBATCH --job-name=DIST\n#SBATCH --time=10:00\n#SBATCH --mem 64G\n#SBATCH --cpus-per-task 32\n#SBATCH --partition small-g\n#SBATCH --gpus-per-node 4\n\nexport NCCL_SOCKET_IFNAME=hsn\nexport NCCL_NET_GDR_LEVEL=3\n\nexport CXI_FORK_SAFE=1\nexport CXI_FORK_SAFE_HP=1\nexport FI_CXI_DISABLE_CQ_HUGETLB=1\n\nexport MIOPEN_USER_DB_PATH=/tmp/${USER}-miopen-cache-${SLURM_JOB_ID}\nexport MIOPEN_CUSTOM_CACHE_DIR=${MIOPEN_USER_DB_PATH}\n\nexport MASTER_ADDR=$(scontrol show hostname $SLURM_NODELIST | head -n1)\n\nexport OMP_NUM_THREADS=8\n\nsrun singularity exec --rocm \\\n        $SCRATCH/pytorch_latest.sif \\\n        torchrun --nnodes=$SLURM_NNODES \\\n        --nproc_per_node=$SLURM_GPUS_PER_NODE \\\n        --rdzv_id=$SLURM_JOB_ID \\\n        --rdzv_backend=c10d \\\n        --rdzv_endpoint=$MASTER_ADDR $@\n</code></pre> <p>The environment variables containing <code>NCCL</code> and <code>CXI</code> are used by RCCL for communication over Slingshot.</p> <p>The ones containing <code>MIOPEN</code> are for MIOpen to create its caches in the <code>/tmp</code> (which is local to each node and in memory). If this is not set, then MIOpen will create its cache in the user home directory (the default), which is a problem since each node needs its own cache.</p>"},{"location":"access/lumi-software/","title":"LUMI Software","text":"<p>LUMI offers a wide range of programs in different fields:</p> <ul> <li>Machine learning on top of PyTorch and TensorFlow</li> <li>Image classification - ResNet</li> <li>Object recognition and XFMR translation - SSD, XFMR</li> <li>Scientific software suites - Gromacs (molecular dynamics), CP2K (quantum chemistry), and ICON (climate science)</li> <li>Weather prediction application - GridTools allowing measures stencil-based</li> <li>etcetera</li> </ul> <p>More information about installed software and how to install software yourself can be found in the LUMI documentation.</p> <p>The list of available programs can be found in the LUMI Software Library. There you can also find license information - whether the program is free to use, requires pre-registration, or the user must provide their own license.</p>"},{"location":"access/lumi-software/#first-time-use","title":"First time use","text":"<p>To be able to use a program, the user first has to install it. Installation can be done by SPACK or by EasyBuild. The list of available programs in EasyBuild can be found in the LUMI Software Library as well as installation instructions. The list of programs that can be installed by SPACK can be viewed by the <code>spack list</code> command. The same program can be installed by both SPACK and EasyBuild.</p> <p>There are two possible places where programs can be installed - the user's <code>$HOME</code> or project directory. The latter is recommended since other users of the project will be able to use installed programs as well. Moreover, <code>$HOME</code> size is limited to 20GB. More about data storage at LUMI and storage billing.</p> <p>The project <code>XXX</code> number can be found in ETAIS as <code>Effective ID</code>.</p> <p></p> <p>It is good practice to add the place where programs will be installed into your <code>.profile</code> or <code>.bashrc</code> file. To do this, give the command:</p> Bash<pre><code>echo \"export EBU_USER_PREFIX=/project/project_XXX\" &gt;&gt; .bashrc\n</code></pre> <p>where <code>XXX</code> is a project number.</p> <p>To install programs, use the following commands:</p> Bash<pre><code>export EASYBUILD_PREFIX=$HOME/easybuild \nexport EASYBUILD_BUILDPATH=/tmp/$USER\n</code></pre>"},{"location":"access/lumi-software/#program-installation","title":"Program installation","text":""},{"location":"access/lumi-software/#installation-by-spack","title":"Installation by SPACK","text":"<ol> <li> <p>Initialize SPACK:</p> Bash<pre><code>export SPACK_USER_PREFIX=/project/project_XXX/spack \nmodule list\n</code></pre> <p>where the project <code>XXX</code> number can be found in ETAIS as <code>Effective ID</code>.</p> <p>After, the user should load the following modules:</p> Bash<pre><code>module load LUMI/YYY partition/ZZZ \nmodule load spack/RRR\n</code></pre> <p>where <code>YYY</code> is the version of LUMI, which will appear in the module list. Partition <code>ZZZ</code> is determined depending on whether CPUs (partition/C) or GPUs (partition/G) will be used. <code>RRR</code> is the version of SPACK, which will appear in the module list.</p> <p></p> </li> <li> <p>The entire list of programs available for installation by SPACK can be viewed by the command:</p> Bash<pre><code>spack list\n</code></pre> <p>The list will be too long, so it is better to search for a certain program by the command:</p> Bash<pre><code>spack list program_name\n</code></pre> <p>where the whole name or part of it is given.</p> <p>NB! SPACK is insensitive to caps.</p> </li> <li> <p>Check what flags should be added:</p> Bash<pre><code>spack info program_name\n</code></pre> <p></p> </li> <li> <p>Program installation is done by the command:</p> Bash<pre><code>spack install program_name@version%compiler@version +flag(s) ^forced_dependencies\n</code></pre> <p>where <code>flag</code> is an installation option taken from variants of SPACK info (see above). It is recommended to try the <code>cce</code> (Cray Compiler Edition) and for MPI-dependent software to force the <code>cray-mpich</code> dependency.</p> <p>For example:</p> Bash<pre><code>spack install nwchem@7.0.2%cce@15.0.1 +openmp ^cray-mpich@8.1.25\n</code></pre> <p>or</p> Bash<pre><code>spack install kokkos+rocm+debug_bounds_check amdgpu_target=gfx90a %gcc@11.2.0\n</code></pre> <p>Refresh the module list:</p> Bash<pre><code>spack module tcl refresh -y\n</code></pre> <p>For more details, see the LUMI guide.</p> <p>NB! Program installation will require time up to hours.</p> </li> <li> <p>When the program is already installed, the user should load it before use by commands:</p> Bash<pre><code>module load program_name\n</code></pre> </li> </ol>"},{"location":"access/lumi-software/#installation-by-easybuild","title":"Installation by EasyBuild","text":"<ol> <li> <p>To install a program with EasyBuild, initialize it by the following commands:</p> Bash<pre><code>module use /projappl/project_XXXX/easybuild/modules/all\nmodule list\n</code></pre> <p>where the project <code>XXX</code> number can be found in ETAIS as <code>Effective ID</code>.</p> <p>After, the user should load the following modules:</p> Bash<pre><code>module load LUMI/YYY partition/ZZZ \nmodule load EasyBuild-user\n</code></pre> <p>where <code>YYY</code> is the version of LUMI that can be found on the program's page in the LUMI Software Library. Sometimes partition <code>ZZZ</code> is determined in the description of the program in the LUMI Software Library. In case it is not, partition <code>ZZZ</code> is used depending on whether CPUs (partition/C) or GPUs (partition/G) will be used.</p> <p></p> </li> <li> <p>After EasyBuild is loaded, the user can install the program needed by the command <code>eb</code>.</p> Bash<pre><code>eb program_eb_file\n</code></pre> <p>NB! The full name of <code>program_eb_file</code> as well as some additional flags needed for installation can be found on the program's page in the LUMI Software Library.</p> <p>NB! Program installation will require time up to an hour.</p> </li> <li> <p>When the program is already installed, the user should load it before use by commands:</p> Bash<pre><code>module load program_name\n</code></pre> </li> </ol>"},{"location":"access/lumi-software/#loading-program-adding-modules-into-slurm","title":"Loading program &amp; adding modules into Slurm","text":"<p>When the program is already installed, the user should load it before use or add it into the Slurm script. If the program was installed by SPACK, the following commands should be given:</p> Bash<pre><code>export SPACK_USER_PREFIX=/project/project_XXX/spack\nmodule load LUMI/YYY partition/ZZZ \nmodule load spack/YYY\nmodule load program_name/VVV\n</code></pre> <p>where <code>XXX</code> is a project number, and can be found in ETAIS as <code>Effective ID</code>. <code>YYY</code> is the version of LUMI, which will appear in the module list. Partition <code>ZZZ</code> is determined depending on whether CPUs (partition/C) or GPUs (partition/G) will be used. <code>RRR</code> is the version of SPACK, which will appear in the module list. <code>VVV</code> is the version of the program, which will appear in the module list.</p> <p>If the program was installed by EasyBuild, the following commands should be given:</p> Bash<pre><code>module use /projappl/project_465000338/easybuild/modules/all\nmodule load LUMI/YYY partition/ZZZ \nmodule load EasyBuild/\nmodule load program_name/VVV\n</code></pre> <p>where <code>XXX</code> is a project number, and can be found in ETAIS as <code>Effective ID</code>. <code>YYY</code> is the version of LUMI, which will appear in the module list. Partition <code>ZZZ</code> is determined depending on whether CPUs (partition/C) or GPUs (partition/G) will be used. <code>RRR</code> is the version of SPACK, which will appear in the module list. <code>VVV</code> is the version of the program, which will appear in the module list.</p> <p>Examples of Slurm scripts can be found here.</p>"},{"location":"access/lumi-software/#uninstalling-programs","title":"Uninstalling Programs","text":"<p>To uninstall a program installed by SPACK, use the following command:</p> Bash<pre><code>spack uninstall program_name@version\n</code></pre> <p>For example:</p> Bash<pre><code>spack uninstall nwchem@7.0.2\n</code></pre> <p>To uninstall a program installed by EasyBuild, use the following command:</p> Bash<pre><code>eb -r program_eb_file\n</code></pre> <p>For example:</p> Bash<pre><code>eb -r GROMACS-2020.4-foss-2020a.eb\n</code></pre> <p>Make sure to replace <code>program_name@version</code> and <code>program_eb_file</code> with the actual program name and version or EasyBuild file name.</p> <p>For more details, refer to the LUMI documentation.</p>"},{"location":"access/lumi-start/","title":"LUMI Supercomputer","text":"<p>Info</p> <p>If you face any problems during registration to LUMI, let us know by email at hpcsupport@taltech.ee or in Teams in the \"HPC Support Chat\".</p>"},{"location":"access/lumi-start/#getting-started-on-lumi","title":"Getting Started on LUMI","text":"<ol> <li> <p>Log in to minu.etais.ee with MyAccessID.</p> <p></p> </li> <li> <p>Choose your affiliation (ttu.ee).</p> <p></p> </li> <li> <p>Identify yourself with Uni-ID (six letters taken from the user\u2019s full name), but for long time employees, it could be name.surname.</p> <p></p> </li> <li> <p>Agree with all propositions and press the button to continue.</p> <p></p> </li> <li> <p>After notification of successful account creation, you need to open a new tab and go to mms.myaccessid.org.</p> <p></p> <p>After filling in the required fields and agreeing with the use policy and terms of use, press the \"Submit\" button.</p> </li> <li> <p>Register your SSH key in MyAccessID. To do this, click on <code>Manage SSH key</code>.</p> <p></p> <p>Then add your SSH key into the corresponding field. In Linux and Mac, the public SSH key can be found in the <code>.ssh/id_rsa.pub</code> file. Windows by default saves the public SSH key at <code>C:\\Users\\your_username/.ssh/id_rsa.pub</code>. Instructions on how to generate SSH keys can be found here.</p> <p></p> <p>It is important to add your SSH key immediately after account creation, as this SSH key will be automatically transferred to LUMI and used for user authentication during the first and subsequent connections to LUMI.</p> </li> <li> <p>If you are a project leader -- Contact the Resource Allocator (<code>hpcsupport@taltech.ee</code>). The HPC Centre will add LUMI resources to your account. If the name of the project is already known, add it as well.    If you are a team member -- contact your project leader (or course teacher) to be added to a project.</p> <p>NB! Just adding HPC-LUMI resources to an existing project will not work.</p> </li> <li> <p>to After your you minu.etais.ee receive account. an answer from the HPC Centre, log in to your minu.etais.ee account.</p> </li> <li> <p>The corresponding project appears in your ETAIS account.</p> <p></p> </li> <li> <p>In a short time, you will receive a letter from <code>info-noreply@csc.fi</code> where you will be given your username for LUMI.</p> </li> <li> <p>Try to connect to LUMI with the received username using the command:</p> Bash<pre><code>ssh LUMI-user-name@lumi.csc.fi\n</code></pre> <p>NB! Synchronization may take some time, so if you have problems with connection with the SSH key, wait longer and try again.</p> </li> <li> <p>Read the LUMI documentation. Especially check the different filesystems and their prices, since LUMI charges TB/hour, as well as the guide for containerization of Python environments (pip, conda).</p> </li> </ol>"},{"location":"access/lumi/","title":"LUMI","text":""},{"location":"access/lumi/#what-is-lumi","title":"What is LUMI?","text":"<p>LUMI is the fastest supercomputer in Europe. It's an HPE Cray EX supercomputer consisting of several hardware partitions targeted at different use cases:</p> <ul> <li>2560 GPU-based nodes (LUMI-G), each node with one 64-core AMD Trento CPU and four AMD MI250X GPUs.</li> <li>1536 dual-socket CPU nodes (LUMI-C) with 64-core 3rd-generation AMD EPYC\u2122 CPUs, and between 256 GB and 1024 GB of memory.</li> <li>Large memory GPU nodes (LUMI-D), with a total of 32 TB of memory in the partition for data analytics and visualization.</li> <li>Main storage - (LUMI-P) has 4 independent Lustre file systems with 20 PB and an aggregate bandwidth of 240 GB/s each. Each Lustre file system is composed of 1 MDS (metadata server) and 32 Object Storage Targets (OSTs).</li> <li>Flash storage - (LUMI-F) has a Lustre file system with a storage capacity of 8 PB and an aggregate bandwidth of 1,740 GB/s.</li> <li>Object store - (LUMI-O) provides 30 PB of storage.</li> </ul> <p>More about LUMI system architecture can be found in the overview and LUMI\u2019s full system architecture.</p> <p>LUMI uses Slurm as a job scheduler and resource manager just as the TalTech HPC centre. Slurm partitions can be allocated by node or by resources. More about partitions can be found here.</p>"},{"location":"access/lumi/#why-lumi","title":"Why LUMI?","text":"<p>There are several reasons to choose LUMI instead of HPC:</p> <ul> <li>If the job is run using GPUs (note that LUMI uses AMD GPUs).</li> <li>If the job needs a lot of memory and TalTech HPC machines with large memory are queued up.</li> <li>If the queue on TalTech HPC is too long.</li> </ul>"},{"location":"access/lumi/#getting-started","title":"Getting started","text":"<ul> <li>How to get access to LUMI</li> <li>Software</li> <li>Examples of jobs and Slurm scripts</li> <li>Billing</li> </ul>"},{"location":"access/mpi/","title":"Available MPI Versions (with comparison)","text":"<p>The cluster has OpenMPI installed.</p> <p>On all nodes:</p> Bash<pre><code>module load openmpi/4.1.1-gcc-10.3.0-r8-tcp\n</code></pre> <p>NB: Currently, only use TCP transport! There are several OpenMPI modules for different versions. Different modules for the same version only differ in environment variables for transport selection.</p> <p>Normally, OpenMPI will choose the fastest interface. It will try RDMA over Ethernet (RoCE), which causes \"[qelr_create_qp:683] create qp: failed on ibv_cmd_create_qp\" messages. These can be ignored as it will fail over to IB (higher bandwidth anyway) or TCP.</p> <p>NB: <code>mpirun</code> is not available; use <code>srun</code>.</p> <p>For MPI jobs, prefer the green-ib partition (<code>#SBATCH -p green-ib</code>) or stay within a single node (<code>#SBATCH -N 1</code>).</p> Bash<pre><code>export OMPI_MCA_btl_openib_warn_no_device_params_found=0 srun ./hello-mpi\n</code></pre>"},{"location":"access/mpi/#layers-in-openmpi","title":"Layers in OpenMPI","text":"<ul> <li>PML = Point-to-point Management Layer:</li> <li>UCX</li> <li>MTL = Message Transfer Layer:</li> <li>PSM</li> <li>PSM2</li> <li>OFI</li> <li>BTL = Byte Transfer Layer:</li> <li>TCP</li> <li>openib</li> <li>self</li> <li>sm (OpenMPI 1), vader (OpenMPI 4)</li> </ul> <p>The layers can be confusing. For example, openib was originally developed for InfiniBand but is now used for RoCE and is deprecated for IB. However, on some IB cards and configurations, it is the only working option. Also, the MVAPICH implementation still uses openib (verbs) instead of UCX.</p> <p>Layers can be selected using environment variables:</p> <p>To select TCP transport:</p> Bash<pre><code>export OMPI_MCA_btl=tcp,self,vader\n</code></pre> <p>To select RDMA transport (verbs):</p> Bash<pre><code>export OMPI_MCA_btl=openib,self,vader\n</code></pre> <p>To select UCX transport:</p> Bash<pre><code>export OMPI_MCA_pml=ucx\n</code></pre> <p>NB! UCX is not supported on QLogic FastLinQ QL41000 Ethernet controllers.</p> <p>For further explanations and details, see:</p> <ul> <li>https://www.open-mpi.org/faq/?category=tuning</li> <li>https://www.open-mpi.org/faq/?category=openfabrics</li> </ul> <p>Different MPI implementations exist:</p> <ul> <li>OpenMPI</li> <li>MPICH</li> <li>MVAPICH</li> <li>IBM Platform MPI (MPICH descendant)</li> <li>IBM Spectrum MPI (OpenMPI descendant)</li> <li>(at least one for each network and CPU manufacturer)</li> </ul>"},{"location":"access/mpi/#openmpi","title":"OpenMPI","text":"<ul> <li>Available in any Linux or BSD distribution</li> <li>Combines technologies and resources from several other projects (incl. LAM/MPI)</li> <li>Can use TCP/IP, shared memory, Myrinet, InfiniBand, and other low latency interconnects</li> <li>Chooses the fastest interconnect automatically (can be manually chosen too)</li> <li>Well integrated into many schedulers (e.g., SLURM)</li> <li>Highly optimized</li> <li>FOSS (BSD license)</li> </ul>"},{"location":"access/mpi/#mpich","title":"MPICH","text":"<ul> <li>Highly optimized</li> <li>Supports TCP/IP and some low latency interconnects</li> <li>(Older versions) DO NOT support InfiniBand (however, it supports MELLANOX IB)</li> <li>Available in many Linux distributions</li> <li>? Not integrated into schedulers </li> <li>Used to be a PITA to get working smoothly</li> <li>FOSS</li> </ul>"},{"location":"access/mpi/#mvapich","title":"MVAPICH","text":"<ul> <li>Highly optimized (maybe slightly faster than OpenMPI)</li> <li>Fork of MPICH to support IB</li> <li>Comes in many flavors to support TCP/IP, InfiniBand, and many low latency interconnects: OpenSHMEM, PGAS</li> <li>Need to install several flavors, and users need to choose the right one for the interconnect they want to use</li> <li>Generally not available in Linux distributions</li> <li>Not integrated with schedulers (integrated with SLURM only after version 18)</li> <li>FOSS (BSD license)</li> </ul>"},{"location":"access/mpi/#recommendation","title":"Recommendation","text":"<ul> <li>Default: use OpenMPI on our clusters</li> <li>If unsatisfied with performance and running on a single node or over TCP, try MPICH</li> <li>If unsatisfied with performance and running on IB, try MVAPICH</li> </ul> <p>For a comparison, see for example:</p> <ul> <li>https://www.chpc.utah.edu/documentation/software/mpilibraries.php</li> <li>https://stackoverflow.com/questions/2427399/mpich-vs-openmpi</li> </ul>"},{"location":"access/putty/","title":"Putty","text":"<p>Warning</p> <p>This page has not been completely updated for Rocky 8 yet!</p> <p>PuTTY is a free and open-source terminal emulator for Windows, Mac and Linux. For TalTech Windows machines PuTTY client should be download from Microsoft Apps, in a all other cases Putty can be download from the official webpage.  </p>"},{"location":"access/putty/#setup","title":"Setup","text":"<ol> <li> <p>Open Putty</p> </li> <li> <p>Write the host name, the name of a session and save it.</p> <p></p> </li> <li> <p>To enable running of graphical applications, go to SSH and enable X11 forwarding.</p> <p></p> </li> </ol>"},{"location":"access/samba/","title":"Accessing SMB/CIFS network shares","text":"<p>Warning</p> <p>This page has not been completely updated for Rocky 8 yet!</p> <p>The HPC center exports two filesystems as Windows network shares:</p> Local Path on Cluster Linux Network URL Windows Network URL /gpfs/mariana/smbhome/$USER smb://smb.hpc.taltech.ee/smbhome \\\\smb.hpc.taltech.ee\\smbhome /gpfs/mariana/smbgroup smb://smb.hpc.taltech.ee/smbgroup \\\\smb.hpc.taltech.ee\\smbgroup /gpfs/mariana/home/$USER not exported not exported <p>These can be accessed from within the university or from EduVPN.</p> <p>Each user automatically has a directory within smbhome. To get a directory for group access, please contact us (a group and a directory need to be created).</p>"},{"location":"access/samba/#windows-access","title":"Windows Access","text":"<p>From Windows, the shares can be found using the Explorer \"Map Network Drive\".</p>"},{"location":"access/samba/#gui","title":"GUI","text":"<p>Right-click on \"This PC\" and select \"Add a network location\" or \"Map network drive\".</p> Text Only<pre><code>Server: \\\\smb.hpc.taltech.ee\\smbhome\nUsername: INTRA\\&lt;uni-id&gt;\n</code></pre> <p> </p>"},{"location":"access/samba/#powershell","title":"PowerShell","text":"<p>Run the following command:</p> PowerShell<pre><code>net use \\\\smb.hpc.taltech.ee\\smbhome /user:INTRA\\&lt;uni-id&gt;\n</code></pre> <p>Check success with:</p> PowerShell<pre><code>get-smbconnection\n</code></pre>"},{"location":"access/samba/#linux-access","title":"Linux Access","text":"<p>On Linux with a GUI Desktop, the shares can be accessed with the Nautilus browser.</p> <p>From the Linux command line, the shares can be mounted as follows:</p> Bash<pre><code>dbus-run-session bash\ngio mount smb://smb.hpc.taltech.ee/smbhome/\n</code></pre> <p>or</p> Bash<pre><code>dbus-run-session bash\ngio mount smb://smb.hpc.taltech.ee/smbgroup/\n</code></pre> <p>You will be asked for \"User\" (which is your UniID), \"Domain\" (which is \"INTRA\"), and your password.</p> <p>To disconnect from the share, unmount with:</p> Bash<pre><code>gio mount -u smb://smb.hpc.taltech.ee/smbhome/\ngio mount -u smb://smb.hpc.taltech.ee/smbgroup/\n</code></pre> <p>If you get \"Error mounting location: Location is not mountable\", then you are not in the correct network (e.g., VPN is not running), or you don't have a dbus session.</p> <p>On Debian, the following packages need to be installed: <code>gvfs gvfs-common gvfs-daemons gvfs-fuse gvfs-libs libsmbclient gvfs-backends libglib2.0-bin</code></p>"},{"location":"access/ssh/","title":"Getting SSH keys to work","text":"<p>Warning</p> <p>This page has not been completely updated for Rocky 8 yet!</p>"},{"location":"access/ssh/#generating-ssh-keys","title":"Generating SSH keys","text":"<p>In Linux and macOS, SSH keys can be generated in the terminal with the command:</p> Bash<pre><code>ssh-keygen -t rsa\n</code></pre> <p>In Windows, SSH keys can be generated in PowerShell with the command:</p> Bash<pre><code>ssh-keygen.exe -t rsa\n</code></pre> <p>The program prompts the user through the process, setting the location and asking the user to set a passphrase, which should be created. This process ends with two keys in the path it showed. One of them is named (by default) <code>id_rsa</code> and the other <code>id_rsa.pub</code>. The <code>.pub</code> key is your public key, the other is a private key.</p> <p></p> <p>Note: Never share your private key with anyone.</p> <p>More detailed guides can be found here:</p> <ul> <li>Linux - Generate SSH Keys on Linux</li> <li>macOS - Manually Generating Your SSH Key in Mac OS X</li> <li>Windows - Generate SSH Key on Windows 10</li> </ul>"},{"location":"access/ssh/#uploading-ssh-keys-to-base","title":"Uploading SSH keys to base","text":"<p>Once the keys are created, the public (.pub) key needs to be uploaded to the base. There are a couple of ways to do it. On the base, SSH public keys are found in the <code>/etc/AuthorizedKeys/$USER</code> file, and there is a link to it from the <code>.ssh/authorized_keys</code> file.</p> <p>Several SSH keys can be used simultaneously to access the same user account in case of using several different devices.</p> <p>On Mac and Linux, to copy keys to the cluster:</p> Bash<pre><code>ssh-copy-id Uni-ID@base.hpc.taltech.ee\n</code></pre> <p></p> <p>On Windows, it can be copied in PowerShell with this command:</p> Bash<pre><code>type $env:USERPROFILE\\.ssh\\id_rsa.pub | ssh Uni-ID@base.hpc.taltech.ee \"cat &gt;&gt; .ssh/authorized_keys\"\n</code></pre> <p>A more thorough explanation with an example can be found here.</p>"},{"location":"jobs/singularity/","title":"Containers (Singularity &amp; Docker)","text":"<p>Containers are a popular way of creating a reproducible software environment. Container solutions include Docker and Singularity/Apptainer. We support Singularity.</p> <p>The Singularity user guides are a great resource for learning what you can do with Singularity.</p>"},{"location":"jobs/singularity/#running-a-container","title":"Running a container","text":"<p>Native installation from Rocky 8 EPEL of <code>singularity-ce version 4.1.5-1.el8</code>, no modules to load.</p>"},{"location":"jobs/singularity/#on-all-nodes-using-cpu-only","title":"On all nodes using CPU only","text":"<p>Pull the Docker image you want, here <code>ubuntu:18.04</code>:</p> Bash<pre><code>singularity pull docker://ubuntu:18.04\n</code></pre> <p>Write an sbatch file (here called <code>ubuntu.slurm</code>):</p> Bash<pre><code>#!/bin/bash\n#SBATCH -t 0-00:30\n#SBATCH -N 1\n#SBATCH -c 1\n#SBATCH --cpus-per-task=2   # Singularity can use multiple cores\n#SBATCH --mem-per-cpu=4000\nsingularity exec docker://ubuntu:18.04 cat /etc/issue\n</code></pre> <p>Submit to the queueing system with:</p> Bash<pre><code>sbatch ubuntu.slurm\n</code></pre> <p>When the resources become available, your job will be executed.</p>"},{"location":"jobs/singularity/#on-gpu-nodes-using-gpu","title":"On GPU nodes (using GPU)","text":"<p>When running Singularity through SLURM (srun, sbatch), only GPUs reserved through SLURM are visible to Singularity.</p> <p>Pull the Docker image you want, here <code>ubuntu:20.04</code>:</p> Bash<pre><code>singularity pull docker://ubuntu:20.04\n</code></pre> <p>Write an sbatch file (here called <code>ubuntu.slurm</code>):</p> Bash<pre><code>#!/bin/bash\n#SBATCH -t 0-00:30\n#SBATCH -N 1\n#SBATCH -c 1\n#SBATCH -p gpu\n#SBATCH --gres=gpu:A100:1     # Only use this if your job actually uses GPU\n#SBATCH --mem-per-cpu=4000\nsingularity exec --nv docker://ubuntu:20.04 nvidia-smi\n# or singularity exec --nv ubuntu_20.04.sif nvidia-smi\n# The --nv option to Singularity passes the GPU to it\n</code></pre> <p>Submit to the queueing system with:</p> Bash<pre><code>sbatch ubuntu.slurm\n</code></pre> <p>When the resources become available, your job will be executed.</p> <p>For more on Singularity and GPUs, see https://sylabs.io/guides/3.9/user-guide/gpu.html.</p>"},{"location":"jobs/singularity/#hints","title":"Hints","text":"<p>By default, there is no network isolation in Singularity, so there is no need to map any port (-p in Docker). If the process inside the container binds to an IP:port, it will be immediately reachable on the host. Singularity also mounts <code>$HOME</code> and <code>$TMP</code> by default, so the directory you run the container from will be the working directory within the container (unless the directory is not on the same filesystem as <code>$HOME</code>).</p> <p>Singularity will use all cores reserved using <code>--cpus-per-task</code>. If fewer should be used, the Singularity parameter <code>--cpus</code> can be used. Similarly, if a container should use less memory, this can be restricted by the Singularity parameter <code>--memory</code>. These parameters can be useful if a single batch job starts several containers concurrently.</p>"},{"location":"jobs/singularity/#example-interactive-pytorch-job-without-and-with-gpu","title":"Example: Interactive PyTorch job (without and with GPU)","text":"<p>Start an interactive session on AMP, make the modules available, and run the Docker image in Singularity:</p> <p>Without GPU:</p> Bash<pre><code>srun -t 1:00:00 --pty bash\nsingularity exec docker://pytorch/pytorch python\n</code></pre> <p>With GPU:</p> Bash<pre><code>srun -t 1:00:00 -p gpu --gres=gpu:1 --pty bash\nsingularity exec --nv docker://pytorch/pytorch python\n</code></pre> <p>Inside the container Python session, run:</p> Python<pre><code>import torch\ntorch.cuda.is_available()\ntorch.cuda.get_device_name()\n</code></pre> <p>You can also shorten it to a single command:</p> Bash<pre><code>srun -t 1:00:00 -p gpu --mem 32G --gres=gpu:1 singularity exec docker://pytorch/pytorch python -c \"import torch;print(torch.cuda.is_available())\"\n</code></pre> <p>This should give the same result (without the GPU name). If you remove the <code>--nv</code> flag, the result changes as Singularity no longer exposes the GPU.</p>"},{"location":"jobs/singularity/#example-interactive-tensorflow-job-without-and-with-gpu","title":"Example: Interactive TensorFlow job (without and with GPU)","text":"<p>Start an interactive session on AMP, make the modules available, and run the Docker image in Singularity:</p> <p>Without GPU:</p> Bash<pre><code>srun -t 1:00:00 --mem=16G --pty bash\nsingularity run docker://tensorflow/tensorflow\n</code></pre> <p>With GPU:</p> Bash<pre><code>srun -t 1:00:00 -p gpu --gres=gpu:1 --mem=16G --pty bash\nsingularity run --nv docker://tensorflow/tensorflow:latest-gpu\n</code></pre> <p>With GPU and Jupyter:</p> Bash<pre><code>srun -t 1:00:00 -p gpu --gres=gpu:1 --mem=16G --pty bash\nsingularity run --nv docker://tensorflow/tensorflow:latest-gpu-jupyter\n</code></pre> <p>Inside the container, run:</p> Python<pre><code>python\nfrom tensorflow.python.client import device_lib\nprint(device_lib.list_local_devices())\n</code></pre> <p>The following is the \"TensorFlow 2 quickstart for beginners\" from https://www.tensorflow.org/tutorials/quickstart/beginner. Continue inside the Python:</p> Python<pre><code>import tensorflow as tf\nprint(\"TensorFlow version:\", tf.__version__)\nmnist = tf.keras.datasets.mnist\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\n\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Flatten(input_shape=(28, 28)),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(10)\n])\npredictions = model(x_train[:1]).numpy()\npredictions\ntf.nn.softmax(predictions).numpy()\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nloss_fn(y_train[:1], predictions).numpy()\nmodel.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=5)\nmodel.evaluate(x_test,  y_test, verbose=2)\nprobability_model = tf.keras.Sequential([\n    model,\n    tf.keras.layers.Softmax()\n])\nprobability_model(x_test[:5])\n</code></pre>"},{"location":"jobs/singularity/#example-job-for-opendronemap-odm","title":"Example job for OpenDroneMap (ODM)","text":"<p>OpenDroneMap needs a writable directory for the data. This directory needs to contain a subdirectory named <code>images</code>.</p> <p>Assume you keep your ODM projects in the directory <code>opendronemap</code>:</p> Text Only<pre><code>opendronemap\n|\n|-Laagna-2021\n| |\n| |-images\n|\n|-Paldiski-2015\n| |\n| |-images\n|\n|-Paldiski-2018\n| |\n| |-images\n|\n|-TalTech-2015\n| |\n| |-images\n</code></pre> <p>If you want to create a 3D model for Laagna-2021, you would run the following Singularity command:</p> Bash<pre><code>singularity run --bind $(pwd)/opendronemap/Laagna-2021:/datasets/code docker://opendronemap/odm --project-path /datasets\n</code></pre> <p>For creating a DEM, you would need to add <code>--dsm</code> and potentially <code>-v \"$(pwd)/odm_dem:/code/odm_dem\"</code>.</p> <p>GPU use for Singularity is enabled with the <code>--nv</code> switch. Be aware that ODM uses the GPU only for the matching, which is only a small percentage of the time of the whole computation.</p> <p>The SLURM job script looks like this:</p> Bash<pre><code>#!/bin/bash\n#SBATCH --nodes 1\n#SBATCH --ntasks 1\n#SBATCH --cpus-per-task=10\n#SBATCH --time 01:30:00\n#SBATCH --partition gpu\n#SBATCH --gres=gpu:A100:1\n\nsingularity run --nv --bind $(pwd)/opendronemap/Laagna-2021:/datasets/code docker://opendronemap/odm --project-path /datasets --dsm\n</code></pre>"},{"location":"jobs/singularity/#obtaining-and-building-singularity-containers","title":"Obtaining and Building Singularity Containers","text":"<p>When you want to use a container with the cluster, you'll need to get the image from somewhere. You cannot build containers on the cluster for security reasons (even with <code>--fakeroot</code>), so there are two ways to get your containers into the cluster.</p>"},{"location":"jobs/singularity/#from-container-registries","title":"From Container Registries","text":"<p>Singularity can pull and convert Docker images from Docker container registries (most significantly Docker Hub) directly into Singularity images. This is the method used in the previous examples. You can read more here: https://docs.sylabs.io/guides/3.9/user-guide/singularity_and_docker.html.</p> <p>You can also use GitHub's Container Registry or TalTech's Software Science GitLab (You'll need to sign in with an access token to pull containers from the registry. More on that here: https://docs.sylabs.io/guides/3.9/user-guide/endpoint.html).</p>"},{"location":"jobs/singularity/#building-images-locally-then-moving-to-cluster","title":"Building images locally then moving to cluster","text":"<p>Since Singularity images are single files, you can transfer them quite easily with any tool used to sync data with the cluster, such as <code>scp</code> or <code>rsync</code>. You can build locally with either just the <code>singularity</code> tool or <code>singularity</code> and <code>docker</code>.</p> <ul> <li>Building images from a Singularity definition file then transferring to the cluster.</li> <li>Building images with Docker from Dockerfiles, then saving the image with <code>docker save</code> to an archive, e.g.:</li> </ul> Bash<pre><code>docker build -t pytorch .\ndocker save pytorch | gzip &gt; pytorch.tar.gz\n</code></pre> <p>This creates a file <code>pytorch.tar.gz</code>, which you can either convert to a Singularity image locally with <code>singularity build docker-archive://pytorch.tar.gz</code> or move the archive to the cluster and build from there. Building from a Docker archive is the only form of image building allowed in the cluster.</p>"},{"location":"jobs/slurm-example/","title":"Examples of slurm scripts for submitting jobs","text":"<p>Warning</p> <p>This page has not been completely updated for Rocky 8 yet!</p>"},{"location":"jobs/slurm-example/#a-single-process-job","title":"A single process job","text":"<p>The following script launches a job named <code>bowtie2</code> using one thread. A directory <code>bowtie2-results</code> will be created where results will be written into the <code>bowtie2-%J.log</code> output file (<code>%J</code> is a job allocation number and step number in the format <code>jobid.stepid</code>).</p> Bash<pre><code>#!/bin/bash \n#SBATCH --job-name=bowtie2        ### job name \n#SBATCH --output=bowtie2-%J.log   ### output file \n#SBATCH --ntasks=1                ### number of threads   \n\n## load bowtie2 environment \nmodule load bowtie2-2.1.0\n\n## creating directory for results \nmkdir bowtie2-results \ncd bowtie2-results \n\n## building bowtie2 index \nbowtie2-build $BT2_HOME/example/reference/lambda_virus.fa lambda_virus\n\n## aligning against the index, output to eg1.sam file \nbowtie2 -x lambda_virus -U $BT2_HOME/example/reads/reads_1.fq -S eg1.sam\n</code></pre>"},{"location":"jobs/slurm-example/#an-openmp-parallel-job","title":"An OpenMP parallel job","text":"<p>The following script launches a job named <code>HelloOMP</code> using OpenMP. For this job, Slurm reserves one node and 12 threads. The maximum run time is 10 minutes.</p> <p>Even though it is <code>--cpus-per-task</code>, Slurm reserves threads, not CPUs, since \"cpu\" in Slurm's language is the smallest unit.</p> <p>Note: Each thread needs sufficient work to do to make up for the time spent in launching the thread. Therefore, it is not useful to run small/short jobs in parallel.</p> Bash<pre><code>#!/bin/bash\n#SBATCH --job-name=HelloOMP        ### job name           -J\n#SBATCH --time=00:10:00            ### time limit         -t\n#SBATCH --nodes=1                  ### number of nodes    -N \n#SBATCH --ntasks-per-node=1        ### number of tasks (MPI processes)\n#SBATCH --cpus-per-task=12         ### number of threads per task (OMP threads)\n\n## load environment    \nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n\n## run job\n./hello_omp \n</code></pre> <p>NOTE: Parallel does not (necessarily) mean faster!!! Parallel execution introduces overhead (starting threads, communication)! For optimal execution time and optimal use of resources, one needs to test and find the sweet spot.</p>"},{"location":"jobs/slurm-example/#a-script-for-mpi-parallel-job-openfoam","title":"A script for MPI parallel job (OpenFOAM)","text":"<p>The following script reserves 4 CPU cores for 10 hours (since <code>mpirun</code> uses cores by default), loads the OpenMPI module, the OpenFOAM variables, changes into the case directory, and runs the typical commands necessary for a parallel OpenFOAM job. It also sets OpenMPI transport properties to use Ethernet TCP!</p> <p>It would be possible to request all tasks to be on the same node using the <code>-N</code> and <code>--tasks-per-node</code> options. This would be useful to make use of the very low latency shared memory communication of MPI (provided the job fits into the RAM of a single node).</p> <p>Flag <code>-l</code> in the <code>#!/bin/bash</code> row means that settings in <code>/home/user/.bash_profile</code> will be executed.</p> <p>Note: Each task needs sufficient work to do to make up for the time spent with inter-process communication. Therefore, it is not useful to run small/short jobs in parallel.</p> Bash<pre><code>#!/bin/bash -l    \n#SBATCH -n 4                     ### number of CPUs \n#SBATCH -t 10:00:00              ### run time   \n#SBATCH -J openfoam-damBreak     ### job name\n#SBATCH --partition=infiniband   ### partition \n\n## load environment    \nmodule load rocks-openmpi\nsource /share/apps/OpenFOAM/OpenFOAM-v1912/etc/bashrc\n\n## run program\ncd $WM_PROJECT_USER_DIR/damBreak/damBreak\nblockMesh\ndecomposePar\nsetFields\nmpirun -n $SLURM_NTASKS interFoam -parallel\nreconstructPar\n</code></pre> <p>NOTE: Parallel does not (necessarily) mean faster!!! Parallel execution introduces overhead (starting threads, communication)! For optimal execution time and optimal use of resources, one needs to test and find the sweet spot.</p> <p></p> <p>The division into the areas is a combined decision taking into account \"real\" (wall clock) and \"user\" (summed time of all threads) time (from the time command). \"Wall clock\" (real) time is the time one needs to wait till the job is finished, \"Summed thread time\" (user) is the sum of the times that all individual threads needed, it should be roughly user = numtreads x real. For parallel programs, one can expect that \"user\" time of the parallel run is larger than for the sequential, due to communication overhead, if it is smaller, that probably means the individual threads could make better use of cache.</p> area why explanation when to use sweet spot minimal \"user\" time = minimal heat production, optimal use of resources regular use good range linear speedup for \"real\", with constant or slightly increasing \"user\" approaching deadline OK range slightly less than linear speedup for \"real\", and slightly increasing \"user\" pushing hard to make a deadline avoid ascending slope in the diagram for \"real\" and \"user\" one actually needs to wait longer compared to the case with fewer cores NEVER <p>Recommended in this case would be to request 8 threads <code>-n 8 --ntasks-per-node 8</code> but use <code>mpirun -n 4</code>. OpenFOAM does not seem to benefit from hyperthreading.</p>"},{"location":"jobs/slurm-example/#an-array-parameter-sweep-job","title":"An array (parameter sweep) job","text":"<p>This script reserves 10 threads and runs an array of jobs in the range of 13-1800. The <code>$SLURM_ARRAY_TASK_ID</code> variable calls the input files in the given range in turn and data is written in output files <code>arrayjob</code>, which also contain job allocation ID and job array index number (<code>-%A</code> and <code>-%a</code>, respectively).</p> Bash<pre><code>#!/bin/bash \n#SBATCH --job-name=array-parameter-scan  ### job name  \n#SBATCH --output=arrayjob-%A-%a          ### output file \n#SBATCH --ntasks=10                      ### number of threads  \n#SBATCH --array=13-1800                  ### Array tasks for parameter sweep\n\n## run job\n./myarrayjob $SLURM_ARRAY_TASK_ID\n</code></pre>"},{"location":"jobs/slurm-example/#a-gpu-job","title":"A GPU job","text":"<p>The GPU scripts can be run only on amp.</p> <p>The following script reserves 1 GPU (Nvidia A100), uses the GPU partition, and has a time limit of 10 minutes.</p> Bash<pre><code>#!/bin/bash \n#SBATCH --job-name=uses-gpu        ### job name\n#SBATCH -p gpu                     ### use gpu\n#SBATCH --gres=gpu:A100:1          ### specifying the GPU type\n#SBATCH -t 00:10:00                ### time limit \n\n## run job    \n./mygpujob\n</code></pre> <p>This script reserves 4 GPUs without specifying the GPU type.</p> Bash<pre><code>#!/bin/bash \n#SBATCH --job-name=uses-gpu        ### job name\n#SBATCH -p gpu                     ### use gpu\n#SBATCH --gres=gpu:4               ### number of GPUs\n#SBATCH -t 00:10:00                ### time limit \n\n## run job    \n./mygpujob\n</code></pre>"},{"location":"jobs/slurm-example/#a-job-using-the-scratch-partition-sequential-or-openmp-parallel","title":"A job using the scratch partition (sequential or OpenMP parallel)","text":"<p>The following script creates a directory named <code>scratch-%x-%j</code> (where <code>%x</code> is a job name and <code>%j</code> is a job ID of the running job). This scratch directory is done on the scratch partition of the node to provide fast local storage that does not require a network. After, the Slurm script runs the job and copies the output files back into the permanent home directory once the job is completed.</p> Bash<pre><code>#!/bin/bash -l\n\n#SBATCH -N 1                     ### number of nodes\n#SBATCH -t 00:10:00              ### time limit  \n#SBATCH -J using-scratch         ### job name\n\n## creates scratch directory, copy files from working directory to scratch directory, goes to scratch directory\nmkdir /state/partition1/scratch-%x-%j\ncp -r $HOME/were/input/is/* /state/partition1/scratch-%x-%j/\ncd /state/partition1/scratch-%x-%j/\n\n## run job\nmyjob\n\n## copy files from scratch directory to working directory and remove scratch directory\ncp -r /state/partition1/scratch-%x-%j/* $HOME/were/input/is\nrm -rf /state/partition1/scratch-%x-%j\n</code></pre> <p>Please note that the scratch is not shared between nodes, so parallel MPI jobs that span multiple nodes cannot access each other's scratch files.</p>"},{"location":"jobs/slurm-statistics/","title":"SLURM finished job statistics","text":"<p>Warning</p> <p>This page has not been completely updated for Rocky 8 yet!</p> <p>A statistics file is created after finishing a job, which shows hardware and job-related info to the user. Statistics files are separated by the ID of the job, and the filename is generated using the SLURM environment variable allocated to the job as the ID: <code>slurm-$SLURM_JOB_ID.stat</code>. The <code>.stat</code> file is created in the user\u2019s current working directory where the job is run.</p> <p>The statistics file contains:</p> <ul> <li>JobID</li> <li>The state of the job upon completion</li> <li>Nodelist</li> <li>Cores per node</li> <li>CPU Utilized (This figure represents the time that the CPUs were used.)</li> <li>CPU Efficiency (Represents the percentage that the CPUs were used versus the overall runtime of the job.)</li> <li>Job Wall-clock time (The duration of the job.)</li> <li>Memory Utilized (Total amount of memory utilized in kilobytes)</li> <li>Memory Efficiency (Percentage of total memory allocated)</li> <li>MaxDiskWrite (Displays disk usage)</li> <li>Command (Displays the command that was run for this job, making it easier for users to identify where an error might have occurred). This info may be used to better allocate resources depending on the job or modify the job scripts so that they use the allocated resources more efficiently.</li> </ul> <p>Disabling this feature: Create the file <code>.no_slurm_stat</code> by running the command: <code>touch ~/.no_slurm_stat</code>.</p> <p>To re-enable this feature, simply remove the <code>.no_slurm_stat</code> by running the following command: <code>rm ~/.no_slurm_stat</code></p>"},{"location":"learning/learning/","title":"Courses and introductions","text":"<p>The HPC Centre provides an online introductory course on Moodle.</p> <p>The course consists of four modules:</p> <ul> <li>Module 1: Linux Command Line</li> <li>Module 2: Introduction</li> <li>Module 3: Introduction to OpenStack/ETAIS Use (in preparation)</li> <li>Module 4: Remote Visualization</li> </ul>"},{"location":"learning/learning/#resources-for-learning-linux","title":"Resources for Learning Linux","text":"<p>Linux command-line (shell, terminal):</p> <ul> <li>Linux Command-Line Tutorial (Learning the Shell)</li> <li>Linux Command-Line Tutorial</li> <li>Linux Command-Line Tutorial</li> <li>Debian Handbook</li> <li>Learn Linux the Hard Way</li> <li>The Linux Command Line</li> </ul> <p>TalTech courses:</p> <ul> <li>ICA0007 Linux Administration</li> </ul>"},{"location":"learning/learning/#resources-for-learning-scientific-computing","title":"Resources for Learning Scientific Computing","text":"<p>TalTech courses:</p> <ul> <li>YFX1510 Scientific Computing (requires knowledge of C programming and Linux)</li> <li>YFX0500 Introduction to Programming in Python</li> <li>YMX0110 Numerical Methods and Packages of Mathematics</li> </ul>"},{"location":"learning/performance/","title":"Performance","text":"<p>How fast is your program? How well does it use the available hardware? Could it run faster on the same hardware?</p>"},{"location":"learning/performance/#benchmarking","title":"Benchmarking","text":"<p>Benchmarking is the art and skill of finding out how fast your program and hardware are. Many factors influence the execution time of your program. The obvious ones are the speed and type of processors, speed of memory, etc., but the less obvious ones can have a huge impact (maybe larger than the speed of the processor). The less obvious ones include programming style and language! Yes, there are slow and fast languages. Slow languages include Python and BASIC, while fast languages include C, Fortran, Julia, and Rust. The programming style has an impact as well, as it influences memory access and optimization possibilities of the compiler/interpreter.</p> <p>To find out just how fast your hardware is, you need software that uses all of the available components in an optimal way.</p>"},{"location":"learning/performance/#desktop-vs-compute-node","title":"Desktop vs. Compute Node","text":"<p>Why is my (single-thread) job not faster on the cluster than on my desktop?</p> <ul> <li>Compute nodes do not have higher clock frequencies than desktop computers, but they have more cores and more RAM.</li> <li>A single-thread job on your desktop probably uses \u201cboost-frequency\u201d when the other cores are idle. A compute node usually has many busy cores, and therefore \u201cboost-frequency\u201d is not possible.</li> <li>A CPU typically has 6 memory channels. Several applications share these on the compute node, while on the desktop only your single application uses them.</li> </ul> <p>Why is my x-ntask parallel job not x-times as fast as my sequential job?</p> <ul> <li>Not enough to do for each core (task).</li> <li>Load imbalance, some tasks need to wait until another task finishes.</li> <li>Communication between tasks introduces overhead.</li> <li>Congestion of memory channels (see above).</li> </ul>"},{"location":"learning/performance/#parallel-scaling","title":"Parallel Scaling","text":"<p>The assumption that all programs run faster when more cores are used is generally wrong. Especially a sequential (single-threaded) program will not be able to use more than 1 core, but also for parallel programs, there is an optimum number of cores to use. The relationship between program speed and the number of cores used is called scaling. Usually, the scaling needs to be tested! It is not uncommon for programs to run slower when more cores are used. For the rational use of resources, it is necessary to determine the optimum for this particular program.</p>"},{"location":"learning/performance/#strong-scaling","title":"Strong Scaling","text":"<p>Strong scaling is defined as how the solution time varies with the number of processors for a fixed total problem size.</p> <p>If a simulation exhibits strong scaling, then the same problem is solved faster with linear speedup and the workload per processor is reduced. Strong scaling is mostly used for long-running CPU-bound applications. However, the speedup achieved by increasing the number of processes usually decreases.</p>"},{"location":"learning/performance/#weak-scaling","title":"Weak Scaling","text":"<p>Weak scaling is defined as how the solution time varies with the number of processors for a fixed problem size per processor.</p> <p>Thus, both the number of processors and the problem size are increased, which results in a constant workload per processor. Weak scaling is mostly used for large memory-bound applications.</p>"},{"location":"learning/performance/#how-many-cores-or-tasks-to-use","title":"How Many Cores or Tasks to Use?","text":"<p>NOTE: Parallel does not (necessarily) mean faster!!! Parallel execution introduces overhead (starting threads, communication)! For optimal execution time and optimal use of resources, one needs to test (run the same (a typical) simulation with different numbers of parallel tasks) and find the sweet spot.</p> <p></p> <p>The division into the areas is a combined decision taking into account \"real\" (wall clock) and \"user\" (summed time of all threads) time (from the time command). \"Wall clock\" (real) time is the time one needs to wait until the job is finished, \"Summed thread time\" (user) is the sum of the times that all individual threads needed. It should be roughly user = numtreads x real. For parallel programs, one can expect that \"user\" time of the parallel run is larger than for the sequential, due to communication overhead. If it is smaller, that probably means the individual threads could make better use of the cache.</p> Area Why Explanation When to Use Sweet spot Minimal \"user\" time = minimal heat production, optimal use of resources Regular use Good range Linear speedup for \"real\", with constant or slightly increasing \"user\" Approaching deadline OK range Slightly less than linear speedup for \"real\", and slightly increasing \"user\" Pushing hard to make a deadline Avoid Ascending slope in the diagram for \"real\" and \"user\" One actually needs to wait longer compared to the case with fewer cores NEVER <p>In our example case, it is recommended to request 4 or 8 threads; 8 threads if the software does not benefit from HyperThreading: <code>-n 8 --ntasks-per-node 8</code> but use <code>mpirun -n 4</code>, this would make sure the software has the full cores for itself and not be sharing it with other software.</p> <p>A rule of thumb for FEA/CFD is: keep at least 50000 cells per core (500k cells means not more than 10 tasks or cores).</p> <p>Here are a couple more examples:</p> <p>Matrix-matrix multiplication of two 2000x2000 matrices:</p> <p>Wall-clock time of the whole job:</p> ntasks None Bind cores Bind threads Bind sockets 2 57.198766 56.398039 57.485236 56.268668 4 28.018947 8 20.035140 16 11.012887 32 4.914047 5.050369 5.454213 40 3.951628 48 3.309776 <p>CPU-efficiency reported by SLURM:</p> ntasks None Bind cores Bind threads Bind sockets 2 97.46% 97.41% 97.46% 97.41% 4 94.17% 8 81.82% 16 77.68% 32 71.18% 69.10% 73.61% 40 61.11% 48 60.19% <p>Billed CPU-time:</p> ntasks None Bind cores Bind threads Bind sockets 2 1:56 4 2:00 8 3:28 16 3:44 32 4:48 40 6:00 48 7:12 <p></p> <p>A simple way to check if CPUs are busy enough: run an interactive session on the same node:</p> Bash<pre><code>srun -w green&lt;Nnmber&gt; --pty bash \nhtop\n</code></pre> <p>Ideally, the bars of your process are all green. To find your processor of your job, you can add a column or use:</p> Bash<pre><code>ps aux |grep $USER      # to get the PID \ntaskset -c -p PID \n</code></pre> <p>(Add 1 to the number from taskset to find the processor in htop, because taskset starts from 0, htop from 1).</p> <p>These jobs have spent over 90% CPU-time in calculations:</p> <p> </p> <p>This job spends already much more CPU-time in communication:</p> <p></p> <p>This job spends less than 10% CPU-time in calculation and over 90% in communication. Reducing the number of <code>ntasks</code> will probably speed it up considerably:</p> <p></p>"},{"location":"learning/performance/#clean-vs-fast-code-python","title":"Clean vs. Fast Code (Python)","text":"<p>Clean Code states:</p> <p>The first rule of functions is that they should be small. The second rule of functions is that they should be smaller than that.</p> <p>This rule is unfortunately often taken to the extreme, leading to several problems:</p> <ol> <li>Someone reading the code for the first time is hopping around the code to find out what the tiny functions are actually doing.</li> <li>Hot and cold code is mixed (instruction cache misses).</li> <li>Function calls are expensive, especially in Python, regarding computation time. (In contrast to Python, a C/C++/Fortran compiler may inline small functions, thus solving the issue, though there is no guarantee that the compiler will inline, not even with the inline statement.)</li> <li>A lot of variables have to be passed over multiple levels of function calls, thus increasing memory use and cache misses. Object orientation and global variables are used as a remedy for this, but this leads to functions with side effects (the reader does not know what variables the function is changing by looking at the function call).</li> </ol>"},{"location":"learning/performance/#an-example-of-the-time-function-calls-can-waste","title":"An Example of the Time Function Calls Can Waste","text":"<p>Time the following codes:</p> Python<pre><code>for i in range(n):\n    pass\n</code></pre> <p>and:</p> Python<pre><code>def f():\n    pass\n\nfor i in range(n):\n    f()\n</code></pre> <p>An average function call in Python costs about 150ns. This means that you lose about 2-3 orders of magnitude of processor speed! That means that your fancy 2.5 GHz from 2021 runs as slow as a 25 MHz processor from 1991 (or even as slow as a 2.5 MHz processor)! Now that's something to think about.</p> <p>A good read on this is the-cost-of-a-python-function-call and python_function_call_overhead; and so is small-functions-considered-harmful.</p> <p>To learn:</p> <p>Use meaningful function blocks. Define functions for code blocks that are re-used in other parts of the program. Do not define 1-line functions, except you have a very good reason!</p> <p>Are newer processors better/faster in every case? When does it make sense to switch to the newer nodes? When does it make sense to still use the older nodes?</p> <p>Intel Xeon E5-2630L 6C 2.00 GHz (max turbo 2.50 GHz) 4 memory channels 42.6 GB/s max memory bandwidth (~10.65 GiB/s memory bandwidth per channel) GFlops per core or CPU</p> <p>Memory channels per core: 4/6 (0.75) Bandwidth per core: ~7.99 GiB/s Bandwidth per GFlop:</p> <p>Xeon Gold 6148 20C 2.40 GHz (max turbo 3.70 GHz) Turbo Frequency (20 Cores): 3.00 GHz 6 memory channels 119.21 GiB/s max memory bandwidth 19.87 GiB/s memory bandwidth per channel GFlops per core or CPU</p> <p>Memory channels per core: 6/20 (0.30) Bandwidth per core: 5.961 GiB/s Bandwidth per GFlop: 0.133 GiB/s</p> <p>Bandwidth Single 19.87 GiB/s Double 39.74 GiB/s Quad 79.47 GiB/s Hexa 119.21 GiB/s</p>"},{"location":"learning/profiling/","title":"Profiling","text":"<p>Warning</p> <p>This page has not been completely updated for Rocky 8 yet!</p> <p>Profiling is the skill and art of finding which part of your code needs the most time, and therefore finding the place where you can (should, need to) optimize (first). The optimization can involve different things, like using library functions instead of self-written ones, rearranging memory access, removing function calls, or writing C/Fortran functions for your Python code.</p> <p>Profiling can be done manually by adding time and print statements to your code or (better) by using tools like Valgrind, TAU, HPCToolkit, Score-P, or Python's Scalene or cProfile.</p> <p>Tools to profile applications and perform efficiency, scaling, and energy analysis are described in this document by the Virtual Institute High Performance Computing.</p>"},{"location":"learning/profiling/#monitoring-jobs-on-the-node","title":"Monitoring jobs on the node","text":"<p>It is possible to submit a second (this time) interactive job to the node where the main job is running. Check with <code>squeue</code> where your job is running, then submit:</p> Bash<pre><code>srun -w &lt;nodename&gt; --pty htop\n</code></pre> <p>Note that there must be free slots on the machine, so you cannot use <code>-n 80</code> or <code>--exclusive</code> for your main job (use <code>-n 78</code>).</p> <p>Alternative method if you have X11, e.g., on Linux computers:</p> <p>When you log in to base, use <code>ssh -X -Y UniID@base.hpc.taltech.ee</code>,</p> <p>then submit your main job with <code>srun --x11 -n &lt;numtasks&gt; --cpus-per-task=&lt;numthreads&gt; --pty bash</code> and start an <code>xterm -e htop &amp;</code> in the session.</p> <p>In <code>sbatch</code>, the option <code>--x11=batch</code> can be used. Note that the ssh session to base needs to stay open!</p>"},{"location":"learning/profiling/#valgrind","title":"Valgrind","text":"<p>Manual: http://valgrind.org/docs/manual/manual.html</p> <p>Valgrind is an instrumentation framework for building dynamic analysis tools. There are Valgrind tools that can automatically detect many memory management and threading bugs and profile your programs in detail. You can also use Valgrind to build new tools.</p> <p>The Valgrind distribution currently includes seven production-quality tools: a memory error detector, two thread error detectors, a cache and branch-prediction profiler, a call-graph generating cache and branch-prediction profiler, and two different heap profilers. It also includes an experimental SimPoint basic block vector generator.</p>"},{"location":"learning/profiling/#callgrind","title":"Callgrind","text":""},{"location":"learning/profiling/#cachegrind","title":"Cachegrind","text":""},{"location":"learning/profiling/#profiling-python","title":"Profiling Python","text":"<p>Python is very slow. The best improvement is achieved by rewriting (parts of) the program in Fortran or C. See also \"Python Performance Matters\" by Emery Berger (Strange Loop 2022)</p>"},{"location":"learning/profiling/#python-scalene","title":"Python Scalene","text":"<p>Scalene is a CPU, GPU, and memory profiler for Python that is very performant (introduces very little overhead).</p> <p>Installation: load your favorite Python module, e.g.,</p> Bash<pre><code>module load green-spack\nmodule load python/3.8.7-gcc-10.3.0-plhb\npy-pip/21.1.2-gcc-10.3.0-python-3.8.7-bj7d\n</code></pre> <p>then install using pip:</p> Bash<pre><code>python -m pip install --user scalene\n</code></pre> <p>GitHub page and quickstart can be found here</p>"},{"location":"learning/profiling/#python-cprofile","title":"Python cProfile","text":""},{"location":"learning/profiling/#perf","title":"perf","text":"<p>perf home page</p> <p>perf is powerful: it can instrument CPU performance counters, tracepoints, kprobes, and uprobes (dynamic tracing). It is capable of lightweight profiling. It is also included in the Linux kernel, under tools/perf, and is frequently updated and enhanced.</p> <p>perf began as a tool for using the performance counters subsystem in Linux and has had various enhancements to add tracing capabilities.</p>"},{"location":"learning/profiling/#tau-jumpshot-paraprof","title":"TAU, Jumpshot, Paraprof","text":"<p>TAU can be used for profiling and for MPI tracing (not at the same time, though). See e.g., https://wiki.mpich.org/mpich/index.php/TAU_by_example</p> <p>Load the spack TAU module:</p> Bash<pre><code>module load green-spack\nmodule load tau/2.30.2-gcc-10.3.0-2wge\n</code></pre> <p>Profiling</p> <p>TAU supports different methods of instrumentation:</p> <ul> <li>Dynamic: statistical sampling of a binary through preloading of libraries</li> <li>Source: parser-aided automatic instrumentation at compile time</li> <li>Selective: a subcategory of source, it is automatic but guided source code instrumentation</li> </ul> <p>The simplest and only for existing binary software is dynamic profiling through <code>tau_exec</code>. Just run:</p> Bash<pre><code>srun tau_exec your_program\n</code></pre> <p>Several <code>profile.*</code> files will be created. This method can unfortunately only profile MPI functions and not user-defined ones. Note that profile files are only generated if the program exits normally, not if an error occurs or SLURM kills it!</p> <p>You can generate reports with <code>pprof</code> and visualize them with <code>paraprof</code>.</p> <p>MPI tracing</p> <p>The tracing can take a lot of space; it is not uncommon for trace files to be several GB in size for each MPI task!</p> Bash<pre><code>export TAU_TRACE=1\nsrun  -n 2 tau_exec ./pingpong-lg-mpi4\ntau_treemerge.pl\n</code></pre> <p>TAU does not have a tracing visualizer but provides tools to convert its traces to other formats, e.g., slog2 for jumpshot, otf(2), or paraver:</p> Bash<pre><code>tau2slog2 tau.trc tau.edf -o tau.slog2\ntau_convert -paraver tau.trc tau.edf trace.prv\n</code></pre> <p>The traces can be visualized using <code>jumpshot</code> (in the tau module). Just run:</p> Bash<pre><code>jumpshot tau.slog2\n</code></pre> <p>jumpshot may open a huge window (larger than screen size). In this case, use the \"maximize\" option of your window manager (fvwm: in the left window corner menu). jumpshot opens 3 windows: \"jumpshot-4\", \"Legend\", and \"Timeline\" (if you cannot find them, use the window manager menu, e.g., fvwm: right mouse button on desktop background).</p>"},{"location":"learning/profiling/#eztrace-vite","title":"EZTrace + ViTE","text":"<p>EZTrace 1.1 and ViTE 1.2 are installed on amp and viz.</p> <p>EZTrace is a tool to analyze event traces. It has several modules:</p> <ul> <li>stdio: Module for stdio functions (read, write, select, poll, etc.)</li> <li>starpu: Module for StarPU framework</li> <li>pthread: Module for PThread synchronization functions (mutex, semaphore, spinlock, etc.)</li> <li>papi: Module for PAPI Performance counters</li> <li>openmpi: Module for MPI functions</li> <li>memory: Module for memory functions (malloc, free, etc.)</li> </ul> <p>ViTE is the visualization tool to visualize the generated traces. It can also visualize .otf2 traces obtained from other MPI tracing tools (e.g., converted from TAU).</p>"},{"location":"learning/profiling/#hpctoolkit","title":"HPCToolkit","text":"<p>Load modules:</p> Bash<pre><code>module load hpctoolkit\n</code></pre> <p>Run application with binary instrumentation:</p> Bash<pre><code>srun -n 2 -p green-ib hpcrun &lt;your_application&gt;\nhpcstruct `which &lt;your_application&gt;`\nhpcprof hpctoolkit-&lt;your_application&gt;-measurements-&lt;PID&gt;\n</code></pre> <p>Run GUI tool for interpretation:</p> Bash<pre><code>hpcviewer hpctoolkit-&lt;your_application&gt;-database-&lt;PID&gt;\n</code></pre> <p>Starts <code>hpcviewer</code> and opens the database.</p>"},{"location":"learning/profiling/#paraver-trace-visualizer","title":"Paraver trace visualizer","text":"<p>Load the module:</p> Bash<pre><code>module load green\nmodule load Paraver\n</code></pre> <p>Start paraver:</p> Bash<pre><code>wxparaver\n</code></pre> <p>Then load the .prv trace file.</p>"},{"location":"learning/profiling/#extrae","title":"Extrae","text":""},{"location":"learning/profiling/#score-p","title":"Score-P","text":"<p>Scalable Performance Measurement Infrastructure for Parallel Codes</p> Bash<pre><code>module load green-spack\nmodule load scorep\n</code></pre> <p>The module with hash \"mlw5\" contains the PDT instrumenter. The module with hash \"o4v3\" contains the PDT instrumenter and libunwind.</p> <p>Score-P Quickstart</p> <p>Compilation: prefix the compiler command with \"scorep\", e.g., <code>scorep gcc ...</code> or <code>scorep mpicc ...</code>. This can also be used in Makefiles:</p> Makefile<pre><code>MPICC = $(PREP) mpicc\n</code></pre> <p>(and analogously for linkers and other compilers). One can then use the same makefile to either build an instrumented version with:</p> Bash<pre><code>make PREP=\"scorep\"\n</code></pre> <p>A simple <code>make</code> will generate an uninstrumented binary.</p> <p>The environment variables SCOREP_ENABLE_TRACING and SCOREP_ENABLE_PROFILING control whether event trace data or profiles are stored in this directory. By setting either variable to true, the corresponding data will be written to the directory. The default values are true for SCOREP_ENABLE_PROFILING and false for SCOREP_ENABLE_TRACING.</p>"},{"location":"learning/profiling/#scalasca","title":"Scalasca","text":"<p>Scalasca is a software tool that supports the performance optimization of parallel programs by measuring and analyzing their runtime behavior. The analysis identifies potential performance bottlenecks \u2013 in particular those concerning communication and synchronization \u2013 and offers guidance in exploring their causes.</p> Bash<pre><code>module load green-spack\nmodule load scalasca\n</code></pre>"},{"location":"learning/profiling/#openspeedshop","title":"Open|SpeedShop","text":"Bash<pre><code>module load green-spack\nmodule load openspeedshop\nmodule load openspeedshop-utils\n</code></pre>"},{"location":"learning/profiling/#discopop","title":"DiscoPoP","text":"<p>To be installed.</p> <p>Analyzes source code to find areas that can be parallelized.</p> <p>Works only with LLVM 11.1 (included in the module).</p> Bash<pre><code>module load green\nmodule load discopop\n</code></pre>"},{"location":"learning/profiling/#pgi-nvidia-hpc-sdk","title":"PGI / Nvidia HPC SDK","text":"<p>On the GPU servers, the Nvidia HPC SDK is installed, which contains the PGI compilers and profilers.</p>"},{"location":"learning/profiling/#igprof","title":"IgProf","text":"<p>The Ignominous Profiler. IgProf is a simple, nice tool for measuring and analyzing application memory and performance characteristics. IgProf requires no changes to the application or the build process.</p> <p>Quick start can be found here.</p>"},{"location":"software/cad-mesh/","title":"CAD &amp; Mesh-Tools","text":"<p>CAD and meshing software installed on viz, amp, and green cluster nodes.</p>"},{"location":"software/cad-mesh/#freecad","title":"FreeCAD","text":"<p>FreeCAD is an easy-to-use open-source CAD software that can use Gmsh or Netgen for meshing. It can also serve as a frontend for CalculiX and ElmerFEM, thus providing similar functionality as SolidWorks.</p> Bash<pre><code>module load rocky8\nmodule load freecad\nfreecad\n</code></pre> <p>(Please don't run simulations on viz) FreeCAD is best used within a VNC session.</p>"},{"location":"software/cad-mesh/#how-to-cite","title":"How to cite","text":"<p>FreeCAD is available from here.</p>"},{"location":"software/cad-mesh/#salome","title":"Salome","text":"<p>Salome is a multi-platform environment, allowing the realization of physics simulations. Salome is suitable for various stages of a study: from the creation of the CAD model and the mesh to the post-processing and visualization of the results. Other functionalities such as uncertainty treatment and data assimilation are also implemented. Salome does not contain a physics solver, but it provides the computing environment necessary for their integration.</p> Bash<pre><code>module load rocky8\nmodule load salome/9.13.0\n</code></pre> <p>Salome has a Python interface, so the meshing can be done as a batch job on the cluster nodes.</p> <p>The current state can be dumped into a script from the GUI by <code>CTRL+D</code>. The script can then be executed on the command line by:</p> Bash<pre><code>salome -t scriptname.py\n</code></pre> <p>It is therefore possible to prepare the geometry on your workstation or an OnDemand desktop session, dump the script, add the meshing command, and run the script on the cluster in batch mode.</p> <p>See the separate page on visualization.</p>"},{"location":"software/cad-mesh/#how-to-cite_1","title":"How to cite","text":"<p>Salome can be cited as DOI:10.13140/RG.2.2.12107.08485.</p>"},{"location":"software/cad-mesh/#brl-cad","title":"BRL-CAD","text":"<p>BRL-CAD is a CAD software that has been in development since 1979 and is open-source since 2004. It is based on CSG modeling. BRL-CAD does not provide volume meshing; however, the CSG geometry can be exported to BREP (boundary representation, like STL, OBJ, STEP, IGES, PLY). The <code>g-*</code> tools are for this, while the <code>*-g</code> tools are for importing. An introduction can be found in this PDF.</p>"},{"location":"software/cad-mesh/#gmsh","title":"Gmsh","text":"<p>Gmsh meshes can be used with ElmerFEM and OpenFOAM. For OpenFOAM, make sure it is saved as version 2 and ASCII format.</p> <p>The GUI can be used within an OnDemand desktop session. Large meshes can be done as batch jobs on cluster nodes (use srun or sbatch). On the command line, run:</p> Bash<pre><code>gmsh -3 -format msh2 -o outfilename.msh infilename.geo\n</code></pre> <p>This creates a volume mesh and saves it as version 2 format suitable for OpenFOAM.</p> <p>Use the SPACK module:</p> Bash<pre><code>module load rocky8-spack\nmodule load gmsh\n</code></pre>"},{"location":"software/cad-mesh/#how-to-cite_2","title":"How to cite","text":"<p>If you use Gmsh, please cite C. Geuzaine and J.-F. Remacle. Gmsh: a three-dimensional finite element mesh generator with built-in pre- and post-processing facilities. Int. J. Numer. Methods Eng., 79(11), pp. 1309-1331, 2009. You can also cite additional references for specific features and algorithms.</p>"},{"location":"software/cad-mesh/#netgen-ngsolve","title":"Netgen (NGsolve)","text":"<p>Netgen is a part of the NGsolve suite. Netgen is an automatic 3D tetrahedral mesh generator containing modules for mesh optimization and hierarchical mesh refinement. It accepts input from constructive solid geometry <code>.csg</code> or boundary representation (BRep) from <code>.stl</code> files, but also handles <code>.brep</code>, <code>.step</code>, and <code>.iges</code> formats. Those meshes generated can be exported in several formats (e.g., neutral, Abaqus, Fluent, ElmerFEM, Gmsh, and OpenFOAM). Netgen has a GUI (e.g., use an X2GO session on viz), but can also be used through its Python interface.</p> <p>A Python example using the OpenCASCADE kernel:</p> Python<pre><code>from netgen.NgOCC import *\ngeo = LoadOCCGeometry('screw.step')\ngeo.Heal()\nmesh = geo.GenerateMesh()\n# mesh.Save('screw.vol')\nmesh.Export(\"export.msh\",\"Gmsh Format\")\n\nfrom ngsolve import *\nDraw(Mesh(mesh))\n</code></pre> <p>A Python example using the Netgen STL:</p> Python<pre><code>import netgen.stl as stl\ngeo2 = stl.LoadSTLGeometry(\"input.stl\")\nm2 = geo2.GenerateMesh (maxh=0.05)\nm2.Export(\"export.msh\",\"Gmsh2 Format\")\n</code></pre> <p>You can get a list of export formats from the GUI.</p>"},{"location":"software/cad-mesh/#how-to-cite_3","title":"How to cite:","text":"<p>Netgen - J. Sch\u00f6berl. NETGEN An advancing front 2D/3D-mesh generator based on abstract rules. Comput. Vis. Sci., 1(1):41\u201352, 1997.</p> <p>Netgen/NGSolve is open source and available at www.ngsolve.org.</p>"},{"location":"software/cfd/","title":"Computational Fluid Dynamics","text":"<p>Warning</p> <p>This page has not been completely updated for Rocky 8 yet!</p> <p>The following software is available:</p> <ul> <li>OpenFOAM</li> <li>STAR-CCM+ (commercial)</li> <li>code_saturne (upcoming)</li> </ul> <p>The following software is under consideration:</p> <ul> <li>CONVERGE (commercial, free academic license, free training)</li> </ul>"},{"location":"software/cfd/#openfoam","title":"OpenFOAM","text":""},{"location":"software/cfd/#example-use-of-openfoam-on-the-base-cluster","title":"Example use of OpenFOAM on the BASE cluster","text":"<p>For the example, we will use one of the tutorial cases.</p> Bash<pre><code>module load mpi/openmpi-x86_64\nsource /share/apps/HPC2/OpenFOAM/OpenFOAM-v1912/etc/bashrc\n</code></pre> <p>First-time users need to create their <code>$WM_PROJECT_USER_DIR</code>:</p> Bash<pre><code>mkdir $WM_PROJECT_USER_DIR --parent\n</code></pre> <p>Copy the damBreak tutorial case into the <code>$WM_PROJECT_USER_DIR</code>:</p> Bash<pre><code>cp -r /share/apps/HPC2/OpenFOAM/OpenFOAM-v1912/tutorials/multiphase/interFoam/laminar/damBreak/damBreak $WM_PROJECT_USER_DIR/\ncd $WM_PROJECT_USER_DIR/damBreak\npwd\n</code></pre> <p>Now we can run the OpenFOAM case step-by-step or as a batch job.</p> <p>Note: Do not use the <code>Allrun</code> script(s) of the tutorials, as these may try to launch parallel jobs without requesting resources.</p>"},{"location":"software/cfd/#interactive-single-process","title":"Interactive single process","text":"<p>For a non-parallel run of the tutorial case, the <code>decomposeParDict</code> needs to be removed:</p> Bash<pre><code>mv system/decomposeParDict system/decomposeParDict-save\n</code></pre> <p>Running the damBreak case step-by-step interactively:</p> Bash<pre><code>srun --partition=common -t 2:10:00 --pty bash \nsource /share/apps/HPC2/OpenFOAM/OpenFOAM-v1912/etc/bashrc\nblockMesh\nsetFields\ninterFoam\n</code></pre>"},{"location":"software/cfd/#batch-job-non-interactive-parallel-job","title":"Batch-job (non-interactive) parallel job","text":"<p>Alternatively, we can run the job in parallel as a batch job: (If you removed/renamed the <code>decomposeParDict</code> before, copy it back: <code>cp system/decomposeParDict-save system/decomposeParDict</code>)</p> <p>The <code>openfoam.slurm</code> script:</p> Bash<pre><code>#!/bin/bash -l\n\n#SBATCH -n 4\n#SBATCH -t 00:10:00  \n#SBATCH -J openfoam-damBreak\n# #SBATCH --partition=green-ib\n\n# the following 2 lines are only needed if not done manually in command-line\nmodule load mpi/openmpi-x86_64\nsource /share/apps/HPC2/OpenFOAM/OpenFOAM-v1912/etc/bashrc\n\nblockMesh\ndecomposePar\nsetFields\nmpirun interFoam -parallel\nreconstructPar\n</code></pre> <p>And then run in the command-line (<code>module</code> and <code>source</code> are only needed if not in sbatch script):</p> Bash<pre><code>module load mpi/openmpi-x86_64\nsource /share/apps/HPC2/OpenFOAM/OpenFOAM-v1912/etc/bashrc\nsbatch openfoam.slurm\n</code></pre>"},{"location":"software/cfd/#pre-processing-geometry-and-mesh-generation","title":"Pre-processing (geometry and mesh generation)","text":"<p>The geometry and mesh can be either hand-coded using blockMesh or with Gmsh, FreeCAD, or Salome. When using Gmsh, be sure to save the mesh in v2 ASCII format (see separate page on CAD-mesh). This creates a volume mesh.</p> <p>To convert a Gmsh volume .msh file for OpenFOAM, use:</p> Bash<pre><code>gmshToFoam meshfile.msh\n</code></pre> <p>Another possibility is to use CAD for a surface mesh and use the snappyHexMesh utility to adapt a blockMesh volume mesh to the surface (see OpenFOAM motorcycle tutorial).</p>"},{"location":"software/cfd/#visualizing-the-results-post-processing","title":"Visualizing the results (post-processing)","text":"<p>Login to viz, change to the case directory, create an empty .foam file for the case:</p> Bash<pre><code>touch damBreak.foam\n</code></pre> <p>And then use the regular ParaView:</p> Bash<pre><code>paraview\n</code></pre> <p>And open the .foam file from the menu.</p>"},{"location":"software/cfd/#comparison-of-the-execution-time","title":"Comparison of the execution time","text":"<p>It is educational to check the runtime of the code using the <code>time</code> command, e.g. for the single-thread:</p> Bash<pre><code>time interFoam\n</code></pre> <p>And for the parallel run (in the <code>openfoam.slurm</code> script):</p> Bash<pre><code>time mpirun -n $SLURM_NTASKS interFoam -parallel\n</code></pre> <p>As the damBreak case is quite small, it is likely that the parallel run is not faster than the sequential, due to the communication overhead.</p> <p>In a test run, the results have been as follows:</p> time type sequential parallel real 0m8.319s 0m39.463s user 0m6.927s 1m1.755s sys 0m0.432s 0m2.922s <p>Lesson to be learned: Parallel computation is only useful for sufficiently large jobs.</p> <p>NOTE: Parallel does not (necessarily) mean faster!!! Parallel execution introduces overhead (starting threads, communication)! For optimal execution time and optimal use of resources, one needs to test and find the sweet spot.</p> <p> </p> <p>The division into the areas is a combined decision taking into account \"real\" (wall clock) and \"user\" (summed time of all threads) time (from the <code>time</code> command). \"Wall clock\" (real) time is the time one needs to wait till the job is finished, \"Summed thread time\" (user) is the sum of the times that all individual threads needed, it should be roughly user = numtreads x real. For parallel programs, one can expect that \"user\" time of the parallel run is larger than for the sequential, due to communication overhead, if it is smaller, that probably means the individual threads could make better use of cache.</p> area why explanation sweet spot minimal \"user\" time = minimal heat production, optimal use of resources good range linear speedup for \"real\", with constant or slightly increasing \"user\" OK range slightly less than linear speedup for \"real\", and slightly increasing \"user\" avoid ascending slope in the diagram for \"real\" and \"user\" one actually needs to wait longer compared to the case with fewer cores <p>Recommended in this case would be to request 8 threads <code>-n 8 --ntasks-per-node 8</code> but use <code>mpirun -n 4</code>. OpenFOAM does not seem to benefit from hyperthreading.</p>"},{"location":"software/chapel/","title":"Chapel language for HPC","text":"<p>Warning</p> <p>This page has not been completely updated for Rocky 8 yet!</p> <p>A special programming language developed for supercomputing, originally by the Cray Cascade project in DARPA's High Productivity Computing Systems (HPCS) program.</p>"},{"location":"software/chapel/#modules","title":"Modules","text":""},{"location":"software/chapel/#on-amp","title":"On AMP","text":"Bash<pre><code>module load amp\nmodule load llvm/14.0.0\nmodule load chapel/1.28\nmodule load amp-spack\nmodule load cuda/11.3.1-gcc-9.3.0-e4ej\n</code></pre>"},{"location":"software/chapel/#on-green","title":"On Green","text":"Bash<pre><code>module load green-spack/0.17.1\nmodule load gcc/10.3.0-gcc-10.3.0-qshu\nmodule load llvm/13.0.0-gcc-10.3.0-dhdd\nmodule load opempi/4.1.1-gcc-10.3.0\n</code></pre>"},{"location":"software/chapel/#environment-variables","title":"Environment Variables","text":""},{"location":"software/chapel/#on-amp_1","title":"On AMP","text":"Bash<pre><code>source /gpfs/mariana/software/amp/chapel/chapel-1.28.0/util/quickstart/setchplenv.bash\n</code></pre>"},{"location":"software/chapel/#on-green_1","title":"On Green","text":"Bash<pre><code>source /gpfs/mariana/software/green/chapel/chapel-1.28.0/util/quickstart/setchplenv.bash\n</code></pre> <p>Check which environments are available and compare them to current settings:</p> Bash<pre><code>$CHPL_HOME/util/chplenv/printchplbuilds.py\n</code></pre> <p>Check current variables:</p> Bash<pre><code>$CHPL_HOME/util/printchplenv\n</code></pre> <p>Set some environment variables:</p> Bash<pre><code>CHPL_TASKS=fifo\nCHPL_GMP=none\nCHPL_TARGET_CPU=native\nCHPL_COMM=gasnet\nCHPL_COMM_SUBSTRATE=ibv\nCHPL_TARGET_COMPILER=gnu\nCHPL_RE2=none\nCHPL_LLVM=none\nCHPL_LAUNCHER_PARTITION=green-ib\nCHPL_SANITIZE=\nCHPL_HOME=/gpfs/mariana/software/green/chapel/chapel-1.28.0\nCHPL_LAUNCHER=slurm-srun\nCHPL_LAUNCHER_NODE_ACCESS=unset\nCHPL_MEM=jemalloc\n</code></pre> Bash<pre><code>CHPL_COMM_SUBSTRATE=ibv        # several locales (nodes) with IB\nCHPL_COMM_SUBSTRATE=smp        # several locales within a single node\nCHPL_COMM_SUBSTRATE=mpi        # several locales (nodes) using MPI\nCHPL_COMM_SUBSTRATE=udp        # several locales (nodes) using \n</code></pre> Bash<pre><code>CHPL_LOCALE_MODEL=flat  # normal\nCHPL_LOCALE_MODEL=numa  # locale is subdivided into NUMA domains\nCHPL_LOCALE_MODEL=gpu   # for GPU\n</code></pre> Bash<pre><code>CHPL_LAUNCHER_PARTITION=green-ib\nCHPL_LAUNCHER_PARTITION=gpu\n</code></pre>"},{"location":"software/chapel/#example-chapel-programs","title":"Example Chapel Programs","text":"<p>Can be copied from <code>$CHPL_HOME/examples</code>.</p> <p>Set environment variables as needed.</p> <p>Compile:</p> Bash<pre><code>chpl -o jacobi-cpu-green jacobi-cpu.chpl\n</code></pre> <p>Run the Chapel program with the appropriate number of locales (typically a locale is a full node!):</p> Bash<pre><code>./jacobi-cpu-green -nl 1\n</code></pre> <p>The program can be started directly (without <code>srun</code> or <code>sbatch</code>), it uses the specified launcher set in the environment variable <code>CHPL_LAUNCHER=slurm-srun</code>.</p> <p>Fewer threads on a node can be specified using the <code>CHPL_RT_NUM_THREADS_PER_LOCALE</code> environment variable.</p>"},{"location":"software/cp2k/","title":"cp2k","text":"<p>Warning</p> <p>This page has not been completely updated for Rocky 8 yet!</p>"},{"location":"software/cp2k/#cp2k-short-introduction","title":"CP2K Short Introduction","text":"<ol> <li> <p>Create a cp2k.slurm batch script for parallel calculations:</p> Bash<pre><code>#!/bin/bash\n#SBATCH -p common\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=2         # CPU cores per MPI process\n#SBATCH --mem-per-cpu=1G          # host memory per CPU core\n#SBATCH --time=0-03:00            # time (DD-HH:MM)\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\nmodule load green/spack\nmodule load cp2k/8.2-gcc-10.3.0-7cv4\nsrun cp2k.psmp -i H2O-32.inp -o H2O-32.out\n</code></pre> </li> <li> <p>Copy the job input file H2O-32.inp.</p> </li> <li> <p>Submit the job on base:</p> Bash<pre><code>sbatch cp2k.slurm\n</code></pre> </li> </ol>"},{"location":"software/cp2k/#cp2k-long-version","title":"CP2K Long Version","text":""},{"location":"software/cp2k/#environment","title":"Environment","text":"<p>The environment is set up by the commands:</p> Bash<pre><code>module load green-spack\nmodule load cp2k\n</code></pre>"},{"location":"software/cp2k/#running-cp2k-jobs","title":"Running CP2K Jobs","text":"<p>CP2K is MPI and SMP parallelized; it requires the OpenMPI environment to be initialized, which means <code>mpirun</code> or <code>srun</code> needs to be used to start it.</p>"},{"location":"software/cp2k/#cp2k-with-gpus-on-amp","title":"CP2K with GPUs on amp","text":"<p>Version 7.1 has a bug; use only a single MPI task and a single GPU. For multiple MPI tasks and GPUs (1 GPU per task), use version 9.1!</p> <p>Log in to amp or amp2 using SSH (SSH keys need to be configured):</p> Bash<pre><code>ssh amp2\n</code></pre> <p>Initialize the environment:</p> Bash<pre><code>source /usr/share/lmod/6.6/init/bash\nmodule use /gpfs/mariana/modules/system\nmodule load amp-spack/0.19.0\nmodule load ucx/1.13.0-gcc-9.3.0-5yyu\nmodule load openmpi/4.1.1-gcc-9.3-amp\nmodule load cp2k/7.1-gcc-9.3.0-openblas-m7xt\n</code></pre> <p>Either run the job with <code>srun</code>:</p> Bash<pre><code>srun -p gpu-test --gres=gpu:2 -n 2 --cpus-per-task=1 --mem=16G cp2k.psmp -i H2O-32.inp -o log-H2O-32\n</code></pre> <p>Or better, use the cp2k-gpu.slurm script to submit the job:</p> Bash<pre><code>#!/bin/bash\n#SBATCH -p gpu-test\n#SBATCH --gres=gpu:3                  # total number of GPUs\n#SBATCH --ntasks=2\n##SBATCH --ntasks-per-gpu=1        # total of 2 MPI processes\n#SBATCH --cpus-per-task=2         # CPU cores per MPI process\n#SBATCH --mem-per-cpu=5G          # host memory per CPU core\n#SBATCH --time=0-03:00            # time (DD-HH:MM)\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\nsource /usr/share/lmod/6.6/init/bash\nmodule use /gpfs/mariana/modules/system\nmodule load amp-spack/0.19.0\nmodule load ucx/1.13.0-gcc-9.3.0-5yyu\nmodule load openmpi/4.1.1-gcc-9.3-amp\nmodule load cp2k/9.1-gcc-9.3.0-openblas-vgen\n\nsrun cp2k.psmp -i H2O-32.inp -o log-H2O-32.out\n</code></pre>"},{"location":"software/crest/","title":"CREST","text":""},{"location":"software/crest/#crest-short-introduction","title":"CREST Short Introduction","text":"<ol> <li> <p>Create the crest.slurm batch script for parallel calculations:</p> Bash<pre><code>#!/bin/bash\n#SBATCH --job-name=CREST-test\n#SBATCH --mem-per-cpu=2GB\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=24\n#SBATCH -t 1-00:00:00\n#SBATCH --partition=common\n\nmodule load rocky8/all\nmodule load xtb-crest\n\n# Run calculations \ncrest geometry.xyz --gfn2 --T 24 &gt; final.out\n</code></pre> </li> <li> <p>Copy the job input file geometry.xyz.</p> </li> <li> <p>Submit the job on base:</p> Bash<pre><code>sbatch crest.slurm\n</code></pre> </li> </ol> <p>NB! CREST can be run only on 1 node. If the job requires a large amount of memory, the bigmem or gpu partition with 1TB RAM can be used.</p> <p>NB! It is recommended to optimize the geometries obtained from CREST by more accurate methods. At the end of this page, home-made bash scripts are provided that can be helpful during this process.</p>"},{"location":"software/crest/#crest-long-version","title":"CREST Long Version","text":"<p>CREST (Conformer\u2013Rotamer Ensemble Sampling Tool) was designed as a conformer sampling program by Grimme's group. Conformational search can be done at various levels of theory, including molecular mechanics and semiempirical methods (GFNn-xTB) in gas or solvent (using several continuum models). By default, CREST uses root-mean-square-deviation (RMSD) based meta-dynamics, short regular MD simulations, and Genetic Z-matrix crossing (GC) algorithms for the generation of new conformers. CREST can also be used for searching protonation states, tautomerism studies, and non-covalent complex modeling. More information can be found in the original article.</p>"},{"location":"software/crest/#environment","title":"Environment","text":"<p>The environment is set up with the commands:</p> Bash<pre><code>module load rocky8/all\nmodule load xtb-crest\n</code></pre>"},{"location":"software/crest/#running-crest-jobs","title":"Running CREST Jobs","text":"<p>The CREST input file should be in <code>.xyz</code> format and is executed with the command <code>crest</code>. This command is usually placed in a <code>slurm</code> script.</p> Bash<pre><code>crest geometry.xyz --gfn2 --gbsa h2o --T 24 &gt; final.out\n</code></pre> <p>In CREST calculations, options are specified as command line arguments. <code>--T</code> is the number of processors used, <code>--gfn2</code> is the calculation method (here GFN2-xTB), and <code>--g h2o</code> is the GBSA implicit solvation model for water. More about command line arguments and some examples of CREST commands.</p>"},{"location":"software/crest/#time","title":"Time","text":"<p>Calculation time depends on the size of the molecule, its flexibility, the chosen energy window, and the methods used, and can only be determined empirically. For example, for a flexible organic molecule of 65 atoms, a conformational search using the GFN-FF method and 24 cores took about 15-20 minutes, while the semiempirical GFN2 method needed 5-8 hours. However, a lot depends on the energy window applied to the conformational search.</p>"},{"location":"software/crest/#memory","title":"Memory","text":"<p>Our experience shows that memory is the main limiting factor in conformational search calculations by CREST. Since memory consumption depends on many factors (size of the molecule, its flexibility, the chosen energy window, methods used), it can only be determined through trial and error. Perhaps the bigmem or gpu partition with 1TB RAM can be used. In our test runs for a flexible organic molecule of 54 atoms using the semiempirical GFN2 method, 1 GB per core was sufficient, but for a 65-atom molecule using the same level of theory, 2 GB per core were needed.</p>"},{"location":"software/crest/#how-to-cite","title":"How to Cite","text":"<p>The main publication for the CREST program - DOI: 10.1039/C9CP06869D.</p>"},{"location":"software/crest/#useful-bash-scripts","title":"Useful Bash Scripts","text":"<p>It is recommended to optimize the geometries obtained from CREST by more accurate methods. Here are home-made bash scripts that can be helpful.</p> <ul> <li> <p>Start-orca.sh &amp; start-gaussian.sh</p> <p>Start-orca.sh should be run from the directory where the CREST conformer search was done. It splits the CREST output into single geometries, prepares ORCA inputs, and launches calculations. NB! orca.slurm must be in the same folder as <code>start-orca.sh</code> and CREST calculations. NB! Charge, multiplicity, and number of conformers must be given as command line arguments <code>-c</code>, <code>-m</code>, and <code>-n</code>.</p> Bash<pre><code>sh start-orca.sh -c 0 -m 1 -n 500\n</code></pre> <p>By default, ORCA calculations will be done using the following method - RI-BP86-BJD3/def2-SVP. If it does not suit, the method can be changed in the <code>start-orca.sh</code> in the section \"ORCA method\".</p> <p>Start-gaussian.sh by analogy with <code>start-orca.sh</code> will create input for Gaussian and launch calculations.  </p> <p>By default, Gaussian calculations will be done using the following method - BP86-BJD3/def2-SVP SMD(chloroform, Surface=SAS, Radii=Bondi). If it does not suit, the method can be changed in the <code>start-gaussian.sh</code> in the section \"Gaussian method\".</p> <p>NB! If Surface=SAS &amp; Radii=Bondi are not used, just replace them with one space and remove <code>read</code> from <code>scrf</code> keywords. NB! gaussian.slurm must be in the same folder as <code>start-gaussian.sh</code> and CREST calculations. NB! Charge, multiplicity, and number of conformers must be given as command line arguments <code>-c</code>, <code>-m</code>, and <code>-n</code>.</p> </li> <li> <p>Check.sh verifies if all calculations ended normally.</p> </li> </ul> <p>NB! If Gaussian calculations were done, activate disabled rows starting with <code>#</code> and disable the above rows for the ORCA search by adding the <code>#</code> mark before them.</p> <ul> <li>Crest-sorting.sh available only for ORCA calculations.<ol> <li>Creates a CREST folder and moves the initial CREST calculations there.  </li> <li>Merges individual ORCA optimized geometries into a shared file <code>ALL.xyz</code>.  </li> <li>Creates a single CREST file, which will then be treated by CREST algorithms to delete duplicate structures and sort the remaining structures by energy.</li> </ol> </li> </ul>"},{"location":"software/define/","title":"Define","text":"<p>Warning</p> <p>This page has not been completely updated for Rocky 8 yet!</p> <ol> <li> <p>Define contains four main parts:</p> <ul> <li>Geometry menu: Read the geometry of the molecules, set up the coordinates of the system, find out the point group symmetry.</li> <li>Atomic attributes menu: Select the basis sets for the atoms.</li> <li>Initial guess menu: Determine the charge of the molecule and generate the initial guess for the molecular orbitals and their occupation.</li> <li>General menu: Select the computational method and set up advanced options such as excited state calculations.</li> </ul> </li> <li> <p>Some general instructions for define:</p> <ul> <li><code>*</code> (or <code>q</code>): Closes the current menu and writes the data into control.</li> <li><code>&amp;</code>: Returns to the previous menu.</li> <li><code>qq</code>: Quits Define immediately (panic button).</li> </ul> </li> </ol> <p>Usually Define offers a default choice for all questions. The default choice can be accepted simply by pressing <code>Enter</code>.</p> <p>NB! define is case-sensitive.</p>"},{"location":"software/define/#starting-define","title":"Starting define","text":"Text Only<pre><code>***********************************************************\n*                                                         *\n*                       D E F I N E                       *\n*                                                         *\n*         TURBOMOLE`S  INTERACTIVE  INPUT  PROGRAM        *\n*                                                         *\n*  Quantum Chemistry Group       University of Karlsruhe  *\n*                                                         *\n***********************************************************\n\n\nDATA WILL BE WRITTEN TO THE NEW FILE control\n\nIF YOU WANT TO READ DEFAULT-DATA FROM ANOTHER control-TYPE FILE,\nTHEN ENTER ITS LOCATION/NAME OR OTHERWISE HIT &gt;return&lt;.\n</code></pre> <p><code>Enter</code></p> Text Only<pre><code>INPUT TITLE OR \nENTER &amp; TO REPEAT DEFINITION OF DEFAULT INPUT FILE\n</code></pre> <p><code>Enter</code></p> Text Only<pre><code>SPECIFICATION OF MOLECULAR GEOMETRY ( #ATOMS=0     SYMMETRY=c1  )\nYOU MAY USE ONE OF THE FOLLOWING COMMANDS : \nsy &lt;group&gt; &lt;eps&gt; : DEFINE MOLECULAR SYMMETRY (default for eps=3d-1)\ndesy &lt;eps&gt;       : DETERMINE MOLECULAR SYMMETRY AND ADJUST \n                 COORDINATES (default for eps=1d-6)\nsusy             : ADJUST COORDINATES FOR SUBGROUPS\nai               : ADD ATOMIC COORDINATES INTERACTIVELY\na &lt;file&gt;         : ADD ATOMIC COORDINATES FROM FILE &lt;file&gt;\naa &lt;file&gt;        : ADD ATOMIC COORDINATES IN ANGSTROEM UNITS FROM FILE &lt;file&gt;\nsub              : SUBSTITUTE AN ATOM BY A GROUP OF ATOMS\ni                : INTERNAL COORDINATE MENU \nired             : REDUNDANT INTERNAL COORDINATES\nred_info         : DISPLAY REDUNDANT INTERNAL COORDINATES\nff               : UFF-FORCEFIELD CALCULATION\nm                : MANIPULATE GEOMETRY\nfrag             : Define Fragments for BSSE calculation\nw &lt;file&gt;         : WRITE MOLECULAR COORDINATES TO FILE &lt;file&gt; \nr &lt;file&gt;         : RELOAD ATOMIC AND INTERNAL COORDINATES FROM FILE &lt;file&gt;\nname             : CHANGE ATOMIC IDENTIFIERS \ndel              : DELETE ATOMS \ndis              : DISPLAY MOLECULAR GEOMETRY \nbanal            : CARRY OUT BOND ANALYSIS \n*                : TERMINATE MOLECULAR GEOMETRY SPECIFICATION \n                    AND WRITE GEOMETRY DATA TO CONTROL FILE\n\nIF YOU APPEND A QUESTION MARK TO ANY COMMAND AN EXPLANATION\nOF THAT COMMAND MAY BE GIVEN\n</code></pre> <p><code>a start-coord</code></p> Text Only<pre><code>CARTESIAN COORDINATES FOR  12 ATOMS HAVE SUCCESSFULLY \nBEEN ADDED. \n........\nSPECIFICATION OF MOLECULAR GEOMETRY ( #ATOMS=12    SYMMETRY=c1  )\n</code></pre> <p><code>ired</code></p> Text Only<pre><code>GEOIRED: NBDIM      30  NDEGR:      30 ......\n</code></pre> <p><code>*</code></p> Text Only<pre><code>ATOMIC ATTRIBUTE DEFINITION MENU  ( #atoms=12    #bas=12    #ecp=4     )\n\nb    : ASSIGN ATOMIC BASIS SETS \nbb   : b RESTRICTED TO BASIS SET LIBRARY \nbl   : LIST ATOMIC BASIS SETS ASSIGNED\nbm   : MODIFY DEFINITION OF ATOMIC BASIS SET\nbp   : SWITCH BETWEEN 5d/7f AND 6d/10f\nlib  : SELECT BASIS SET LIBRARY\necp  : ASSIGN EFFECTIVE CORE POTENTIALS \necpb : ecp RESTRICTED TO BASIS SET LIBRARY \necpi : GENERAL INFORMATION ABOUT EFFECTIVE CORE POTENTIALS\necpl : LIST EFFECTIVE CORE POTENTIALS ASSIGNED\necprm: REMOVE EFFECTIVE CORE POTENTIAL(S)\nc    : ASSIGN NUCLEAR CHARGES (IF DIFFERENT FROM DEFAULTS) \ncem  : ASSIGN NUCLEAR CHARGES FOR EMBEDDING \nm    : ASSIGN ATOMIC MASSES (IF DIFFERENT FROM DEFAULTS) \ndis  : DISPLAY MOLECULAR GEOMETRY \ndat  : DISPLAY ATOMIC ATTRIBUTES YET ESTABLISHED \nh    : EXPLANATION OF ATTRIBUTE DEFINITION SYNTAX \n*    : TERMINATE THIS SECTION AND WRITE DATA OR DATA REFERENCES TO control\nGOBACK=&amp; (TO GEOMETRY MENU !)\n</code></pre> <p><code>*</code></p> Text Only<pre><code>OCCUPATION NUMBER &amp; MOLECULAR ORBITAL DEFINITION MENU\n\nCHOOSE COMMAND \ninfsao     : OUTPUT SAO INFORMATION \natb        : Switch for writing MOs in ASCII or binary format\neht        : PROVIDE MOS &amp;&amp; OCCUPATION NUMBERS FROM EXTENDED HUECKEL GUESS \nuse &lt;file&gt; : SUPPLY MO INFORMATION USING DATA FROM &lt;file&gt; \nman        : MANUAL SPECIFICATION OF OCCUPATION NUMBERS \nhcore      : HAMILTON CORE GUESS FOR MOS\nflip       : FLIP SPIN OF A SELECTED ATOM\n&amp;          : MOVE BACK TO THE ATOMIC ATTRIBUTES MENU\nTHE COMMANDS  use  OR  eht  OR  *  OR q(uit) TERMINATE THIS MENU !!! \nFOR EXPLANATIONS APPEND A QUESTION MARK (?) TO ANY COMMAND\n</code></pre> <p><code>eht</code> <code>Enter</code> <code>0</code> <code>Enter</code> </p> Text Only<pre><code>GENERAL MENU : SELECT YOUR TOPIC \nscf    : SELECT NON-DEFAULT SCF PARAMETER \nmp2    : OPTIONS AND DATA GROUPS FOR rimp2 and mpgrad\ncc     : OPTIONS AND DATA GROUPS FOR ricc2\npnocc  : OPTIONS AND DATA GROUPS FOR pnoccsd\nex     : EXCITED STATE AND RESPONSE OPTIONS\nprop   : SELECT TOOLS FOR SCF-ORBITAL ANALYSIS \ndrv    : SELECT NON-DEFAULT INPUT PARAMETER FOR EVALUATION\n      OF ANALYTICAL ENERGY DERIVATIVES \n      (GRADIENTS, FORCE CONSTANTS) \nrex    : SELECT OPTIONS FOR GEOMETRY UPDATES USING RELAX\nstp    : SELECT NON-DEFAULT STRUCTURE OPTIMIZATION PARAMETER\ne      : DEFINE EXTERNAL ELECTROSTATIC FIELD \ndft    : DFT Parameters \nri     : RI Parameters \nrijk   : RI-JK-HF Parameters \nrirpa  : RIRPA Parameters \nsenex  : seminumeric exchange parameters \nhybno  : hybrid Noga/Diag parameters\ndsp    : DFT dispersion correction\ntrunc  : USE TRUNCATED AUXBASIS DURING ITERATIONS \nmarij  : MULTIPOLE ACCELERATED RI-J \ndis    : DISPLAY MOLECULAR GEOMETRY \nlist   : LIST OF CONTROL FILE \n&amp;      : GO BACK TO OCCUPATION/ORBITAL ASSIGNMENT MENU\n\n* or q : END OF DEFINE SESSION\n</code></pre> <p><code>*</code></p>"},{"location":"software/define/#examples-of-define-files","title":"Examples of define files","text":""},{"location":"software/define/#dft-calculation-pb86-d3bjdef2-svp","title":"DFT calculation (PB86-D3BJ/def2-SV(P))","text":"<p>In this example, the BP86 functional (dft/on) and def2-SV(P) basis set (b/all def2-SV(P)) will be used for the calculation. This is the default level of theory for DFT calculations in TURBOMOLE. BP86 functional has a good and stable performance throughout the periodic system, and by default def2 basis sets include ECPs for atoms beyond Kr. Additionally, will be used Grimme's dispersion correction D3BJ (dsp/on/bj). The geometry will be read from the file start-coord. Will be used the redundant internal coordinates (ired) since they typically result in the smallest number of optimization steps. To speed up calculations resolution-of-the-identity (RI) and multipole-accelerated RI-J (MARIJ) approximations will be used (ri/on and marij/on). The molecule's charge is -1. An initial guess of the molecular orbitals will be done by eht* and up to 100 iterations will be done during scf cycle (scf/iter/100**).</p> <p>start-coord</p> Text Only<pre><code>$coord\n    -1.95916500780981     -0.42159243893826      0.00000000000000      ir\n    -1.95916500780981      4.47279824523555      0.00000000000000       i\n    -6.85355569198362     -0.42159243893826      0.00000000000000       i\n    -1.95916500780981     -0.42159243893826     -4.89439068417382       i\n    -1.95916500780981     -0.42159243893826      3.83614404975786       c\n    -2.45256841929780     -2.26300137375688      4.51014577207379       h\n    -0.11775648873094      0.07181261661147      4.51014577207379       h\n    -3.30717015319520      0.92641151591967      4.51014673583412       h\n     1.87697904194805     -0.42159243893826      0.00000000000000       c\n    -1.95916500780981     -4.25773648869612      0.00000000000000       c\n     4.25501040757134     -0.42159243893826      0.00000000000000       o\n    -1.95916500780981     -6.63576785431941      0.00000000000000       o\n$end\n</code></pre> <p>define</p> Text Only<pre><code>define &lt;&lt;EOF\n\n\na start-coord  \nired  \n*\nb\nall def2-SV(P)\n*\neht  \nyes  \n-1   \nyes  \nscf  \niter  \n100\n\nri  \non\n\nmarij  \non\n\ndft  \non\n\ndsp  \non  \nbj\n\n*\nEOF\n</code></pre> <p>comands to run</p> Text Only<pre><code>ridft &gt; JOB.out 2&gt;JOB.err    # single point calculation using RI-approximation\njobex -ri -c 45 &gt; FINAL.out 2&gt;FINAL.err # geometry optimization using RI-approximation,\n                                          will be don up to 45 steps\n</code></pre>"},{"location":"software/define/#dft-calculation-mo6-hybrid-functional-and-different-basis-sets-including-ecp-with-frozen-position-of-several-atoms","title":"DFT calculation (MO6 (hybrid functional) and different basis sets including ECP) with frozen position of several atoms","text":"<p>In this example, the M06 functional (dft/on/func/m06) and two different basis sets (_b/1 6-31G/ecp/\"i\" DZP_*) will be used for calculations. For the first atom 6-31G will bu applied and for all I atoms - def-SV(P) with ECP. The geometry will be read from the file start-coord and Cartesian coordinates will be used for further calculations. In addition, the position of the two first atoms will be frozen, that can be done only in Cartesian coordinates. The molecule's charge is 0. An initial guess of the molecular orbitals will be done by eht** and up to 30 default iterations will be done during scf cycle.</p> <p>start-coord</p> Text Only<pre><code>$coord\n    0.74398670919525      0.42159243893826      0.00000000000000       n    f\n    2.02272352743997      2.22995142429552      3.13219908774303       i    f\n    2.02265750040888     -3.19517438119679      0.00000000000000       i\n    2.02272352743997      2.22995142429552     -3.13219909530193       i\n$end\n</code></pre> <p>define</p> Text Only<pre><code>define &lt;&lt;EOF\n\n\na start-coord  \n*\nno\nb\n1 6-31G*\necp  \n\"i\" DZP\n\n*\neht  \nyes  \n0   \nyes\n\ndft  \non  \nfunc  \nm06\n\n*\nEOF\n</code></pre> <p>NB!  In coord file should appear the corresponding \"f\" marks.</p> Text Only<pre><code>$coord\n    0.74398670919525      0.42159243893826      0.00000000000000       n    f\n    2.02272352743997      2.22995142429552      3.13219908774303       i    f\n</code></pre> <p>comands to run</p> Text Only<pre><code>dft &gt; JOB.out 2&gt;JOB.err      # single point calculation \njobex -c 45 &gt; FINAL.out 2&gt;FINAL.err # geometry optimization, will be done up to 45 steps\n</code></pre>"},{"location":"software/define/#hf-optimization-with-frozen-internal-coordinated","title":"HF &amp; optimization with frozen internal coordinated","text":"<p>In this example, HF and minix basis set will be used for calculations. Some internal coordinates will be frozen (i/idef/f tors 1 2 3 4/f bend 1 2 3/f stre 1 2) during geometry optimization. To speed up calculations RI-approximations will be used.</p> <p>coord file</p> Text Only<pre><code>$coord\n     1.27839972889714      0.80710203135546      0.00041573974923       c\n     1.42630859331810      2.88253155131977      0.00372276048178       h\n     3.06528696563114     -0.57632867600746     -0.00069919866917       o\n    -1.91446264796512     -0.31879679861781      0.00039684248791       o\n    -2.98773260513752      1.98632893279876     -0.00701088395301       h\n$end\n</code></pre> <p>define</p> Text Only<pre><code>define &lt;&lt;EOF\n\n\na start-coord  \ni\nidef\nf tors 1 2 3 4\nf bend 1 2 3 \nf stre 1 2\n\n\n\nired  \n*\nbb all minix\n\n*\neht  \nyes  \n0   \nyes\n\nri  \non\n\n*\nEOF\n</code></pre> <p>NB!  In coord file should appear a corresponding part with list of defined internal coordinates:</p> Text Only<pre><code>$intdef\n# definitions of internal coordinates\n1 f  1.0000000000000 tors    1    2    3    4 val=  -0.04664\n2 f  1.0000000000000 bend    1    2    3      val=  26.89863\n3 f  1.0000000000000 stre    1    2           val=   2.08070\n</code></pre> <p>comands to run</p> Text Only<pre><code>dscf &gt; JOB.out 2&gt;JOB.err              # single point calculation  \njobex -ri &gt; FINAL.out 2&gt;FINAL.err     # geometry optimization \n(RI-approximation will be used if it is specified in control file)\n</code></pre>"},{"location":"software/define/#ri-mp2-calculation","title":"RI-MP2 calculation","text":"<p>In this example, calculations will be performed at the MP2/def2-TZVP level of theory (b/all def2-TZVP and cc/ricc2/mp2/geoopt model=mp2), inner core electrons will be freezed and conergence criteria  increaced (_mp2/freeze//cbas/b/all def2-TZVP//denconv/0.1E-07_*). The symmetry of the molecule is determined and will be utilized (desy). To speed up calculations RI-approximations will be used (ricc2). The molecule's charge is 0. An initial guess of the molecular orbitals will be done by eht* and up to 70 iterations will be done during scf cycle.</p> <p>coord file</p> Text Only<pre><code>$coord\n         0.00000000000000     -0.00000000000000      0.00000000000000  c\n        -1.18649579051912      1.18649579051912      1.18649579051912  h\n         1.18649579051912     -1.18649579051912      1.18649579051912  h\n        -1.18649579051912     -1.18649579051912     -1.18649579051912  h\n         1.18649579051912      1.18649579051912     -1.18649579051912  h\n$end\n</code></pre> <p>define</p> Text Only<pre><code>define &lt;&lt;EOF\n\n\na start-coord \ndesy\nired  \n*\nb\nall def2-TZVP\n*\neht  \nyes  \n0   \nyes\n\nscf\niter\n70\n\nmp2 \nfreeze\n*\ncbas\nb\nall def2-TZVP\n*   \ndenconv\n0.1E-07\n*\ncc\nricc2\nmp2 \ngeoopt model=mp2\n*\n*\n*\nEOF\n</code></pre> <p>comands to run</p> Text Only<pre><code>jobex -ri  -level mp2 &gt; FINAL.out 2&gt;FINAL.err\n</code></pre>"},{"location":"software/define/#cc2-calculation","title":"CC2 calculation","text":"<p>In this example, calculations will be performed at the cc22/def2-TZVP level of theory (b/all def2-TZVP and cc/ricc2/cc2/geoopt model=cc2), inner core electrons will be freezed and conergence criteria  increaced (_mp2/freeze//cbas/b/all def2-TZVP//denconv/0.1E-07_*). The symmetry of the molecule is determined and will be utilized (desy). To speed up calculations RI-approximations will be used (ricc2). The molecule's charge is 0. An initial guess of the molecular orbitals will be done by eht* and up to 70 iterations will be done during scf cycle.</p> <p>coord file</p> Text Only<pre><code>$coord\n         0.00000000000000     -0.00000000000000      0.00000000000000  c\n        -1.18649579051912      1.18649579051912      1.18649579051912  h\n         1.18649579051912     -1.18649579051912      1.18649579051912  h\n        -1.18649579051912     -1.18649579051912     -1.18649579051912  h\n         1.18649579051912      1.18649579051912     -1.18649579051912  h\n$end\n</code></pre> <p>define</p> Text Only<pre><code>define &lt;&lt;EOF\n\n\na start-coord \ndesy\nired  \n*\nb\nall def2-TZVP\n*\neht  \nyes  \n0   \nyes\n\nscf\niter\n70\n\ncc\nfreeze\n*\ncbas\nb\nall def2-TZVP\n*   \ndenconv\n0.1E-07\nricc2\ncc2 \ngeoopt model=cc2 \n*\n*\n*\nEOF\n</code></pre> <p>comands to run</p> Text Only<pre><code>jobex -ri  -level cc2 &gt; FINAL.out 2&gt;FINAL.err\n</code></pre>"},{"location":"software/easybuild/","title":"EasyBuild","text":"<p>Warning</p> <p>This page has not been completely updated for Rocky 8 yet! This page is a work in progress!</p> <p>EasyBuild is a package manager used to install software packages. An advantage is the ability to relatively easily install consistent dependencies and multiple versions of software. List of packages</p>"},{"location":"software/easybuild/#modules","title":"Modules","text":"<p>EasyBuild is available on amp from the AI lab.</p> Bash<pre><code>module use /illukas/software/modules\n</code></pre>"},{"location":"software/easybuild/#user-built-software","title":"User-built software","text":"<p>EasyBuild can also be used by users to manage their own software stack inside their home directory (be aware, this takes a lot of space!).</p> <p>See EasyBuild documentation here.</p> <p>A similar tool is SPACK. They support different lists of software packages. SPACK includes GPU-offloading compilers for both Nvidia and AMD, profiling tools (Tau, HPCToolkit), and engineering simulation packages (ElmerFEM, OpenFOAM), while EasyBuild seems to be more AI and Python-oriented.</p> <p>EasyBuild will be used on LUMI, while SPACK is used by the University of Tartu, LRZ, and HLRS.</p>"},{"location":"software/elmerfem/","title":"ElmerFEM","text":"<p>Warning</p> <p>This page is a work in progress!</p> <p>Elmer is a multi-physics simulation software developed by CSC. It can perform coupled mechanical, thermal, fluid, and electro-magnetic simulations and can be extended by custom equations.</p> <p>Some useful links:</p> <ul> <li>Elmer homepage: http://www.elmerfem.org/blog/</li> <li>Elmer manuals and tutorials: https://www.nic.funet.fi/pub/sci/physics/elmer/doc/</li> </ul>"},{"location":"software/elmerfem/#how-to-cite","title":"How to cite","text":"<p>CSC \u2013 IT CENTER FOR SCIENCE LTD., 2020. Elmer, Available at: https://www.csc.fi/web/elmer/.</p>"},{"location":"software/elmerfem/#loading-the-module","title":"Loading the module","text":"<p>To use ElmerFEM, the module needs to be loaded:</p> Bash<pre><code>module load rocky8-spack    \nmodule load elmerfem/9.0-gcc-10.3.0-netlib-lapack-qjdi\n</code></pre> <p>This makes the following main commands <code>ElmerGrid</code> and <code>ElmerSolver</code> available. <code>ElmerGUI</code> can be used with X11 forwarding or in an OnDemand desktop session to set up the case file. The use of ElmerGUI to run simulations is not recommended.</p>"},{"location":"software/elmerfem/#running-a-tutorial-case-quick-start-for-the-impatient","title":"Running a tutorial case (quick-start for the impatient)","text":"<ol> <li> <p>Copy the tutorial directory (here the linear elastic beam) and go into it:</p> Bash<pre><code>cp -r /share/apps/HPC2/ElmerFEM/tutorials-CL-files/ElasticEigenValues/ linear-elastic-beam-tutorial\ncd linear-elastic-beam-tutorial\n</code></pre> </li> <li> <p>Start an interactive session on a node:</p> Bash<pre><code>srun -t 2:00:00 --pty bash\n</code></pre> </li> <li> <p>Create the mesh:</p> Bash<pre><code>ElmerGrid 7 2 mesh.FDNEUT\n</code></pre> </li> <li> <p>Run the solver:</p> Bash<pre><code>ElmerSolver\n</code></pre> </li> <li> <p>Postprocessing involves visualizing the <code>eigen_values.vtu</code> file in ParaView.</p> </li> </ol>"},{"location":"software/elmerfem/#setting-up-a-simulation-for-new-users","title":"Setting up a simulation (for new users)","text":"<p>The following steps are needed to configure a simulation case (mostly on base).</p> <ol> <li> <p>Create geometry in Gmsh, group, and name physical volumes and surfaces (can be done on viz).</p> </li> <li> <p>Create the mesh in Gmsh (large meshes can be created from the CLI in a batch job):</p> Bash<pre><code>gmsh -3 geometry.geo\n</code></pre> </li> <li> <p>Convert the mesh to Elmer's format using ElmerGrid, including scaling if needed:</p> Bash<pre><code>ElmerGrid 14 2 geometry.msh -scale 0.001 0.001 0.001\n</code></pre> </li> <li> <p>Create a new project in ElmerGUI (can be done on viz):</p> </li> <li>Create project  </li> <li>Load Elmer mesh (point to the created mesh directory)</li> <li>Add equation(s)  </li> <li>Add material(s)  </li> <li>Add boundary conditions  </li> <li>Create sif</li> <li> <p>Edit &amp; save sif</p> </li> <li> <p>Edit the <code>case.sif</code> file (mesh directory, some other parameters [e.g., calculate PrincipalStresses] can only be added in the sif file, not in the GUI).</p> </li> <li> <p>Run the simulation:</p> Bash<pre><code>srun ElmerSolver \n</code></pre> <p>or create a batch file and submit using sbatch.</p> </li> <li> <p>Postprocess in ParaView.</p> </li> </ol>"},{"location":"software/fea/","title":"Finite Element Analysis","text":"<p>Warning</p> <p>This page has not been completely updated for Rocky 8 yet!</p> <p>The following software is installed on the cluster:</p> <ul> <li>ElmerFEM: Multiphysics ElmerModelsManual.pdf</li> <li>CalculiX: Mechanical analysis, heat, electromagnetic, CFD; solver makes use of the Abaqus input format. Overview of the finite element capabilities of CalculiX Version 2.18</li> <li>deal.ii (via SPACK): A C++ software library supporting the creation of finite element codes and an open community of users and developers.</li> <li>Abaqus: Commercial</li> <li>Comsol: Commercial, License belongs to a research group (...)</li> <li>code_aster (upcoming from generic package)</li> <li>ngsolve (upcoming from source)</li> </ul> <p>The following software is under consideration:</p> <ul> <li>FEniCS (under consideration) SPACK</li> <li>Dune (under consideration) source</li> <li>FEATool Multiphysics (under consideration) source</li> <li>FreeFEM (under consideration) SPACK</li> <li>MFEM (under consideration) SPACK</li> <li>MoFEM (under consideration) SPACK problematic</li> </ul>"},{"location":"software/fea/#elmerfem","title":"ElmerFEM","text":"<p>Elmer is a multi-physics simulation software developed by CSC. It can perform coupled mechanical, thermal, fluid, and electromagnetic simulations and can be extended by custom equations.</p> <ul> <li>Elmer homepage</li> <li>Elmer manuals and tutorials</li> </ul>"},{"location":"software/fea/#loading-the-module","title":"Loading the module","text":"<p>To use ElmerFEM, the module needs to be loaded:</p> Bash<pre><code>module load elmerfem-9.0\n</code></pre> <p>This makes the following main commands <code>ElmerGrid</code>, <code>ElmerSolver</code> available (and <code>ElmerGUI</code> can be used on viz to set up the case file). The use of ElmerGUI for simulations is not recommended.</p>"},{"location":"software/fea/#running-a-tutorial-case-quick-start-for-the-impatient","title":"Running a tutorial case (quick-start for the impatient)","text":"<p>Copy the tutorial directory (here the linear elastic beam):</p> Bash<pre><code>cp -r /share/apps/HPC2/ElmerFEM/tutorials-CL-files/ElasticEigenValues/ linear-elastic-beam-tutorial\ncd linear-elastic-beam-tutorial\n</code></pre> <p>Start an interactive session on a node:</p> Bash<pre><code>srun -t 2:00:00 --pty bash\n</code></pre> <p>Create the mesh:</p> Bash<pre><code>ElmerGrid d 7 2 mesh.FDNEUT\n</code></pre> <p>Run the solver:</p> Bash<pre><code>ElmerSolver\n</code></pre> <p>Postprocessing would be visualizing the <code>eigen_values.vtu</code> file in <code>paraview</code> on viz.</p>"},{"location":"software/fea/#setting-up-a-simulation-for-new-users","title":"Setting up a simulation (for new users)","text":"<p>The following steps are needed to configure a simulation case (mostly on base):</p> <ol> <li>Create geometry in Gmsh, group and name physical volumes and surfaces (can be done on viz)</li> <li>Create mesh in Gmsh (large meshes can be created from the CLI in a batch job: <code>gmsh -3 geometry.geo</code>)</li> <li>Convert the mesh to Elmer's format using ElmerGrid, including scaling if needed: <code>ElmerGrid 14 2 geometry.msh -scale 0.001 0.001 0.001</code></li> <li>Create a new project in ElmerGUI (can be done on viz)<ul> <li>Create project</li> <li>Load Elmer mesh (point to the created mesh directory)</li> <li>Add equation(s)</li> <li>Add material(s)</li> <li>Add boundary conditions</li> <li>Create sif</li> <li>Edit &amp; save sif</li> </ul> </li> <li>Edit the <code>case.sif</code> file (mesh directory, some other parameters [e.g. calculate PrincipalStresses] can only be added in the sif file, not in the GUI)</li> <li>Run simulation <code>srun ElmerSolver</code> (or create batch file and submit using sbatch)</li> <li>Postprocessing in ParaView (on viz)</li> </ol>"},{"location":"software/fea/#an-example-simulation-case-from-start-to-finish","title":"An example simulation case from start to finish","text":""},{"location":"software/fea/#calculix","title":"CalculiX","text":"<p>The two programs that form CalculiX are <code>cgx</code> and <code>ccx</code>, where <code>cgx</code> is a graphical frontend (pre- and post-processing) and <code>ccx</code> is the solver doing the actual numerics.</p> <p>As mentioned above, CalculiX uses the Abaqus format.</p>"},{"location":"software/fea/#abaqus","title":"Abaqus","text":""},{"location":"software/fea/#comsol","title":"Comsol","text":"<p>The license belongs to the research group of ... . In case you need access, please agree with them on a cooperation.</p>"},{"location":"software/fea/#dealii","title":"deal.ii","text":"<p>Available through a SPACK module.</p> Bash<pre><code>module use /share/apps/HPC2/SPACK/spack/share/spack/modules/linux-centos7-skylake_avx512/\n</code></pre> <p>deal.ii is a C++ library that can be used to write custom FEA software.</p>"},{"location":"software/gaussian/","title":"Gaussian","text":"<p>Info</p> <p>To run Gaussian, the user should be added to the Gaussian group. Contact hpcsupport@taltech.ee to be added.</p>"},{"location":"software/gaussian/#gaussian-short-introduction","title":"Gaussian short introduction","text":"<ol> <li> <p>Make gaussian.slurm batch script:</p> Bash<pre><code>#!/bin/bash\n#SBATCH --job-name=Job_Name\n#SBATCH --mem-per-cpu=2GB\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=24\n#SBATCH -t 1-00:00:00\n#SBATCH --partition=common\n#SBATCH --no-requeue\n\nmodule load rocky8/all \nmodule load gaussian/16.c02 \n\nSCRATCH=/state/partition1/$SLURM_JOB_ID\nexport GAUSS_SCRDIR=$SCRATCH\nmkdir -p $SCRATCH\n\ng16 -m=48gb -p=24 &lt; job.com &gt; job.log\n\n# Clean after yourself\nrm -rf $SCRATCH\n</code></pre> </li> <li> <p>Copy the job-input file job.com.</p> </li> <li> <p>Submit the job on base:</p> Bash<pre><code>sbatch gaussian.slurm\n</code></pre> <p>NB! More cores do not mean faster!!! See benchmarks.</p> </li> <li> <p>Check results using visualization software.</p> </li> </ol>"},{"location":"software/gaussian/#gaussian-long-version","title":"Gaussian long version","text":"<p>Gaussian is a general-purpose package for the calculation of electronic structures. It can calculate properties of molecules (structures, energies, spectroscopic and thermochemical properties, atomic charges, electron affinities, electrostatic potentials, electron densities, etc.) as well as reaction properties (such as reaction pathways, IRC) using different methods (such as Molecular mechanics, Semi-empirical methods, Hartree-Fock, Density functional theory, M\u00f8ller-Plesset perturbation theory, coupled cluster). More about Gaussian features can be found here.</p>"},{"location":"software/gaussian/#environment","title":"Environment","text":"<p>There are several versions of Gaussian available at HPC: g09 (revision C.01) and g16 (revisions B.01, C.01, and C.02). The environment and Gaussian version are set up by the commands:</p> Bash<pre><code>module load rocky8/all\nmodule load gaussian/16.c02\n</code></pre>"},{"location":"software/gaussian/#running-gaussian-jobs","title":"Running Gaussian jobs","text":"<p>Gaussian input files are executed by the commands <code>g09</code> or <code>g16</code> depending on the version of Gaussian used. This command is usually placed in the <code>slurm</code> script.</p> Bash<pre><code>g16 &lt; job.com &gt; job.log\n</code></pre>"},{"location":"software/gaussian/#single-core-calculations","title":"Single core calculations","text":"<p>Gaussian by default executes jobs on only a single processor.</p> <p>NB! If more processors are defined in the <code>slurm</code> script, they will be reserved but not used.</p>"},{"location":"software/gaussian/#parallel-jobs","title":"Parallel jobs","text":"<p>To run multiple processors/cores job, a number of cores should be specified. The number of cores can be defined via the <code>-p</code> flag (e.g. -p=4) in the command line of the <code>slurm</code> script or by adding the <code>%NprocShared</code> keyword into the Gaussian input file (e.g. %NprocShared=4). For more information see the Gaussian manual. The number of processors requested should correspond to the number of processors requested in the <code>slurm</code> script.</p> <p>NB! More cores do not mean faster!!! See benchmarks.</p> <p>Example of <code>slurm</code> script:</p> Bash<pre><code>#!/bin/bash\n\n#SBATCH --job-name=Job_Name    # Job name\n#SBATCH --mem=8GB              # Memory\n#SBATCH --nodes=1              # Number of nodes \n#SBATCH --ntasks=1             # Number of threads \n#SBATCH --cpus-per-task=4\n#SBATCH -t 1-00:00:00          # Time\n#SBATCH --partition=common     # Partition\n#SBATCH --no-requeue           # Job will not be restarted by default \n\nmodule load rocky8/all\nmodule load gaussian/16.c02\n\nSCRATCH=/state/partition1/$SLURM_JOB_ID\nexport GAUSS_SCRDIR=$SCRATCH\nmkdir -p $SCRATCH   \n\ng16 &lt; job.com &gt; job.log\n\nrm -rf $SCRATCH\n</code></pre> <p>Example of Gaussian input:</p> Text Only<pre><code>%Mem=8GB\n%NprocShared=4\n%chk=job.chk\n#P B3LYP/6-311++G** Opt EmpiricalDispersion=GD3BJ \n\nJob_Name\n\n0,1\nC          0.67650        0.42710        0.00022\nH          0.75477        1.52537        0.00197\nO          1.62208       -0.30498       -0.00037\nS         -1.01309       -0.16870        0.00021\nH         -1.58104        1.05112       -0.00371\n</code></pre>"},{"location":"software/gaussian/#memory","title":"Memory","text":"<p>The default dynamic memory requested by Gaussian is frequently too small for successful job termination. Herein, if the amount of memory requested is insufficient, the job will crash. There is no golden rule for memory requests. Usually, for common calculations (e.g. optimization, frequency, etc.) 2 GB per 1 core is sufficient. This can be done by the <code>-m</code> flag in the command line (e.g. -m=48gb) or by adding the <code>%Mem</code> keyword in the Gaussian input file (e.g. %Mem=2GB). For more information see the Gaussian manual and taltech user-guides.</p> <p>However, there are calculations that require more memory (e.g. TD-DFT, large SCF calculations, etc.). Data from a <code>slurm-JOBID.stat</code> file can be useful to determine the amount of memory required for a computation. In the <code>slurm-JOBID.stat</code> file, the efficiency of memory utilization is shown.</p> <p>Bad example:</p> Text Only<pre><code>Memory Utilized: 3.08 GB \nMemory Efficiency: 11.83% of 26.00 GB\n</code></pre> <p>Good example:</p> Text Only<pre><code>Memory Utilized: 63.12 GB \nMemory Efficiency: 98.62% of 64.00 GB\n</code></pre>"},{"location":"software/gaussian/#time","title":"Time","text":"<p>Time limits depend on the time partition used. If the calculation time exceeds the time limit requested in the <code>slurm</code> script, the job will be killed, and at the end of <code>slurm-JOBID.out</code> will be written \"error: JOB 317255 ON green23 CANCELLED AT 2023-08-11T22:28:01 DUE TO TIME LIMIT\"</p> <p>Therefore, it is recommended to request more time than is usually needed for the calculation and create checkpoint files (by <code>%chk=job.chk</code> line in the input file) that allow restarting the job.</p>"},{"location":"software/gaussian/#using-gpus","title":"Using GPUs","text":"<p>GPUs are effective for large molecules, their energies, gradients, and frequencies calculations. GPUs are not effective for small jobs, as well as for MP2 or CCSD calculations.</p> <p>GPU jobs can be run only on amp or amp2. To access amp the user has to have ssh-keys copied to the base (how to do that).</p> <p>amp can be accessed by the command:</p> Bash<pre><code>ssh -J uni-ID@base.hpc.taltech.ee uni-ID@amp\n</code></pre> <p>Each GPU must be controlled by a specific CPU, wherein, CPUs used as GPU controllers do not participate as compute nodes during the calculations.</p> <p>The GPUs and CPUs used for calculations are specified with the <code>%GPUCPU</code> command, where gpu- and cpu-lists are comma-separated lists, possibly including numerical ranges (e.g., 0-4,6). The corresponding items in the two lists are the GPU and its controlling CPU.</p> Text Only<pre><code>%cpu=0-9\n%gpucpu=0-1=0-1\n</code></pre> <p>NB! The controlling CPUs are included in the <code>%CPU</code> command.</p> <p>NB! The GPU and CPU count starts from zero.</p> <p>Example of gaussian-gpu.slurm script for amp:</p> Bash<pre><code>#!/bin/bash\n#SBATCH --job-name=Job_Name\n#SBATCH -t 1:00:00\n#SBATCH --no-requeue\n#SBATCH --partition=gpu        # Partition\n#SBATCH --gres=gpu:A100:2      # 2 GPU are reserved\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=10     # 10 CPU are reserved\n#SBATCH --mem=160GB            # Memory\n\nmodule use /gpfs/mariana/modules/system\nmodule load amp/all\nmodule load Gaussian/16.c02\n\nSCRATCH=/state/partition1/$SLURM_JOB_ID\nexport GAUSS_SCRDIR=$SCRATCH\nmkdir -p $SCRATCH\n\ng16 job.com &gt; job.log\n\n# Clean after yourself\nrm -rf $SCRATCH\n</code></pre> <p>Example of Gaussian input job-gpu.com (bad example, since the molecule is small):</p> Text Only<pre><code>%mem=160GB\n%cpu=0-9\n%gpucpu=0-1=0-1\n# wb97xd/cc-pVDZ opt\n\nGPU test\n\n0 1\nN                  0.15134980    0.09540020    1.45819090\nC                  0.75130720   -1.21343470    1.83361500\nH                 -0.39763650    0.48328420    2.23924210\nH                  0.89227330    0.78663430    1.26643050\nC                  2.08354180   -1.05427080    2.58684570\nH                  2.46028270   -2.07052410    2.83958100\nH                  1.89047670   -0.54642730    3.56027240\nH                  0.94017970   -1.74157910    0.87502690\nC                 -0.27659390   -2.02386680    2.62322330\nH                 -1.22744720   -2.09807790    2.06259160\nH                  0.09671630   -3.04868580    2.81503150\nH                 -0.48448270   -1.55043170    3.60777900\nC                  3.18448030   -0.29819020    1.82025510\nC                  4.47118690   -0.16399430    2.65811660\nC                  3.50345720   -0.94054930    0.45675520\nH                  2.82725210    0.74302520    1.61940860\nC                  5.57163930    0.60375030    1.90697440\nH                  4.83763400   -1.18683360    2.90651280\nH                  4.24528280    0.32880900    3.62913610\nC                  4.59840130   -0.17643230   -0.29951390\nH                  3.82853470   -1.99290760    0.62773340\nH                  2.59267050   -0.99453480   -0.16920790\nC                  5.87589210   -0.03780040    0.54257470\nH                  6.49233690    0.65984540    2.52562860\nH                  5.23630520    1.65412070    1.74873850\nH                  4.81114970   -0.67947290   -1.26682010\nH                  4.21177720    0.83532620   -0.55856640\nH                  6.64325110    0.55322730   -0.00130230\nH                  6.31606730   -1.04822960    0.70571860\n</code></pre>"},{"location":"software/gaussian/#allocation-of-memory","title":"Allocation of memory","text":"<p>Allocating sufficient amounts of memory for GPU jobs is even more important than for CPU jobs. GPUs can have up to 40GB (amp1) and 80GB (amp2) of memory, wherein, at least an equal amount of memory must be given to the GPU and each control CPU thread from the available 1 TB of RAM.</p> <p>Gaussian gives equal shares of memory to each thread, which means that the total memory allocated should be the number of threads times the memory required.</p>"},{"location":"software/gaussian/#restarting-a-failedinterrupted-calculation","title":"Restarting a failed/interrupted calculation","text":"<p>Killed or failed jobs can be restarted, but for this, a checkpoint file should be generated via a <code>%Chk</code> command within the Gaussian input file. For more information see Gaussian FAQ, Gaussian restart and Using Gaussian Checkpoint Files.</p> <p>NB! Checkpoint files are very heavy and are readable only on the machine on which they were generated. After successful completion of the calculation, it is recommended to delete these files.</p>"},{"location":"software/gaussian/#how-to-cite","title":"How to cite","text":"<ul> <li>Gaussian 16 - https://gaussian.com/citation/</li> <li>Gaussian 09 - Gaussian 09, Revision C.01, Frisch, M.J.; Trucks, G.W.; Schlegel, H.B.; Scuseria, G.E.; Robb, M.A.; Cheeseman, J.R.; Scalmani, G.; Barone, V.; Mennucci, B.; Petersson, G.A.; Nakatsuji, H.; Caricato, M.; Li, X.; Hratchian, H.P.; Izmaylov, A.F.; Bloino, J.; Zheng, G.; Sonnenberg, J.L.; Hada, M.; Ehara, M.; Toyota, K.; Fukuda, R.; Hasegawa, J.; Ishida, M.; Nakajima, T.; Honda, Y.; Kitao, O.; Nakai, H.; Vreven, T.; Montgomery, J.A., Jr.; Peralta, J.E.; Ogliaro, F.; Bearpark, M.; Heyd, J.J.; Brothers, E.; Kudin, K.N.; Staroverov, V.N.; Kobayashi, R.; Normand, J.; Raghavachari, K.; Rendell, A.; Burant, J.C.; Iyengar, S.S.; Tomasi, J.; Cossi, M.; Rega, N.; Millam, N.J.; Klene, M.; Knox, J.E.; Cross, J.B.; Bakken, V.; Adamo, C.; Jaramillo, J.; Gomperts, R.; Stratmann, R.E.; Yazyev, O.; Austin, A. J.; Cammi, R.; Pomelli, C.; Ochterski, J. W.; Martin, R.L.; Morokuma, K.; Zakrzewski, V.G.; Voth, G.A.; Salvador, P.; Dannenberg, J.J.; Dapprich, S.; Daniels, A.D.; Farkas, \u00d6.; Foresman, J.B.; Ortiz, J.V.; Cioslowski, J.; Fox, D.J. Gaussian, Inc., Wallingford CT, 2009.</li> </ul>"},{"location":"software/gaussian/#benchmarks-for-parallel-jobs","title":"Benchmarks for parallel jobs","text":"<p>Gaussian example benchmarks performed with Gaussian 16 C01. The job had 37 atoms.</p> <p></p> <p></p> <p></p> <p></p>"},{"location":"software/jupyter/","title":"JupyterLab","text":"<p>JupyterLab is an interactive notebook environment (with a web-browser interface) for Julia, Python, Octave, and R.</p>"},{"location":"software/jupyter/#jupyterlab-short-introduction","title":"JupyterLab Short Introduction","text":"<ol> <li> <p>Download the jupyterlab.slurm or jupyterlab-gpu.slurm batch script:</p> Bash<pre><code>#!/bin/bash\n#SBATCH --nodes 1\n#SBATCH --ntasks 1\n#SBATCH --time 01:30:00\n# #SBATCH -p gpu                # Uncomment if needed\n# #SBATCH --gres=gpu:1          # Uncomment if needed\n\n# Load Python modules\nmodule load rocky8-spack/\nmodule load r\nmodule load julia\nmodule load octave\nmodule load py-jupyterlab\nmodule load py-pip\n# module load cuda              # Uncomment if needed\n\nmodule list\necho $SLURM_JOB_NODELIST\n\n# Create Jupyter config\ncat &lt;&lt; EOF &gt; JupyterLab.conf.py\n# Set IP to '*' to bind on all interfaces\nc.NotebookApp.ip = '*'\nc.NotebookApp.open_browser = False\n\n# It is a good idea to set a known, fixed port for server access\nc.NotebookApp.port = 9900\nEOF\n\n# jupyter-lab --no-browser\njupyter-lab --no-browser --config=JupyterLab.conf.py\n\n# Remove Jupyter config\nrm JupyterLab.conf.py\n</code></pre> </li> <li> <p>Submit the job on base</p> Bash<pre><code>ssh uni-ID@base.hpc.taltech.ee\nsbatch jupyterlab.slurm\n</code></pre> </li> <li> <p>Check the <code>slurm-JOBID.out</code> file for the URL, host, and port to connect to.</p> <p>NB! Text in <code>slurm-JOBID.out</code> can appear with some delay.</p> </li> <li> <p>Close the terminal and establish a new connection to base with port forwarding</p> Bash<pre><code>ssh uni-ID@base.hpc.taltech.ee -L 99YY:greenX:99YY\n</code></pre> </li> <li> <p>Point your web browser to the URL from the <code>slurm-JOBID.out</code>, e.g.</p> Bash<pre><code>http://green1:9900/lab?token=6ddef678a64245352e74fc66987d378ce075f390d89a2c7f\n</code></pre> </li> </ol> <p>NB: Node name (green1 in the above example), port (here 9900), and token will be different for each job you submit! More details can be found in the Accessing Jupyter web interface part.</p>"},{"location":"software/jupyter/#jupyterlab-on-taltech-hpc-long-version","title":"JupyterLab on TalTech HPC (Long Version)","text":""},{"location":"software/jupyter/#network-access-to-taltech-hpc","title":"Network Access to TalTech HPC","text":"<p>To access the HPC head-node base.hpc.taltech.ee, you have to use some Estonia network or VPN.</p> <p>TalTech OpenVPN can be used for access outside of the Estonia network.</p>"},{"location":"software/jupyter/#copy-the-slurm-start-script-to-hpc-head-node","title":"Copy the SLURM Start Script to HPC Head Node","text":""},{"location":"software/jupyter/#1-command-line-solution","title":"1. Command Line Solution","text":"<p>Open the command prompt.</p> <p>On Linux and Mac, open any terminal app.</p> <p>On Windows, open the PowerShell app (preferred) or Command Prompt app.</p> <p>Log into the HPC head-node with an SSH client (UniID - your UniID, usually a six-letter string):</p> Bash<pre><code>ssh uni-ID@base.hpc.taltech.ee\nmkdir Jupyter  # Create folder for your Jupyter\nexit  \n</code></pre> <p>Copy the sbatch script jupyterlab.slurm (for the SLURM queuing system) to your newly created Jupyter folder in HPC.</p> <p>NB! The file jupyterlab.slurm must be in the same folder where you execute the copy command.</p> <p>Copy with sftp:</p> Bash<pre><code>sftp uni-ID@base.hpc.taltech.ee:Jupyter/\nput local/path/from/where/to/copy/jupyterlab.sh \n</code></pre> <p>Copy with scp:</p> Bash<pre><code>scp local/path/from/where/to/copy/jupyterlab.sh uni-ID@base.hpc.taltech.ee:Jupyter/ \n</code></pre>"},{"location":"software/jupyter/#2-gui-solution","title":"2. GUI Solution","text":"<p>Download sftp capable file transfer software.</p> <p>For example, WinSCP (Windows) or FileZilla Client (any platform).</p> <p>Install the sftp client, connect to base.hpc.taltech.ee, make your work directory, and copy the jupyterlab.slurm file. More details about using WinSCP and FileZilla Client can be found here.</p>"},{"location":"software/jupyter/#starting-jupyterlab-as-a-batch-job-in-hpc","title":"Starting JupyterLab as a Batch Job in HPC","text":"<p>If necessary, start VPN connection.</p> <p>Open the terminal and ssh to HPC:</p> Bash<pre><code>ssh uni-ID@base.hpc.taltech.ee\n</code></pre> <p>Change the directory to the Jupyter project folder and start JupyterLab as a batch job:</p> Bash<pre><code>cd Jupyter\nsbatch jupyterlab.slurm\n</code></pre> <p>Then the system will say: <code>Submitted batch job JOBID</code></p> <p>Where JOBID is the unique number of a submitted job.</p> <p>You can check whether the job runs, waits in the queue, or has finished with the command:</p> <p><code>squeue -u $uni-ID</code></p> <p>It gives you information about the job status, on which machine it runs, how long it has worked, etc.</p> <p>As a default behavior, the submitted job standard output will be written into the file slurm-JOBID.out.</p> <p>If you need to stop and remove the job, this can be done with the command <code>scancel JOBID</code>.</p>"},{"location":"software/jupyter/#values-in-jupyterlabsh-slurm-batch-job-script","title":"Values in jupyterlab.sh SLURM Batch Job Script","text":"<p>Values concerning the queuing system SLURM:</p> Bash<pre><code>#SBATCH --time 01:30:00 # Max runtime of the job, in this example 1 hour 30 minutes\n#SBATCH --ntasks 1  # Number of cores\n#SBATCH --nodes 1 # Job must run on one node\n</code></pre> <p>Jupyter specific:</p> Bash<pre><code>c.NotebookApp.port = 9900 # The port where Jupyter can be accessed\n</code></pre> <p>If the specified port is occupied, then Jupyter takes the next available port. The actual port number is shown in the slurm output file.</p>"},{"location":"software/jupyter/#accessing-jupyter-web-interface","title":"Accessing Jupyter Web Interface","text":"<p>You have to create an ssh tunnel. Enter into the terminal:</p> Bash<pre><code>ssh uni-ID@base.hpc.taltech.ee -L 99YY:greenX:99YY \n</code></pre> <p>Where \"99YY\" is the port number of the Jupyter web server and \"greenX\" is the name of the node where the Jupyter job runs. The port YY and compute node X name must be replaced with the actual values. These values are in the SLURM out file (unique to the running job). To check the content of the SLURM out file, the <code>more</code> or <code>less</code> command can be used:</p> Bash<pre><code>less slurm-JOBID.out\n</code></pre> <p>NB! To quit <code>less</code>, press <code>q</code>.</p> <p>The end part of the file looks something like:</p> Bash<pre><code>To access the server, open this file in a browser:\n    file:///gpfs/mariana/home/.......\nOr copy and paste one of these URLs:\n    http://green27:9900/lab?token=6ddef678a64245352e74fc66987d378ce075f390d89a2c7f\n or http://127.0.0.1:9900/lab?token=6ddef678a64245352e74fc66987d378ce075f390d89a2c7f\n</code></pre> <p>In this example, the port number is 9901 and the compute name is green27. So you have to replace these values in the tunneling command presented above. The command will look now:</p> Bash<pre><code>ssh uni-ID@base.hpc.taltech.ee -L 9901:green27:9901 \n</code></pre> <p>If your tunnel is running (ssh connection is active), then JupyterLab can be accessed from the address:</p> Bash<pre><code>http://127.0.0.1:9901/lab?token=4046f45de18c9523525ed8d972d48618ee333c6417e640f6\n</code></pre> <p>Open the above-presented address in your browser. NB! These values are unique and different during each run.</p>"},{"location":"software/jupyter/#with-the-new-spack-based-modules","title":"With the New SPACK-Based Modules","text":"<p>The following modules need to be loaded to activate Julia, Octave, and Python 3.8.12 in JupyterLab:</p> Bash<pre><code>module load rocky8-spack/\nmodule load r\nmodule load julia\nmodule load octave\nmodule load py-jupyterlab\nmodule load py-pip\n</code></pre>"},{"location":"software/jupyter/#first-time-only-prepare-non-python-kernels","title":"First Time Only, Prepare Non-Python Kernels","text":"<p>Do the following in the command line on base.</p> <p>For Julia (start Julia and add package):</p> Bash<pre><code>julia\n]\nadd IJulia\n</code></pre> <p>For Octave:</p> Bash<pre><code>python -m pip install --user octave_kernel\n</code></pre> <p>Then proceed as above (in the non-SPACK case, but change the module lines in the .slurm script).</p> <p></p>"},{"location":"software/jupyter/#jupyter-notebooks-as-non-interactive-jobs","title":"Jupyter Notebooks as Non-Interactive Jobs","text":"<p>Jupyter notebooks can be run non-interactively from the command line using:</p> <ul> <li> <p>the built-in <code>nbconvert</code></p> Bash<pre><code>jupyter nbconvert --to notebook --inplace --execute mynotebook.ipynb -ExecutePreprocessor.timeout=None\n</code></pre> <p>You can also create a PDF from the notebook by running</p> Bash<pre><code>jupyter nbconvert --to PDF --execute mynotebook.ipynb -ExecutePreprocessor.timeout=None\n</code></pre> </li> <li> <p>the <code>papermill</code> package</p> Bash<pre><code>papermill mynotebook.ipynb mynotebook_output.ipyn [args...]\n</code></pre> <p>Papermill makes it possible to pass parameters, e.g., setting start and end variables</p> Bash<pre><code>papermill mynotebook.ipynb mynotebook_output.ipyn -p start \"2017-11-01\" -p end \"2017-11-30\"\n</code></pre> </li> </ul>"},{"location":"software/modules/","title":"Module environment (lmod)","text":""},{"location":"software/modules/#short-introduction","title":"Short Introduction","text":"<p>HPC has a module system. To use some applications, users need to follow these two steps and insert applications into the search path:</p> <ol> <li> <p>Determine the machine type (e.g., amp or green) by command:</p> Bash<pre><code>module load rocky8-spack      # for most free programs (SPACK package manager)\n</code></pre> <p>or</p> Bash<pre><code>module load rocky8/all        # for licensed programs and some free (non-SPACK managed)\n</code></pre> </li> <li> <p>Load the needed program:</p> Bash<pre><code>module load tau\n</code></pre> </li> </ol> <p>The list of available modules can be viewed by:</p> Bash<pre><code>module avail\n</code></pre> <p>where:</p> <p>Lic - a license is required, see the user guide for more information Uni - commercial software with a site license, the number of concurrent processes may be limited Reg - registration required, see the user guide for more information L - module is loaded Dp - deprecated (old modules, which have been superseded by modules in the new structure) O - obsolete (module moved or superseded by SPACK module) Exp - experimental module, used while testing software installation, module name may change, or software may be deleted D - default module.</p>"},{"location":"software/modules/#long-version","title":"Long Version","text":"<p>The module system is used to manage settings for different applications. Many applications and libraries are not in the standard search path; this way, it is possible to install two different versions of the same software/library that would otherwise create conflicts. The module system is used to insert applications into the search path (or remove them from it) on a per-user and per-occasion basis.</p>"},{"location":"software/modules/#useful-commands","title":"Useful Commands","text":"<ul> <li>All available modules can be viewed by the command:</li> </ul> Bash<pre><code>module avail\n</code></pre> <p>Example output:</p> <p></p> <p>Modules are grouped in a hierarchy; there may be several versions of the same software installed, e.g., of the MPI library. Only one of these can be loaded at a single time. The default module of a group is marked by <code>(D)</code>. If there is only one module in a group, this is the default (unmarked).</p> <ul> <li> <p>To load a certain version of a module/program (here - Open MPI 4.1.1-gcc-8.5.0-r8-ib):</p> Bash<pre><code>module load openmpi/4.1.1-gcc-8.5.0-r8-ib\n</code></pre> <p>To load the default module/program marked <code>(D)</code> (here - Open MPI 4.1.1-gcc-10.3.0-r8-tcp):</p> Bash<pre><code>module load openmpi/\n</code></pre> </li> <li> <p>To list all loaded modules:</p> Bash<pre><code>module list\n</code></pre> </li> <li> <p>Unloading a module (here - Cuda 11.3.1-gcc-10.3.0-ehi3):</p> Bash<pre><code>module unload cuda/11.3.1-gcc-10.3.0-ehi3\n</code></pre> </li> <li> <p>Finding a module containing a certain part (here - fem):</p> Bash<pre><code>module keyword fem\n</code></pre> <p>This will list all modules that have \"fem\" in the description:</p> <p></p> </li> <li> <p>To find out more about a specific module (here - mfem):</p> Bash<pre><code>module spider mfem\n</code></pre> <p>This gives:</p> <p></p> </li> <li> <p>The <code>module whatis</code> command gives you a short explanation of what the software is about, e.g.,</p> <p></p> <p>or</p> <p></p> </li> </ul>"},{"location":"software/modules/#files-modulerclua-and-bashrc","title":"Files .modulerc.lua and .bashrc","text":"<p>Personal preferences and resources can be specified in the files <code>.modulerc.lua</code> and <code>.bashrc</code> in the user's <code>$HOME</code> directory. For example, it is possible to add a path for your own module files for software installed by the user in the user's <code>$HOME</code> directory, automatically load some modules on login, and define your own \"default\" modules using the entry <code>module_version(\"r/4.1.1-gcc-10.3.0-zwgc\",\"default\")</code> or introduce abbreviations using an entry like <code>module_alias(\"z13\", \"r/4.1.1-gcc-10.3.0-zwgc\")</code> to define a module alias \"z13\".</p> <p>Example of <code>.modulerc.lua</code> file:</p> Lua<pre><code>module_version(\"r/4.1.1-gcc-10.3.0-zwgc\",\"default\")\nmodule_alias(\"z13\",\"r/4.1.1-gcc-10.3.0-zwgc\")\n\nmodule_version(\"p/20.2-gcc-10.3.0-python-2.7.18-ij2m\",\"default\")\nmodule_alias(\"p20\",\"p/20.2-gcc-10.3.0-python-2.7.18-ij2m\")\n</code></pre> <p>Example of <code>.bashrc</code> file:</p> Bash<pre><code># .bashrc\n\n# Source global definitions\nif [ -f /etc/bashrc ]; then\n  . /etc/bashrc\nfi\n\n# Uncomment the following line if you don't like systemctl's auto-paging feature:\n# export SYSTEMD_PAGER=\n\n# User-specific aliases and functions\nmodule load rocky8-spack\nmodule load r/4.1.1-gcc-10.3.0-zwgc\n</code></pre>"},{"location":"software/modules/#module-groups","title":"Module Groups","text":"<p>We moved to a new module structure! Modules from <code>/share/apps/modules</code> are being retired. Software is compiled for <code>x86-64</code> and will run on all nodes (no special optimization). Optimized versions for some software for specific nodes may follow later (or not).</p> <p>New modules are grouped; you can activate them by loading one or more of the following modules:</p> Module Group Description rocky8/all manually installed software rocky8-spack software installed with SPACK package manager"},{"location":"software/modules/#modules-used-on-viz","title":"Modules Used on viz","text":"<p>To make the module system work on viz, the following needs to be added to your <code>$HOME/.bashrc</code>:</p> Bash<pre><code>if [[ $(hostname -s) = viz ]]; then\n  source /usr/share/lmod/6.6/init/bash\n  module use /gpfs/mariana/modules/system\nfi\n</code></pre> <p>Further access viz and load modules needed. For example:</p> Bash<pre><code>ssh -X -A -J UNI-ID@base.hpc.taltech.ee UNI-ID@viz.hpc.taltech.ee\n\nmodule load viz-spack\nmodule load jmol\n</code></pre> <p>More about the use of viz can be found on the visualization page.</p>"},{"location":"software/modules/#available-modules","title":"Available Modules","text":"<p>Currently, the following modules are available. This serves as an example; please note that the list on this page will be updated very seldom. Use <code>module avail</code> after login to get an up-to-date list of the available modules.</p> <p>By default, SPACK builds are optimized for the CPU the software is built on. The packages from the amp nodes will work on the green nodes but slower than the optimized modules. Conversely, the skylake-optimized modules will try to use hardware features not present on the green nodes, so the software will not run there.</p>"},{"location":"software/multiwfn/","title":"Multiwfn","text":"<p>Warning</p> <p>This page is a work in progress!</p>"},{"location":"software/multiwfn/#multiwfn-short-introduction","title":"Multiwfn Short Introduction","text":"<ol> <li> <p>Make Multiwfn input <code>.mwfn</code>, <code>.wfn</code>, <code>.wfx</code>, <code>.fch</code>, <code>.molden</code>, <code>.gms</code> (or <code>.cub</code>, <code>.grd</code>, <code>.pdb</code>, <code>.xyz</code>, <code>.mol</code> - for specific purposes).</p> </li> <li> <p>Access viz by remote access programs (more preferable) or by SSH protocol (less preferable):</p> Bash<pre><code>ssh -X -Y -J UNI-ID@base.hpc.taltech.ee UNI-ID@viz\n</code></pre> </li> <li> <p>Load environment:</p> Bash<pre><code>module use /gpfs/mariana/modules/green/chemistry/\nmodule load MultiWFN/3.7\n</code></pre> </li> <li> <p>Run Multiwfn in interactive mode:</p> Bash<pre><code>srun Multiwfn job.wfn\n</code></pre> <p>Multiwfn can also be run by multiwfn.slurm batch script in non-interactive mode with pre-prepared responses:</p> Bash<pre><code>#!/bin/bash\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --job-name=test\n#SBATCH --mem=2GB\n#SBATCH -t 1:00\n#SBATCH --partition=short\n\nmodule load green/all\nmodule load MultiWFN/3.7 \n\nMultiwfn job.wfn &lt;&lt; EOF &gt; /dev/null\n2\n2\n-4\n6\n0\n-10\n100\n2\n1\nmol.pdb\nq\nEOF\n</code></pre> <p>In this case, the job is submitted using the <code>sbatch</code> command:</p> Bash<pre><code>sbatch multiwfn.slurm\n</code></pre> </li> <li> <p>Visualize results if needed:</p> Bash<pre><code>display job.png\n</code></pre> <p>or</p> Bash<pre><code>module use /gpfs/mariana/modules/gray/spack/\nmodule load vmd\nvmd job.pdb\n</code></pre> <p>NB! It is recommended to visualize Multiwfn results in the VMD program. Corresponding scripts are provided in Multiwfn examples (/gpfs/mariana/software/green/MultiWFN/Multiwfn_3.7_bin_Linux/examples/).</p> </li> </ol>"},{"location":"software/multiwfn/#multiwfn-long-version","title":"Multiwfn Long Version","text":""},{"location":"software/multiwfn/#options","title":"Options","text":"<p>Multiwfn is an interactive program performing almost all important wavefunction analyses (showing molecular structure and orbitals, calculating real space function, topology analysis, population analysis, orbital composition analysis, bond order/strength analysis, plotting population density-of-states, plotting various kinds of spectra (including conformational weighted spectrum), quantitative analysis of molecular surface, charge decomposition analysis, basin analysis, electron excitation analyses, orbital localization analysis, visual study of weak interaction, conceptual density functional theory (CDFT) analysis, energy decomposition analysis).</p> <p>For many frequently used analyses, Multiwfn has short YouTube videos and \"quick start\" examples (/gpfs/mariana/software/green/MultiWFN/Multiwfn_3.7_bin_Linux/examples/). More information can be found in the manual.</p>"},{"location":"software/multiwfn/#input","title":"Input","text":"<p>As input, Multiwfn uses output files of other quantum chemistry programs, including Gaussian, ORCA, GAMESS-US, NWChem, xtb, Turbomole. For example, <code>.wfn</code> (wavefunction file), <code>.fch</code> (Gaussian check file), <code>.molden</code> (Molden input file), <code>.gms</code> (GAMESS-US output file), <code>.mwfn</code> (Multiwfn wavefunction file). Other types of files, such as <code>.cub</code>, <code>.grd</code>, <code>.pdb</code>, <code>.xyz</code>, <code>.log</code>, <code>.out</code>, and <code>.mol</code> files, may be used in certain cases and purposes.</p>"},{"location":"software/multiwfn/#environment","title":"Environment","text":"<p>On viz, the environment is set up by the commands:</p> Bash<pre><code>module use /gpfs/mariana/modules/green/chemistry/\nmodule load MultiWFN/3.7\n</code></pre> <p>For the first-time use, the user has to agree to the licenses:</p> Bash<pre><code>touch ~/.licenses/multiwfn-accepted\n</code></pre> <p>If this is the first user license agreement, the following commands should be given:</p> Bash<pre><code>mkdir .licenses\ntouch ~/.licenses/multiwfn-accepted\n</code></pre> <p>NB! After agreeing to the license, the user has to log out and log in again to be able to run <code>Multiwfn</code>.</p> <p>On base, the environment is set up by the commands:</p> Bash<pre><code>module load rocky8/all\nmodule load MultiWFN/3.7\n</code></pre> <p>The user also needs to agree with the licenses, as described above.</p>"},{"location":"software/multiwfn/#running-multiwfn","title":"Running Multiwfn","text":"<p>NB! Since Multiwfn has a lot of functionality, we recommend that the user first study the corresponding section in the manuals (text, video) or examples (/gpfs/mariana/software/green/MultiWFN/Multiwfn_3.7_bin_Linux/examples/). This will greatly simplify the selection of answers in the interactive menu.</p> <p>The best practice is to try to reproduce something from the examples folder. To do this, the corresponding files will need to be copied to the user's directory using the following commands:</p> Bash<pre><code>mkdir examples\ncp -r /gpfs/mariana/software/green/MultiWFN/Multiwfn_3.7_bin_Linux/examples/* examples/\n</code></pre> <p>NB! The user can run Multiwfn only from their own folder, not from the shared one.</p> <p>For visualization that does not perform additional calculations but only reads outputs (for example, spectra visualization), Multiwfn can be run in interactive mode using <code>srun</code>:</p> Bash<pre><code>srun Multiwfn job.log\n</code></pre> <p>or using several threads (here - 4):</p> Bash<pre><code>srun -n 4 Multiwfn job.log\n</code></pre> <p>To exit interactive mode, press the <code>q</code> key.</p> <p>For jobs connected to electron density analysis, especially in large systems, it is recommended to run the multiwfn.slurm batch script with pre-prepared responses. Below is a slurm script for Critical Points (CPs) search using job.wfn:</p> Bash<pre><code>#!/bin/bash\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --job-name=test\n#SBATCH --mem=2GB\n#SBATCH -t 1:00\n#SBATCH --partition=short\n\nmodule load rocky8/all\nmodule load MultiWFN/3.7\n\nMultiwfn job.wfn &lt;&lt; EOF &gt; /dev/null\n2\n2\n-4\n6\n0\n-10\n100\n2\n1\nmol.pdb\nq\nEOF\n</code></pre> <p>The job is submitted by the <code>sbatch</code> command:</p> Bash<pre><code>sbatch multiwfn.slurm\n</code></pre>"},{"location":"software/multiwfn/#results-visualization","title":"Results Visualization","text":"<p>By default, plots made by Multiwfn will be written in the <code>.png</code> format and can be visualized by the command:</p> Bash<pre><code>display job.png\n</code></pre> <p>Although Multiwfn has its own graphical interface, we recommend visualizing Multiwfn results in the VMD (Visual Molecular Dynamics) program. Corresponding scripts are provided in Multiwfn examples (/gpfs/mariana/software/green/MultiWFN/Multiwfn_3.7_bin_Linux/examples/) (with <code>.vmd</code> extensions). More about visualization on viz can be found here and about VMD - here.</p> <p>On base, the VMD environment is set up by the commands:</p> Bash<pre><code>module load green\nmodule load VMD\n</code></pre> <p>VMD is run by the command <code>vmd</code>:</p> Bash<pre><code>vmd job.pdb\n</code></pre>"},{"location":"software/multiwfn/#how-to-cite","title":"How to Cite","text":"<p>Citing the original paper of Multiwfn is mandatory - DOI: 10.1002/jcc.22885</p> <p>In addition, the following articles should be cited depending on the analyses performed:</p> <ul> <li>Quantitative molecular surface analysis (main function 12) - DOI: 10.1016/j.jmgm.2012.07.004</li> <li>Hole-electron analysis (subfunction 1 of main function 18) - DOI: 10.1016/j.carbon.2020.05.023</li> <li>Electrostatic potential evaluation algorithm - DOI: 10.1039/D1CP02805G</li> <li>Orbital composition analysis (main function 8) - Tian Lu, Feiwu Chen, Calculation of Molecular Orbital Composition, Acta Chim. Sinica, 69, 2393-2406 (2011) (in Chinese) (http://sioc-journal.cn/Jwk_hxxb/CN/abstract/abstract340458.shtml)</li> <li>Charge decomposition analysis (CDA) (main function 16) - Meng Xiao, Tian Lu, Generalized Charge Decomposition Analysis (GCDA) Method, Journal of Advances in Physical Chemistry, 4, 111-124 (2015) (in Chinese) (http://dx.doi.org/10.12677/JAPC.2015.44013)</li> <li>Atomic dipole moment corrected Hirshfeld (ADCH) - DOI: 10.1142/S0219633612500113</li> <li>Population analysis module (main function 7) - DOI: 10.3866/PKU.WHXB2012281</li> <li>Laplacian bond order (LBO) - DOI: 10.1021/jp4010345</li> <li>Statistical analysis of area in different ESP ranges on vdW surface - DOI: 10.1007/s11224-014-0430-6</li> <li>Charge-transfer spectrum - DOI: 10.1016/j.carbon.2021.11.005</li> <li>Electron localization function (ELF) - DOI: 10.3866/PKU.WHXB20112786</li> <li>Analysis of valence electron and deformation density - DOI: 10.3866/PKU.WHXB201709252</li> <li>Predicting binding energy of hydrogen bonds based properties of bond critical point - DOI: 10.1002/jcc.26068</li> <li>Electron analysis based on localized molecular orbitals (e.g. subfunction 22 of main function 100) - DOI: 10.1007/s00214-019-2541-z</li> <li>Van der Waals potential analysis (subfunction 6 of main function 20) - DOI: 10.1007/s00894-020-04577-0</li> <li>Interaction region indicator (IRI) (subfunction 4 of main function 20) - DOI: 10.1002/cmtd.202100007</li> <li>Independent gradient model based on Hirshfeld partition (IGMH) (subfunction 11 of main function 20) - DOI: 10.1002/jcc.26812</li> <li>ICSSZZ map (subfunction 4 in main function 200) - DOI: 10.1016/j.carbon.2020.04.099</li> <li>MO-PDOS map (Option -2 in main function 10) - DOI: 10.1016/j.carbon.2020.05.023</li> <li>Molecular polarity index (MPI) (outputted by main function 12) - DOI: 10.1016/j.carbon.2020.09.048</li> <li>Analysis of valence electron and deformation density - DOI: 10.3866/PKU.WHXB201709252</li> <li>Studying molecular planarity via MPP, SDP and coloring atoms according to ds values - DOI: 10.1007/s00894-021-04884-0</li> <li>Energy decomposition analysis based on force field (EDA-FF) - DOI: 10.1016/j.mseb.2021.115425</li> </ul>"},{"location":"software/nwchem/","title":"NWChem","text":"<p>Warning</p> <p>This page is a work in progress!</p>"},{"location":"software/nwchem/#nwchem-short-introduction","title":"NWChem Short Introduction","text":"<ol> <li> <p>Make nwchem.slurm batch script for parallel calculations:</p> Bash<pre><code>#!/bin/bash\n\n#SBATCH --job-name=NWChem\n#SBATCH --mem-per-cpu=2GB\n#SBATCH --nodes=1\n#SBATCH --ntasks=6\n#SBATCH -t 1:00:00\n#SBATCH --partition=common\n\nmodule load rocky8-spack\nmodule load nwchem\n\n# Create scratch directory\nSCRATCH=/state/partition1/$SLURM_JOB_ID\nmkdir -p $SCRATCH\ncp  $SLURM_SUBMIT_DIR/*.nw $SCRATCH/\ncd $SCRATCH/\n\nnwchem job.nw &gt;&gt; job.out\n\n# Copy files back to working directory\ncp $SCRATCH/* $SLURM_SUBMIT_DIR\n\n# Clean after yourself\nrm -rf $SCRATCH\n</code></pre> </li> <li> <p>Copy job-input file job.nw.</p> </li> <li> <p>Submit the job on base:</p> Bash<pre><code>sbatch nwchem.slurm\n</code></pre> </li> <li> <p>Check results using visualization software.</p> </li> </ol>"},{"location":"software/nwchem/#nwchem-long-version","title":"NWChem Long Version","text":"<p>The North West computational chemistry (NWChem) is an ab initio computational chemistry software package. NWChem offers various approaches: density functional (DFT), second-order M\u00f6ller\u2013Plesset perturbation theory (MP2), single- and multi-reference (MR), ground-and excited-state and linear-response (LR) coupled-cluster (CC), multi-configuration self-consistent field (MCSCF), selected and full configuration interaction (CI). A broad range of DFT response properties, ground and excited-state molecular dynamics (MD) using either AMBER or CHARMM force fields or methods of quantum mechanics (QM), nudged elastic band (NEB) method, linear-response (LR), and real-time (RT) time-dependent density functional theory (TDDFT) are available in NWChem. Through its modular design, the ab initio methods can be coupled with classical MD to perform mixed quantum-mechanics and molecular-mechanics simulations (QM/MM). Various solvent models and relativistic approaches are also available. Additionally, Python programs may be embedded into the NWChem input and used to control the execution of NWChem. More about the possibilities of NWChem can be found in this article - 10.1063/5.0004997.</p> <p>Some useful links:</p> <ul> <li>The tutorials site at FAccTs</li> <li>DOI: 10.1016/j.cpc.2010.04.018</li> </ul>"},{"location":"software/nwchem/#environment","title":"Environment","text":"<p>At HPC, the 7.0.2 version of NWChem is installed. To start working with NWChem, an environment needs to be set up with the commands:</p> Bash<pre><code>module load rocky8-spack\nmodule load nwchem\n</code></pre>"},{"location":"software/nwchem/#input-file","title":"Input File","text":"<p>NWChem input files consist of certain blocks: geometry, SCF, DFT, MP2, etc. NWChem also allows combining several jobs into one input file. Below is an example of an NWChem input file (job.nw) where:</p> <ol> <li>The water dimer will be firstly optimized at BP86-D3BJ/def2-SVP level of theory.</li> <li>Frequency calculations will be done at the same level of theory.</li> <li>Single point energy will be calculated using a larger basis set (def2-TZVPP) and B3LYP functional.</li> </ol> <p>Additionally, in the example input file, the implementation of some useful keywords such as <code>print</code> and <code>linopt</code> is shown.</p> Bash<pre><code>start water            # all intermediate files will have this name\ntitle \"Water dimer\"    # title of job\n\necho                   # input file will be printed at the beginning of the output file\n\nmemory total 3000 mb\n\ncharge 0\n\ngeometry units angstrom\nO    -0.093470    -1.154274     0.290542\nH     0.329461    -0.566865    -0.340362\nH    -0.864449    -1.335840    -0.238173\nO    -0.135461     1.136660    -0.233474\nH     0.636237     1.304331     0.298468\nH    -0.563123     0.545569     0.390712\nend\n\nbasis\n    * library Def2-SVP\nend\n\nscf\n    rhf                    # restricted Hartree-Fock\n    singlet                # multiplicity\n    maxiter 100            # maximum number of SCF iterations \n    print low              # will minimize output\nend\n\ndft\n    mult 1                 # multiplicity\n    xc becke88 perdew86    # functional BP86\n    disp vdw 4             # dispersion correction D3BJ\n    print low              # will minimize output\nend\n\ndriver\n    maxiter 100            # maximum number of iterations during optimization\n    linopt 0               # speed up calculations \nend\n\ntask dft optimize\ntask dft freq numerical\n\nbasis\n    * library Def2-TZVPP\nend\n\ndft\n    mult 1                 # multiplicity\n    xc b3lyp               # functional B3LYP\n    disp vdw 4             # dispersion correction D3BJ\n    print low              # will minimize output\nend\n\ntask dft energy\n</code></pre> <p>NWChem is well suited for large system calculations or molecular dynamics simulations with subsequent calculation of system properties. An example of an input (job_MD.nw) for MD simulation with subsequent calculation of dipole moment every 10 steps.</p> <p>More about NWChem input can be found in the NWChem manual.</p>"},{"location":"software/nwchem/#running-nwchem-jobs","title":"Running NWChem Jobs","text":"<p>NWChem input files are executed by the command <code>nwchem</code>. This command is usually placed in a slurm script.</p>"},{"location":"software/nwchem/#single-core-parallel-calculations","title":"Single Core &amp; Parallel Calculations","text":"<p>NWChem jobs can be calculated on one thread, in parallel on one node, or using several nodes at once. Depending on the size of the job, the corresponding parameters must be modified in the slurm file:</p> Bash<pre><code>#SBATCH --ntasks=6\n#SBATCH --nodes=1\n</code></pre> <p>Below is an example of a <code>slurm</code> script for an NWChem parallel run on 1 node and 6 threads with allocated memory of 3 GB:</p> Bash<pre><code>#!/bin/bash\n\n#SBATCH --job-name=NWChem\n#SBATCH --mem=3GB\n#SBATCH --nodes=1\n#SBATCH --ntasks=6\n#SBATCH -t 1:00:00\n#SBATCH --partition=common\n\nmodule load rocky8-spack\nmodule load nwchem\n\n# Create scratch directory\nSCRATCH=/state/partition1/$SLURM_JOB_ID\nmkdir -p $SCRATCH\ncp  $SLURM_SUBMIT_DIR/*.nw $SCRATCH/\ncd $SCRATCH/\n\nnwchem job.nw &gt; job.out\n\n# Copy files back to working directory\ncp $SCRATCH/* $SLURM_SUBMIT_DIR\n\n# Clean after yourself\nrm -rf $SCRATCH\n</code></pre> <p>NB! In the example of the <code>slurm</code> script, calculations will be done on a single node, thus the partition is <code>common</code>. If several nodes will be used, then the partition should be <code>green-ib</code>.</p> Bash<pre><code>#SBATCH --nodes=2\n#SBATCH --ntasks=120\n#SBATCH --partition=green-ib\n</code></pre> <p>NB! To be able to restart calculations, they must be done in the <code>$HOME</code> catalog, and not in the <code>$SCRATCH</code> directory.</p>"},{"location":"software/nwchem/#restarting-a-failedinterrupted-calculation","title":"Restarting a Failed/Interrupted Calculation","text":"<p>NWChem does not give a message about normal termination. If the calculation terminated normally, the output will have this end:</p> Text Only<pre><code>                                  AUTHORS\n                                  -------\n E. Apra, E. J. Bylaska, N. Govind, K. Kowalski, M. Valiev, W. A. de Jong,\n  T. P. Straatsma, H. J. J. van Dam, D. Wang, T. L. Windus, N. P. Bauman,\n   A. Panyala, J. Hammond, J. Autschbach, K. Bhaskaran-Nair, J. Brabec,\nK. Lopata, S. A. Fischer, S. Krishnamoorthy, M. Jacquelin, W. Ma, M. Klemm,\n   O. Villa, Y. Chen, V. Anisimov, F. Aquino, S. Hirata, M. T. Hackler,\n       Eric Hermes, L. Jensen, J. E. Moore, J. C. Becca, V. Konjkov,\n    D. Mejia-Rodriguez, T. Risthaus, M. Malagoli, A. Marenich,\nA. Otero-de-la-Roza, J. Mullin, P. Nichols, R. Peverati, J. Pittner, Y. Zhao,\n    P.-D. Fan, A. Fonari, M. J. Williamson, R. J. Harrison, J. R. Rehr,\n  M. Dupuis, D. Silverstein, D. M. A. Smith, J. Nieplocha, V. Tipparaju,\n  M. Krishnan, B. E. Van Kuiken, A. Vazquez-Mayagoitia, M. Swart, Q. Wu,\nT. Van Voorhis, A. A. Auer, M. Nooijen, L. D. Crosby, E. Brown, G. Cisneros,\n G. I. Fann, H. Fruchtl, J. Garza, K. Hirao, R. A. Kendall, J. A. Nichols,\n   K. Tsemekhman, K. Wolinski, J. Anchell, D. E. Bernholdt, P. Borowski,\n   T. Clark, D. Clerc, H. Dachsel, M. J. O. Deegan, K. Dyall, D. Elwood,\n  E. Glendening, M. Gutowski, A. C. Hess, J. Jaffe, B. G. Johnson, J. Ju,\n    R. Kobayashi, R. Kutteh, Z. Lin, R. Littlefield, X. Long, B. Meng,\n  T. Nakajima, S. Niu, L. Pollack, M. Rosing, K. Glaesemann, G. Sandrone,\n  M. Stave, H. Taylor, G. Thomas, J. H. van Lenthe, A. T. Wong, Z. Zhang.\n\nTotal times  cpu:       56.9s     wall:       57.2s\n</code></pre> <p>If the job was not terminated normally, it can be restarted. However, to do this, calculations must be done in the <code>$HOME</code> catalog, and not in the <code>$SCRATCH</code> directory.</p> <p>To restart the calculation, just change the <code>start</code> command to <code>restart</code> in the initial input file and run the slurm script again.</p> <p>NB! We recommend changing the restart output file name so it is possible to compare progress at the end of calculations.</p>"},{"location":"software/nwchem/#memory","title":"Memory","text":"<p>At the beginning of the NWChem input file, the amount of memory requested for the entire job must be specified. If the amount of memory requested is insufficient, the job can crash. Memory usage in NWChem is controlled by the <code>memory total</code> keywords.</p> Bash<pre><code>memory total 3000 mb\n</code></pre> <p>There is no golden rule for memory requests, since they are basis set and calculation type dependent. Usually, 1-5 GB per 1 CPU is sufficient. Data from a <code>slurm-JOBID.stat</code> file can be useful to determine the amount of memory required for a computation. In the <code>slurm-JOBID.stat</code> file, the efficiency of memory utilization is shown.</p> <p>Bad example:</p> Bash<pre><code>Memory Utilized: 3.08 GB \nMemory Efficiency: 11.83% of 26.00 GB\n</code></pre> <p>Good example:</p> Bash<pre><code>Memory Utilized: 63.12 GB \nMemory Efficiency: 98.62% of 64.00 GB\n</code></pre>"},{"location":"software/nwchem/#time","title":"Time","text":"<p>Time limits depend on the time partition used, see taltech user-guides. If the calculation time exceeds the time limit requested in the <code>slurm</code> script, then the job will be killed. Therefore, it is recommended to request more time than is usually needed for the calculation.</p>"},{"location":"software/nwchem/#how-to-cite","title":"How to Cite","text":"<p>Please cite DOI: 10.1063/5.0004997 when publishing results obtained with NWChem.</p> <p>Also, look at the NWChem manual on the relevant topic; more detailed information on citing will be given there.</p>"},{"location":"software/octave/","title":"Octave/Matlab","text":"<p>Warning</p> <p>This page has not been completely updated for Rocky 8 yet!</p> <p>Octave is a free alternative to Matlab, and if you do not use Matlab's toolboxes, your code should work without many changes. For many of Matlab's toolboxes, (partial) implementations exist for Octave as well. We have set up a module for Octave:</p> Bash<pre><code>module load octave\n</code></pre>"},{"location":"software/octave/#octave-example-of-a-non-interactive-batch-job-single-process","title":"Octave: Example of a non-interactive batch job (single process)","text":"<p>SLURM batch script <code>octave-script.slurm</code></p> Bash<pre><code>#!/bin/bash\n#SBATCH --partition=common\n#SBATCH -t 2:10:00\n#SBATCH -J octave\nmodule load octave\noctave script.m\n</code></pre> <p>The commands that Octave should calculate are in <code>script.m</code>:</p> Octave<pre><code>n = 1000; \nA = normrnd(0,1, n, n); \nX = A'*A; Y = inv(X); \na = mean( diag(Y*X) ); \n## should output 1.0: \na\n</code></pre>"},{"location":"software/octave/#octave-netcdf-toolbox","title":"Octave netcdf toolbox","text":"<p>To use netcdf in Octave, the toolbox octcdf has to be installed from Octave Forge. Note that octcdf is a NetCDF toolbox for Octave which aims to be compatible with the \"original\" Matlab toolbox.</p> <p>To install the toolbox, follow these steps on the frontend, and later the package will be available on all nodes for your user.</p> Bash<pre><code>module load green-spack\nmodule load octave\noctave\n</code></pre> <p>Inside Octave, do:</p> Octave<pre><code>pkg install -forge -verbose octcdf\n</code></pre> <p>You may need to scroll down when necessary, and it should complete the compilation successfully. Then you can start using Octcdf in your Octave scripts by adding the line:</p> Octave<pre><code>pkg load octcdf\n</code></pre>"},{"location":"software/octave/#matlab","title":"Matlab","text":"<p>Matlab is available on the cluster through a campus license. Use</p> Bash<pre><code>module avail\n</code></pre> <p>to see which version is installed. If a newer version is needed, contact us via hpcsupport. Start using it by loading the module:</p> Bash<pre><code>module load green\nmodule load Matlab/R2018a\n</code></pre> <p>Matlab can be used non-interactively (like Octave in the above example).</p>"},{"location":"software/openfoam/","title":"OpenFOAM","text":""},{"location":"software/openfoam/#quick-start-example-use-of-openfoam-on-base-cluster","title":"Quick-start: Example use of OpenFOAM on BASE cluster","text":"<p>For the example, we will use one of the tutorial cases.</p> <ol> <li> <p>Load environment:</p> Bash<pre><code>module load rocky8-spack\nmodule load openfoam\n</code></pre> </li> <li> <p>First-time users need to create their <code>$WM_PROJECT_USER_DIR</code>:</p> Bash<pre><code>export WM_PROJECT_USER_DIR=$HOME/OpenFOAM/$USER-$WM_PROJECT_VERSION\nmkdir $WM_PROJECT_USER_DIR --parent\n</code></pre> </li> <li> <p>Copy the damBreak tutorial case into the <code>$WM_PROJECT_USER_DIR</code> and go into the folder damBreak:</p> Bash<pre><code>cp -r $FOAM_TUTORIALS/multiphase/interFoam/laminar/damBreak/damBreak $WM_PROJECT_USER_DIR/\ncd $WM_PROJECT_USER_DIR/damBreak\npwd\n</code></pre> </li> <li> <p>Now we can run the OpenFOAM case step-by-step or as a batch job.</p> Bash<pre><code>srun --partition=common -t 2:10:00 --pty bash\nblockMesh\nsetFields\ninterFoam\n</code></pre> <p>NB: Do not use the <code>Allrun</code> script(s) of the tutorials, as these may try to launch parallel jobs without requesting resources.</p> </li> <li> <p>Visualize the results (create <code>case.foam</code> file to load in ParaView):</p> Bash<pre><code>touch case.foam\nparaview\n</code></pre> </li> <li> <p>Open <code>case.foam</code> in ParaView.</p> </li> </ol>"},{"location":"software/openfoam/#interactive-single-process","title":"Interactive single process","text":"<p>For a non-parallel run of the tutorial case, the <code>decomposeParDict</code> needs to be removed from the <code>system</code> directory:</p> Bash<pre><code>mv system/decomposeParDict system/decomposeParDict-save\n</code></pre> <p>Running the damBreak case step-by-step interactively:</p> Bash<pre><code>module load rocky8-spack\nmodule load openfoam\n\nsrun --partition=common -t 2:10:00 --pty bash \nblockMesh\nsetFields\ninterFoam\n</code></pre>"},{"location":"software/openfoam/#batch-job-non-interactive-parallel-job","title":"Batch-job (non-interactive) parallel job","text":"<p>Alternatively, we can run the job in parallel as a batch job (if you removed/renamed the <code>decomposeParDict</code> before, copy it back by command):</p> Bash<pre><code>cp system/decomposeParDict-save system/decomposeParDict\n</code></pre> <p>The <code>openfoam.slurm</code> script:</p> Bash<pre><code>#!/bin/bash -l\n\n#SBATCH -n 4\n#SBATCH -t 00:10:00  \n#SBATCH -J openfoam-damBreak\n#SBATCH --partition=green-ib\n\n#the following 2 lines are only needed if not done manually in command-line\n#before submitting the job\nmodule load rocky8-spack\nmodule load openfoam\n\nblockMesh\ndecomposePar\nsetFields\nsrun interFoam -parallel\nreconstructPar\n</code></pre>"},{"location":"software/openfoam/#pre-processing-geometry-and-mesh-generation","title":"Pre-processing (geometry and mesh generation)","text":"<p>The geometry and mesh can be either hand-coded using blockMesh or with Gmsh, FreeCAD, or Salome. When using Gmsh, be sure to save the mesh in v2 ASCII format (see separate page on CAD-mesh). This creates a volume mesh.</p> <p>To convert a Gmsh volume <code>.msh</code> file for OpenFOAM, use</p> Bash<pre><code>gmshToFoam meshfile.msh\n</code></pre> <p>Another possibility is to use CAD for a surface mesh and use the snappyHexMesh utility to adapt a blockMesh volume mesh to the surface (see OpenFOAM motorcycle tutorial).</p>"},{"location":"software/openfoam/#visualizing-the-results-post-processing","title":"Visualizing the results (post-processing)","text":"<ol> <li>Login to viz (manual can be found here).</li> <li>Change to the case directory.</li> <li> <p>Create an empty <code>.foam</code> file for the case:</p> Bash<pre><code>touch damBreak.foam\n</code></pre> </li> <li> <p>Then use the regular ParaView:</p> Bash<pre><code>paraview\n</code></pre> </li> <li> <p>Open the <code>.foam</code> file from the menu.</p> </li> </ol>"},{"location":"software/openfoam/#comparison-of-the-execution-time","title":"Comparison of the execution time","text":"<p>It is educational to check the runtime of the code using the <code>time</code> command, e.g. for the single-thread</p> Bash<pre><code>time interFoam\n</code></pre> <p>and for the parallel run (in the <code>openfoam.slurm</code> script)</p> Bash<pre><code>time mpirun -n $SLURM_NTASKS interFoam -parallel\n</code></pre> <p>As the damBreak case is quite small, it is likely that the parallel run is not faster than the sequential, due to the communication overhead.</p> <p>In a test run, the results have been as follows:</p> time type sequential parallel real 0m8.319s 0m39.463s user 0m6.927s 1m1.755s sys 0m0.432s 0m2.922s <p>Lesson to be learned: Parallel computation is only useful for sufficiently large jobs.</p> <p>NOTE: Parallel does not (necessarily) mean faster!!! Parallel execution introduces overhead (starting threads, communication)! For optimal execution time and optimal use of resources, one needs to test and find the sweet spot.</p> <p></p> <p>The division into the areas is a combined decision taking into account \"real\" (wall clock) and \"user\" (summed time of all threads) time (from the <code>time</code> command). \"Wall clock\" (real) time is the time one needs to wait till the job is finished, \"Summed thread time\" (user) is the sum of the times that all individual threads needed, it should be roughly user = num threads x real. For parallel programs, one can expect that \"user\" time of the parallel run is larger than for the sequential, due to communication overhead, if it is smaller, that probably means the individual threads could make better use of the cache.</p> area why explanation sweet spot minimal \"user\" time = minimal heat production, optimal use of resources good range linear speedup for \"real\", with constant or slightly increasing \"user\" OK range slightly less than linear speedup for \"real\", and slightly increasing \"user\" avoid ascending slope in the diagram for \"real\" and \"user\" one actually needs to wait longer compared to the case with fewer cores <p>Recommended in this case would be to request 8 threads <code>-n 8 --ntasks-per-node 8</code> but use <code>mpirun -n 4</code>. OpenFOAM does not seem to benefit from hyperthreading.</p>"},{"location":"software/openfoam/#some-errors-and-how-to-solve-them","title":"Some errors and how to solve them","text":"<ul> <li>\"slurmstepd: error: Detected 1 oom-kill event(s) in \": this is a SLURM out-of-memory error: solve by increasing the memory request <code>--mem=xxGB</code> where xx is something larger than before</li> <li>a \"Bus error\" means the software tries to access non-existing memory, this is actually a SLURM out-of-memory error: solve by increasing the memory request <code>--mem=xxGB</code> where xx is something larger than before</li> <li>infiniband error: wrong partition, the nodelist contains non-infiniband nodes; or wrong openmpi module</li> </ul>"},{"location":"software/orca/","title":"ORCA","text":"<p>Info</p> <p>To run ORCA, users must register individually and agree to the EULA at Orcaforum.</p>"},{"location":"software/orca/#orca-short-introduction","title":"ORCA short introduction","text":"<ol> <li> <p>Make orca.slurm batch script for parallel calculations:</p> Bash<pre><code>#!/bin/bash\n#SBATCH --job-name=ORCA-test\n#SBATCH --mem=48GB\n#SBATCH --nodes=1\n#SBATCH --ntasks=24\n#SBATCH --cpus-per-task=1\n#SBATCH -t 1-00:00:00\n#SBATCH --partition=common\n#SBATCH --no-requeue\n\nmodule load rocky8/all\nmodule load orca/5.0.4\nexport orcadir=/gpfs/mariana/software/green/Orca/orca_5_0_4_openmpi_411/\n\n# Create scratch directory\nSCRATCH=/state/partition1/$SLURM_JOB_ID\nmkdir -p $SCRATCH\ncp $SLURM_SUBMIT_DIR/* $SCRATCH/\ncd $SCRATCH/\n\n# Run calculations \n$orcadir/orca job.inp &gt; $SLURM_SUBMIT_DIR/job.log\n\n# Copy files back to working directory\ncp $SCRATCH/* $SLURM_SUBMIT_DIR\nrm *tmp*\n\n# Clean after yourself\nrm -rf $SCRATCH\n</code></pre> <p>or orca-single-core.slurm batch script for single-core calculations:</p> Bash<pre><code>#!/bin/bash\n#SBATCH --job-name=Job_Name\n#SBATCH --mem=2GB\n#SBATCH --nodes=1   \n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=1\n#SBATCH -t 10:00:00\n#SBATCH --partition=common\n#SBATCH --no-requeue\n\nmodule load rocky8/all\nmodule load orca/5.0.4\nexport orcadir=/gpfs/mariana/software/green/Orca/orca_5_0_4_openmpi_411/\n\n# Create scratch directory\nSCRATCH=/state/partition1/$SLURM_JOB_ID\nmkdir -p $SCRATCH\ncp $SLURM_SUBMIT_DIR/* $SCRATCH/\ncd $SCRATCH/\n\n# Run calculations \n$orcadir/orca job.inp &gt; $SLURM_SUBMIT_DIR/job.log\n\n# Copy files back to working directory\ncp $SCRATCH/* $SLURM_SUBMIT_DIR\nrm *tmp*\n\n# Clean after yourself\nrm -rf $SCRATCH\n</code></pre> </li> <li> <p>Copy job-input file job.inp (for single-core run remove core specification block).</p> </li> <li> <p>Submit the job on base:</p> Bash<pre><code>sbatch orca.slurm\n</code></pre> <p>NB! More cores do not mean faster!!! See benchmarks. NB! For ORCA parallel run, the full path name is needed. Single-core calculations can be performed with just the <code>orca</code> command.</p> </li> <li> <p>Check results using visualization software.</p> </li> </ol>"},{"location":"software/orca/#orca-long-version","title":"ORCA long version","text":""},{"location":"software/orca/#environment","title":"Environment","text":"<p>Currently, only the latest ORCA 5.0.4 version is available. The environment is set up by the commands:</p> Bash<pre><code>module load rocky8/all\nmodule load orca/5.0.4\n</code></pre> <p>For the first-time use, the user has to agree to the licenses:</p> Bash<pre><code>touch ~/.licenses/orca-accepted \n</code></pre> <p>If this is the first user license agreement, the following commands should be given:</p> Bash<pre><code>mkdir .licenses\ntouch ~/.licenses/orca-accepted  \n</code></pre> <p>NB! After agreeing to the license, the user has to log out and log in again to be able to run ORCA.  </p>"},{"location":"software/orca/#running-orca-jobs","title":"Running ORCA jobs","text":"<p>ORCA input files are executed by the command <code>orca</code>. This command is usually placed in the <code>slurm</code> script.</p> <p>NB! For ORCA parallel run, the full path name is needed, but single-core calculations can be performed with just the <code>orca</code> command.</p> Bash<pre><code>/gpfs/mariana/software/green/Orca/orca_5_0_4_openmpi_411/orca job.inp\n</code></pre>"},{"location":"software/orca/#single-core-calculations","title":"Single-core calculations","text":"<p>ORCA by default executes jobs on only a single processor. Example of ORCA input:</p> Text Only<pre><code>! RI BP86 def2-SVP def2/J D4 printbasis Opt \n\n%maxcore 2000   # Use up to 2 GB of memory\n\n*xyz 0 1\nC          0.67650        0.42710        0.00022\nH          0.75477        1.52537        0.00197\nO          1.62208       -0.30498       -0.00037\nS         -1.01309       -0.16870        0.00021\nH         -1.58104        1.05112       -0.00371\n*\n</code></pre> <p>Example of an orca-single-core.slurm batch script for single-core calculations.</p> <p>NB! If in the <code>slurm</code> script more processors are defined, they will be reserved but not utilized.</p>"},{"location":"software/orca/#parallel-jobs","title":"Parallel jobs","text":"<p>To run multiple processors/cores job, a number of cores should be specified both in the ORCA input file and in the <code>slurm</code> script. In ORCA, it is done with the <code>PAL</code> keyword (e.g., PAL4) or as a block input.</p> <p>Example of ORCA input for 4 cores:</p> Text Only<pre><code>! RI BP86 def2-SVP def2/J D4 printbasis Opt \n\n%maxcore 2000   # Use up to 2 GB of memory\n\n%pal nprocs 4 end   # Use 4 cores\n\n*xyz 0 1\nC          0.67650        0.42710        0.00022\nH          0.75477        1.52537        0.00197\nO          1.62208       -0.30498       -0.00037\nS         -1.01309       -0.16870        0.00021\nH         -1.58104        1.05112       -0.00371\n*\n</code></pre> <p>Example of <code>slurm</code> script:</p> Bash<pre><code>#!/bin/bash\n#SBATCH --job-name=ORCA-test        # Job name\n#SBATCH --mem=8GB               # Memory \n#SBATCH --nodes=1           # Number of nodes \n#SBATCH --ntasks=4          # Number of threads \n#SBATCH --cpus-per-task=1\n#SBATCH -t 2:00:00          # Time\n#SBATCH --partition=common      # Partition\n#SBATCH --no-requeue            # Job will not be restarted by default \n\nmodule load rocky8/all\nmodule load orca/5.0.4\nexport orcadir=/gpfs/mariana/software/green/Orca/orca_5_0_4_openmpi_411/\n\n# Create scratch directory\nSCRATCH=/state/partition1/$SLURM_JOB_ID\nmkdir -p $SCRATCH\ncp $SLURM_SUBMIT_DIR/* $SCRATCH/\ncd $SCRATCH/\n\n# Run calculations \n$orcadir/orca job.inp &gt; $SLURM_SUBMIT_DIR/job.log\n\ncp $SCRATCH/* $SLURM_SUBMIT_DIR\n\n# Clean after yourself\nrm -rf $SCRATCH\n</code></pre> <p>NB! For ORCA parallel run, the full path name is needed. </p> <p>More about ORCA input can be found at ORCA Input Library, ORCA tutorials and ORCA forum.</p>"},{"location":"software/orca/#memory","title":"Memory","text":"<p>The default dynamic memory requested by ORCA is frequently too small for successful job termination. If the amount of memory requested is insufficient, the job will be killed and in <code>slurm-JOBID.out</code> will appear \"... have been killed by the cgroup out-of-memory handler\".</p> <p>Memory usage in ORCA is controlled by the <code>%maxcore</code> keyword.</p> Text Only<pre><code>%maxcore 2000\n</code></pre> <p>There is no golden rule for memory requests, since they are basis set and calculation type dependent. Usually, 2-8 GB per 1 CPU (thread) is sufficient. If the Resolution of the identity (RI) approximation is used, the memory must be increased.  </p> <p>Data from a <code>slurm-JOBID.stat</code> file can be useful to determine the amount of memory required for a computation. In the <code>slurm-JOBID.stat</code> file, the efficiency of memory utilization is shown.</p> <p>Bad example:</p> Text Only<pre><code>Memory Utilized: 3.08 GB \nMemory Efficiency: 11.83% of 26.00 GB\n</code></pre> <p>Good example:</p> Text Only<pre><code>Memory Utilized: 63.12 GB \nMemory Efficiency: 98.62% of 64.00 GB\n</code></pre>"},{"location":"software/orca/#time","title":"Time","text":"<p>Time limits depend on the time partition used, see taltech user-guides. Therefore, it is recommended to request more time than is usually needed for calculation.</p> <p>If the job was killed due to the time limit, this will be written at the end of the <code>slurm-JOBID.out</code> file \"error: JOB 317255 ON green23 CANCELLED AT 2023-08-11T22:28:01 DUE TO TIME LIMIT\"</p> <p>In this case, some files including the checkpoint file <code>gbw</code> will not be copied back to the working directory. To copy files, the user needs to run an interactive session to connect to the node where calculations were done. The node number is written in both <code>slurm-JOBID.stat</code> and <code>slurm-JOBID.out</code>. In the example error message, it was green23.</p> <p>An interactive session is started by the command:</p> Bash<pre><code>srun -w greenXX --pty bash  # connect to green node\npwd             # see path to working directory\nls              # see JOBID from slurm\ncd /state/partition1/JOBID  # go to corresponding directory on green node\ncp job.gbw /gpfs/mariana/home/....../ # copy files needed to your working directory\n\nexit                # terminate interactive session\n</code></pre> <p>where <code>XX</code> - is the node number and <code>JOBID</code> - job serial number.</p> <p></p>"},{"location":"software/orca/#restarting-a-failedinterrupted-calculation","title":"Restarting a failed/interrupted calculation","text":"<p>All ORCA jobs are restart jobs by default.</p> <p>SCF calculations with input file name <code>job.inp</code> will automatically search for a file named <code>job.gbw</code> and will attempt to read in the old orbitals and continue the SCF from there.</p> <p><code>MOREAD</code> and <code>%moinp</code> keywords allow manually specifying where to read the orbitals from.</p> Text Only<pre><code>! MORead\n%moinp \"job2.gbw\" \n# Note that if job2.gbw is the gbw file you read in then job2.inp can not be the name of the input file. \n*xyz 0 1\n</code></pre> <p>Geometry optimization is recommended to be restarted using the last geometry (<code>job.xyz</code>).</p> <p>Numerical frequency calculations also can be restarted if <code>.hess</code> files from the previous calculation are presented.</p> Text Only<pre><code>!\n%freq \nrestart true\nend\n</code></pre> <p>NB! Checkpoint files are very heavy and after successful completion of the calculation, it is recommended to delete these files.</p>"},{"location":"software/orca/#copying-files","title":"Copying files","text":"<p>During calculations, ORCA creates many different additional files. By default, <code>slurm</code> copies all files to the user's directory. However, the user can choose which files to copy back to the working directory.</p> Bash<pre><code>cp $SCRATCH/*.gbw  $SLURM_SUBMIT_DIR/\ncp $SCRATCH/*.engrad  $SLURM_SUBMIT_DIR/\ncp $SCRATCH/*.xyz  $SLURM_SUBMIT_DIR/\ncp $tdir/*.log  $SLURM_SUBMIT_DIR/\ncp $tdir/*.hess  $SLURM_SUBMIT_DIR/\n</code></pre>"},{"location":"software/orca/#how-to-cite","title":"How to cite","text":"<ul> <li>Neese, F. (**2012) The ORCA program system, Wiley Interdiscip. Rev.: Comput. Mol. Sci., 2, 73-78.</li> <li>Neese, F. (2017) Software update: the ORCA program system, version 4.0, Wiley Interdiscip. Rev.: Comput. Mol. Sci., 8, e1327.</li> <li>Neese, F. (2022) Software update: The ORCA program system\u2014Version 5.0, WIREs Computational Molecular Science, 12, e1606.</li> </ul>"},{"location":"software/orca/#additional-information","title":"Additional Information","text":"<ul> <li>Official ORCA website</li> <li>ORCA Input Library</li> <li>ORCA compound scripts repository</li> <li>ORCA Tutorials by FAccTs</li> </ul>"},{"location":"software/software/","title":"Software packages","text":"<p>Software on our systems is installed in the following ways:</p> <ol> <li>as packages from the Linux distribution (free open-source software when available and recent enough) -- no modules needed</li> <li> <p>through the SPACK package manager (free open-source software when available in SPACK) - load the <code>*-spack</code> modules by command:</p> Bash<pre><code>module load rocky8-spack\n</code></pre> </li> <li> <p>manually (mostly non-free software (not GPL/BSD license)) - load the <code>*/all</code> modules by command:</p> Bash<pre><code>module load rocky8/all\n</code></pre> </li> </ol> <p>Here is a list of important software for special purpose:</p> <ul> <li>CAD &amp; Meshing software - FreeCAD, Salome, BRL-CAD, Gmsh and Netgen; see CAD-Mesh</li> <li>Finite element software for multiphysical problems - ElmerFEM, CalculiX, Abaqus</li> <li>Computational Fluid Dynamics -- OpenFOAM, SU2</li> <li>Conformational search  - xtb-CREST</li> <li>General purpose computational chemistry, biology and physics software packages - Gaussian, ORCA, NWChem, TURBOMOLE, xTB, CP2K</li> <li>Wavefunction analysis - Multiwfn</li> <li>Visualization software for computational chemistry, biology and physics - Molden, Avogadro, JMol, VMD, RasMol</li> <li>Interactive and non-interactive Jupyter notebooks for Julia, Python, Octave</li> <li>MATLAB-compatible computation environment - Octave</li> <li>Data analysis - R, MATLAB, Octave, Julia, awk, Python, GNUplot</li> <li>Visualization software - MayaVi2, ParaView, VisIt, COVISE, OpenDX, GNUplot</li> </ul> <p>A more detailed description of available software, as well as a division by area of use, is given below.</p> <p>If software you want to use is missing in the list above, it means that it is not installed, but can be installed by your request to hpcsupport@taltech.ee or create a ticket in Helpdesk Portal. In the case of licensed software, the user must provide the license himself and the corresponding program will be installed.</p>"},{"location":"software/software/#engineering","title":"Engineering","text":""},{"location":"software/software/#cad-mesh-tools","title":"CAD &amp; Mesh-Tools","text":"<p>Computer-aided design (CAD) is software for building models in a virtual space, that allows to visualize various properties of an object, such as height, width, distance, material, etc. This category contains software that is essential for the pre-processing of many simulations: CAD and mesh generation. More about CAD and meshing options on our HPC can be found here.</p>"},{"location":"software/software/#freecad","title":"FreeCAD","text":"<p>FreeCAD is a CAD software, which uses Gmsh or Netgen for meshing. It can also serve as a frontend for CalculiX and ElmerFEM, thus providing similar functionality as SolidWorks. More about FreeCAD on our HPC can be found here.</p>"},{"location":"software/software/#salome","title":"Salome","text":"<p>Salome is a CAD program with interfaces to meshing software. It can be used by a GUI or python scripts. More about Salome on our HPC can be found here.</p>"},{"location":"software/software/#brl-cad","title":"BRL-CAD","text":"<p>BRL-CAD is a CAD software that has been in development since 1979 and is open-source since 2004. It is based on CSG modeling. BRL-CAD does not provide volume meshing, however, the CSG geometry can be exported to BREP (boundary representation, like STL, OBJ, STEP, IGES, PLY), the <code>g-*</code> tools are for this, while the <code>*-g</code> tools are for importing. More about BRL-CAD on our HPC can be found here.</p>"},{"location":"software/software/#blender","title":"Blender","text":"<p>Blender is a 3D modeling software developed for computer animation (movie production).</p>"},{"location":"software/software/#gmsh","title":"Gmsh","text":"<p>Gmsh is an open source 3D finite element mesh generator with a built-in CAD engine and post-processor. More about Gmsh on our HPC can be found here.</p>"},{"location":"software/software/#netgen","title":"Netgen","text":"<p>Netgen is a part of the NGsolve suite. Netgen is an automatic 3d tetrahedral mesh generator containing modules for mesh optimization and hierarchical mesh refinement. More about Netgen on our HPC can be found here.</p>"},{"location":"software/software/#finite-element-analysis-fea","title":"Finite Element Analysis (FEA)","text":"<p>The Finite Element Method (FEM) is a general numerical method for solving partial differential equations in two or three space variables performed by dividing a large system into smaller parts (finite elements). The method is used for numerically solving differential equations in engineering and mathematical modeling.</p> <p>See also under computational-fluid-dynamics-CFD.</p>"},{"location":"software/software/#elmerfem","title":"ElmerFEM","text":"<p>Elmer is a multi-physics simulation software developed by CSC. It can perform coupled mechanical, thermal, fluid, electro-magnetic simulations and can be extended by own equations. Elmer manuals and tutorials can be found here and for more details and example job scripts go here.</p>"},{"location":"software/software/#calculix","title":"CalculiX","text":"<p>CalculiX is a finite-element analysis application. The two programs that form CalculiX are <code>cgx</code> and <code>ccx</code>, where <code>cgx</code> is a graphical frontend (pre- and post-processing) and <code>ccx</code> is the solver doing the actual numerics. CalculiX can be used for grid data generation or mech data generation. It can be applied in such areas as mechanical analysis, heat transfer, electromagnetic calculations, computational fluid dynamics, etc. For more details see overview of the finite element capabilities of CalculiX Version 2.18. Solver makes use of the Abaqus input format.</p>"},{"location":"software/software/#freefem","title":"FreeFEM","text":"<p>FreeFEM is a software focused on solving partial differential equations using the finite element method. Can be used for linear, non-linear elasticity, thermal diffusion/convection/radiation, magnetostatics, electrostatics, CFD, fluid structure interaction; continuous and discontinuous Galerkin method. Allows to implement own physics modules using the FreeFEM language.</p>"},{"location":"software/software/#dealii","title":"deal.II","text":"<p>deal.II - an open source finite element library</p>"},{"location":"software/software/#mfem","title":"MFEM","text":"<p>MFEM is a free, lightweight, scalable C++ library for finite element methods.</p>"},{"location":"software/software/#abaqus","title":"Abaqus","text":"<p>Abaqus is a commercial software suite for finite element analysis and computer-aided engineering. The Abaqus products use Python for scripting and customization. User modules can be written in Fortran or C/C++, our installation is configured to use gcc-10.3.0. Abaqus versions 2018 and 2021 are installed. The number of concurrent processes is limited and managed by flexlm.</p> Bash<pre><code>module load rocky8\nmodule load abaqus\n</code></pre> <p>To use Abaqus' PlatformMPI, SLURM's Global Task ID needs to be cleared</p> Bash<pre><code>unset SLURM_GTIDS\n</code></pre>"},{"location":"software/software/#comsol","title":"Comsol","text":"<p>Commercial multi-physics finite element simulation software. License belongs to a research group.</p>"},{"location":"software/software/#lsdyna","title":"LSDyna","text":"<p>License belongs to a research group.</p> Bash<pre><code>module load rocky8\nmodule load LSDyna/SMP-13.0.0-D\n</code></pre>"},{"location":"software/software/#star-ccm","title":"Star-CCM+","text":"<p>Commercial software. System wide installation, bring your own license. Star-CCM+ can be used with PowerOnDemand (PoD) keys. Power on demand (PoD) licensing for STAR-CCM+ is essentially cloud licensing. To use PoD licensing, a PoD key must be copied from the Star-CCM+ support center and put into the STAR-CCM+ interface.</p> <p>More information about licenses:</p> <ul> <li>Simcenter STAR-CCM+ demand</li> <li>How faculty members in academic institutions can get access to Simcenter STAR-CCM+</li> <li>Guide for students to run Simcenter STAR-CCM+</li> </ul> Bash<pre><code>module load rocky8\nmodule load star-ccm+/18.04.009-R8\nstarccm+\n</code></pre>"},{"location":"software/software/#computational-fluid-dynamics-cfd","title":"Computational Fluid Dynamics (CFD)","text":"<p>See also under FEA.</p>"},{"location":"software/software/#openfoam","title":"OpenFOAM","text":"<p>OpenFOAM is an open source software for computational fluid dynamics (CFD). OpenFOAM has a wide range of tools for modelling complex fluid flows and can be used for solving such problems as chemical reactions, turbulence and heat transfer, to acoustics, solid mechanics and electromagnetics. More about OpenFOAM on our HPC can be found here.</p>"},{"location":"software/software/#su2","title":"SU2","text":"<p>SU2 is a computational analysis and design package that has been developed to solve multiphysics analysis and optimization tasks using unstructured mesh topologies. SU2 is installed through SPACK.</p>"},{"location":"software/software/#water-wave-modelling","title":"Water Wave Modelling","text":""},{"location":"software/software/#wam","title":"WAM","text":"<p>WAM is a third generation wave model, developed and maintained by GKSS. It describes the evolution of the wave spectrum by solving the wave energy transfer equation. WAM predicts directional spectra and wave properties (such as wave height, direction and frequency, swell height and mean direction), wind stress fields etc. WAM can be coupled to a range of other models (NEMO, RegCM, SEAOM, etc.). More info how to use it on HPC see here.</p>"},{"location":"software/software/#swan","title":"Swan","text":"<p>SWAN is a third generation wave model for obtaining realistic estimates of wave parameters in coastal areas, lakes and estuaries from given wind, bottom and current conditions, developed and maintained by TU Delft. The model is based on the wave action balance equation with sources and sinks. SWAN allows to use two types of grids (structured and unstructured) and nesting approach. More info how to use it on HPC see here.</p>"},{"location":"software/software/#chemistry-biology-and-physics","title":"Chemistry, biology and physics","text":""},{"location":"software/software/#conformational-search-sampling","title":"Conformational search &amp; sampling","text":""},{"location":"software/software/#xtb-crest","title":"xtb-CREST","text":"<p>Conformer\u2013Rotamer Ensemble Sampling Tool (xtb-CREST) is designed as conformer sampling program by Grimme's group. CREST uses meta-dynamics, regular MD simulations and Genetic Z-matrix crossing (GC) algorithms with molecular mechanics or semiempirical methods (GFNn-xTB). Conformational search can be done in gas or solvent (using several continuum models). More about xtb-CREST on our HPC can be found here.</p>"},{"location":"software/software/#general-purpose-computational-chemistry-biology-and-physics","title":"General purpose computational chemistry, biology and physics","text":""},{"location":"software/software/#gaussian","title":"Gaussian","text":"<p>Gaussian is a general purpose package for calculation of electronic structures. It can calculate properties of molecules (structures, energies, spectroscopic and thermochemical properties, atomic charges, electron affinities and ionization potentials, electrostatic potentials and electron densities etc.) and reactions properties (such as reaction pathways, IRC) using different methods (such as Molecular mechanics, Semi-empirical methods, Hartree-Fock, Density functional, M\u00f8ller-Plesset perturbation theory, coupled cluster). More about Gaussian on HPC can be found here.</p>"},{"location":"software/software/#orca","title":"ORCA","text":"<p>ORCA is a multi-purpose quantum-chemical software package developed in the research group of Frank Neese. ORCA includes a wide variety of methods (semi-empirical, density functional theory, many-body perturbation, coupled cluster, multireference, nudged elastic band (NEB) methods). In ORCA, molecules' and spectroscopic properties calculations are available, and environmental (MD (including ab initio), QM/MM, Crystal-QMMM) as well as relativistic effects can be taken into account. ORCA is parallelized, and uses the resolution of the identity (RI) approximation and domain based local pair natural orbital (DLPNO) methods, which significantly speed calculations. More about ORCA on HPC can be found here</p>"},{"location":"software/software/#xtb","title":"xtb","text":"<p>Extended tight binding - xTB program developed in the research group of Stefan Grimme for solutions of common chemical problems. The workhorses of xTB are the GFN methods, both semi-empirical and force-field. The program contains several implicit solvent models: GBSA, ALPB, ddCOSMO, and CPCM-X. xTB functionality covers single-point energy calculations, geometry optimization, frequency calculations, reaction path methods. Also allows to perform molecular dynamics, meta-dynamics, and ONIOM calculations. More about xTB on HPC can be found here</p>"},{"location":"software/software/#nwchem","title":"NWChem","text":"<p>The North West computational chemistry (NWChem) is an ab initio computational chemistry software package which includes quantum chemical (HF, DFT, MP2, MCSCF, and CC, including the tensor contraction engine (TCE)) and molecular dynamics (using either force fields (AMBER or CHARMM) or DFT) functionality. In NWChem, ab initio methods can be coupled with the classical MD to perform mixed quantum-mechanics and molecular-mechanics simulations (QM/MM). Various molecular response properties, solvent models, nudged elastic band (NEB) method, relativistic and resolution of the identity (RI) approaches are also available.</p> <p>NWChem was developed to enable large scale calculations by using many CPUs and has parallel scalability and performance. Additionally, python programs may be embedded into the NWChem input and used to control the execution of NWChem.</p>"},{"location":"software/software/#wavefunction-analysis","title":"Wavefunction analysis","text":""},{"location":"software/software/#multiwfn","title":"Multiwfn","text":"<p>Multiwfn it is an interactive program that performs almost all important wavefunction analyzes. In addition, Multiwfn is able to display plots of the predicted spectra. More about Multiwfn on HPC can be found here.</p>"},{"location":"software/software/#visualization-software-for-computational-chemistry-biology-and-physics","title":"Visualization software for computational chemistry, biology and physics","text":"<ul> <li>Molden</li> <li>Avogadro</li> <li>JMol</li> <li>VMD</li> <li>RasMol</li> </ul>"},{"location":"software/software/#data-analysis","title":"Data analysis","text":""},{"location":"software/software/#gnu-r","title":"GNU R","text":"<p>GNU R, often called \"GNU S\" is an open-source implementation of the S statistics language. R offers many build-in features for data analysis, has a large collection of well maintained packages in CRAN (the Comprehensive R Archive Network) and most importantly produces high-quality graphics. While the plots may not look fancy at first sight, they are well layed out with font sizes and they are vector graphics. Another feature is that R integrates well with LaTeX2e documents using Sweave (comes with R) or knitr. That makes it possible to write the data analysis using R code within LaTeX2e documents and have R create figures and tables automatically.</p> <p>CRAN Packages can be installed by the users themselves from inside R</p> Bash<pre><code>install.packages(\"packagename\",lib=paste(Sys.getenv(\"HOME\"),\"/.R/library\",sep=\"\"),repos=\"http://cran.r-project.org\")\n</code></pre> <p>The package will be placed inside the user's $HOME directory (installation into system directories will not be allowed).</p>"},{"location":"software/software/#julia","title":"Julia","text":"<p>Julia is an easy to learn and high-performance interactive language. Julia is as easy (or easier) to learn as Python, but with the speed of C or Fortran for numerics.</p>"},{"location":"software/software/#octavematlab","title":"Octave/Matlab","text":"<p>Octave is software featuring a high-level programming language, intended for prototyping numerical computations. Octave solves linear and nonlinear problems, and for performing other numerical experiments using a language that is mostly compatible with MATLAB. It may also be used as a batch-oriented language. More information about Matlab and Octave on HPC can be found here.</p>"},{"location":"software/software/#gnuplot","title":"GNUplot","text":"<p>GNUplot is a very capable and portable command-line driven graphing utility for Linux and other operating systems.</p>"},{"location":"software/software/#python","title":"Python","text":"<p>Different versions are available as spack modules. Packages for Python can be installed by the users themselves using pip (python2) or pip3 (python3)</p> Bash<pre><code>pip install --user packagename\n</code></pre> <p>or</p> Bash<pre><code>pip3 install --user packagename\n</code></pre> <p>The option <code>--user</code> will install the package into the user's $HOME directory (installation into system directories will not be allowed).</p>"},{"location":"software/software/#jupyterlab","title":"JupyterLab","text":"<p>JupyterLab notebook is an open-source web application that allows creation and sharing documents containing live code, equations, visualizations, and text. Jupyter notebooks allow data transformation, numerical simulation, statistical modeling, data visualization, machine learning, etc. using Julia, Python and Octave. More about Jupyter on our HPC is here.</p>"},{"location":"software/software/#visualization-software","title":"Visualization software","text":"<ul> <li>ParaView (all nodes spack module paraview)</li> <li>VisIt (all nodes spack module visit)</li> <li>COVISE (viz: run <code>/usr/local/covise/bin/covise</code>)</li> <li>MayaVi (all nodes: spack module py-mayavi)</li> <li>GNUplot  (all nodes spack module gnuplot)</li> <li>OpenDX (currently not available will come soon)</li> <li>Software for computational chemistry:</li> <li>Molden</li> <li>Avogadro</li> <li>JMol</li> <li>VMD (all nodes: spack module vmd)</li> <li>RasMol</li> <li>Software for movie animation</li> <li>Blender (all nodes module blender)</li> </ul> <p>The recommended way now is to use a desktop session in OnDemand, see also the visualization page on how to start these and on GPU acceleration.</p>"},{"location":"software/spack/","title":"SPACK","text":"<p>Warning</p> <p>This page has not been completely updated for Rocky 8 yet! This page is a work in progress!</p> <p>SPACK is a package manager used to install software packages. An advantage is the ability to relatively easily install consistent dependencies and multiple versions of software. The following link contains a list of software that should be easy to install: SPACK package list</p>"},{"location":"software/spack/#modules","title":"Modules","text":"<p>SPACK makes use of the module system. To enable SPACK build modules, use:</p> <p>For gray and all nodes (optimized for Xeon E5-2630L Haswell CPUs, will also run on green nodes but will not use all hardware features):</p> Text Only<pre><code>module load gray-spack\n</code></pre> <p>Optimized for green nodes (optimized for Xeon Gold 6148 Skylake CPUs, may not run on gray nodes):</p> Text Only<pre><code>module load green-spack\n</code></pre> <p>Optimized for amp (AMD EPYC 7742 Zen2), available only on amp:</p> Text Only<pre><code>module load amp-spack\n</code></pre> <p>By default, SPACK builds are optimized for the CPU the software is built on. The packages from the gray nodes will work on the green nodes but slower than the optimized modules. Conversely, the Skylake-optimized modules will try to use hardware features not present on the gray nodes, so the software will not run there.</p> <p>You can auto-activate the correct module path in your <code>.bashrc</code> with a code block like this:</p> Bash<pre><code>if [[ $(hostname -s) = base ]]; then\n  module load green-spack\nelif [[ $(hostname -s) = green* ]]; then\n  module load green-spack\nelif  [[ $(hostname -s) = gray* ]]; then\n  module load gray-spack\nelif  [[ $(hostname -s) = amp ]]; then\n  module load amp-spack\nfi\n</code></pre>"},{"location":"software/spack/#user-built-software","title":"User-built software","text":"<p>SPACK can also be used by users to manage their own software stack inside their home directory (be aware, this takes a lot of space!).</p> <p>See documentation on https://spack.readthedocs.io/en/latest/</p> <p>A similar tool is EasyBuild. They support different lists of software packages. SPACK includes GPU-offloading compilers for both Nvidia and AMD, profiling tools (Tau, HPCToolkit), and engineering simulation packages (ElmerFEM, OpenFOAM), while EasyBuild seems to be more AI and Python-oriented https://docs.easybuild.io/en/latest/version-specific/Supported_software.html.</p> <p>SPACK is used by the University of Tartu, LRZ, and HLRS, while EasyBuild will be used on LUMI.</p>"},{"location":"software/swan/","title":"SWAN","text":"<p>Warning</p> <p>This page has not been completely updated for Rocky 8 yet!</p> <p>SWAN is a third-generation wave model for obtaining realistic estimates of wave parameters in coastal areas, lakes, and estuaries from given wind, bottom, and current conditions. The model is based on the wave action balance equation with sources and sinks. SWAN allows the use of two types of grids (structured and unstructured) and a nesting approach. SWAN is not recommended for use on ocean scales.</p> <p>The manual can be found here, and more about SWAN settings can be found here.</p>"},{"location":"software/swan/#quickstart","title":"Quickstart","text":"<ol> <li> <p>Load the module</p> Bash<pre><code>module load green/all\nmodule load Swan\n</code></pre> </li> <li> <p>Copy the examples</p> Bash<pre><code>cp -r $SWANDIR/../Examples Swan-examples\n</code></pre> </li> <li> <p>Unpack one example</p> Bash<pre><code>cd Swan-examples\ntar xf refrac.tar.gz\ncd refrac\n</code></pre> </li> <li> <p>Download the swan.slurm script.</p> </li> <li> <p>Adjust <code>ntasks</code>, <code>nodes</code>, and <code>cpus-per-task</code>, and submit it:</p> Bash<pre><code>sbatch swan.slurm\n</code></pre> <p>Contents of the <code>swan.slurm</code>:</p> Bash<pre><code>#!/bin/bash\n#SBATCH --partition=green-ib  ### Partition\n#SBATCH --job-name=Swan   ### Job Name           -J\n#SBATCH --time=00:10:00       ### WallTime           -t\n#SBATCH --nodes=1             ### Number of Nodes    -N \n#SBATCH --ntasks-per-node=2   ### Number of tasks (MPI processes)\n#SBATCH --cpus-per-task=4     ### Number of threads per task (OMP threads)\n#SBATCH --mem-per-cpu=100     ### Min RAM required in MB\n\n# Set up environment    \nmodule load green/all\nmodule load Swan\n\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK        \nMACHINEFILE=\"machinefile\"\n\n# Generate Machinefile for MPI such that hosts are in the same order as if run via srun\nsrun -l /bin/hostname | sort -n | awk '{print $2}' &gt; $MACHINEFILE\n\n# Do the SWAN simulation\nswanrun -input a11refr.swn -mpi $SLURM_NTASKS -omp=$SLURM_CPUS_PER_TASK\n\n# rm $MACHINEFILE\n</code></pre> </li> </ol>"},{"location":"software/turbomole/","title":"TURBOMOLE","text":"<p>Warning</p> <p>This page has not been completely updated for Rocky 8 yet! This page is a work in progress!</p> <p>Info</p> <p>To run TURBOMOLE, user must be a member of the TURBOMOLE user group or have purchased TURBOMOLE licenses.</p>"},{"location":"software/turbomole/#turbomole-short-introduction","title":"TURBOMOLE short introduction","text":"<ol> <li> <p>Set up the environment and generate the TURBOMOLE coordinate file:</p> Bash<pre><code>module load turbomole7.0\nx2t inputfile.xyz &gt; start-coord\n</code></pre> </li> <li> <p>Run define to create the input files needed.</p> </li> <li> <p>Use the TURBOMOLE batch script:</p> Bash<pre><code>#!/bin/bash\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=1\n#SBATCH --job-name=test\n#SBATCH --mem-per-cpu=1GB\n#SBATCH -t 1:00:00\n#SBATCH --partition=common \n\nmodule load turbomole7.0\n\n# Directory where you run the script\nCALCULATIONDIR=`pwd`\n\n# Create scratch directory\nSCRATCHDIRNAME=/state/partition1/$SLURM_JOBID\nmkdir $SCRATCHDIRNAME\ncp * $SCRATCHDIRNAME\ncd $SCRATCHDIRNAME \n\n# Run calculations \njobex -ri &gt; jobex.out 2&gt;jobex.err  # TURBOMOLE commands\nt2x -c &gt; final.xyz\n\n# Clean after yourself\nmv $SCRATCHDIRNAME/* $CALCULATIONDIR \nrm -rf $SCRATCHDIRNAME\n</code></pre> </li> <li> <p>Submit the job:</p> Bash<pre><code>sbatch TURBOMOLE.slurm\n</code></pre> </li> </ol> <p>NB! More cores do not mean faster!!! See Benchmarks.</p>"},{"location":"software/turbomole/#turbomole-long-version","title":"TURBOMOLE long version","text":""},{"location":"software/turbomole/#environment","title":"Environment","text":"<p>There are currently several versions of TURBOMOLE (6.3 - 7.0) available on HPC, and most of them can be run as parallel jobs. The environment is set up by the command:</p> Bash<pre><code>module load turbomole7.0\nmodule load turbomole7.0-mpi    # for parallel run \n</code></pre>"},{"location":"software/turbomole/#running-turbomole-jobs","title":"Running TURBOMOLE jobs","text":"<p>TURBOMOLE uses its own coordinate file <code>coord</code>, which can be generated from a .xyz file by the TURBOMOLE command (when some TURBOMOLE version is already loaded):</p> Bash<pre><code>x2t inputfile.xyz &gt; start-coord\n</code></pre> <p>Example of TURBOMOLE coordinate file:</p> Text Only<pre><code>$coord\n1.27839972889714      0.80710203135546      0.00041573974923       c\n1.42630859331810      2.88253155131977      0.00372276048178       h\n3.06528696563114     -0.57632867600746     -0.00069919866917       o\n-1.91446264796512     -0.31879679861781      0.00039684248791       s\n-2.98773260513752      1.98632893279876     -0.00701088395301       h\n$end\n</code></pre> <p>In addition to the coordinate file, TURBOMOLE uses a special interactive program <code>define</code> to create the input files, which determine molecules' parameters, levels of theory used, and calculation types.  </p> Bash<pre><code>define\n</code></pre> <p>The answers to <code>define</code>'s questions can be presented as a separate file. More about <code>define</code> can be read in \u2018Quick and Dirty\u2019 Tutorial and TURBOMOLE tutorial. Some examples of define files can be found here.</p> <p>To include solvent effects into calculations, the interactive program <code>cosmoprep</code> should be run after <code>define</code>.</p> Bash<pre><code>cosmoprep \n</code></pre> <p>TURBOMOLE includes the Conductor-like Screening Model (COSMO), where the solvent is described as a dielectric continuum with permittivity \u03b5.</p> <p>After input files are created, TURBOMOLE calculations are executed by one of the following commands: <code>dscf</code>, <code>ridft</code>, <code>jobex</code>, <code>aoforce</code>, <code>NumForce</code>, <code>escf</code>, <code>egrad</code>, <code>mpshift</code>, <code>raman</code>, <code>ricc2</code>, etc. For example:</p> Bash<pre><code>dscf           # for Hartree-Fock energy calculation (single point calculation)\njobex -ri      # geometry optimization using RI-approximation\naoforce        # analytical force constant calculations\nNumForce -ri   # numerical force constant calculations using RI-approximation\n</code></pre> <p>More about TURBOMOLE commands used can be found in TURBOMOLE tutorial.</p>"},{"location":"software/turbomole/#single-core-calculations","title":"Single core calculations","text":"<p>Example of a Slurm script for single point HF calculation performed on a single core:</p> Bash<pre><code>#!/bin/bash\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=1\n#SBATCH --job-name=test\n#SBATCH --mem-per-cpu=1GB\n#SBATCH -t 1:00:00\n#SBATCH --partition=common \n\nmodule load turbomole7.0\n\n# Directory where you run the script\nCALCULATIONDIR=`pwd`\n\n# Create scratch directory\nSCRATCHDIRNAME=/state/partition1/$SLURM_JOBID\nmkdir $SCRATCHDIRNAME\n\ncp * $SCRATCHDIRNAME\ncd $SCRATCHDIRNAME \n\n# Run calculations \ndscf &gt; JOB.out 2&gt;JOB.err  \n\n# Clean after yourself\nmv $SCRATCHDIRNAME/* $CALCULATIONDIR \nrm -rf $SCRATCHDIRNAME\n</code></pre>"},{"location":"software/turbomole/#parallel-jobs-smp","title":"Parallel jobs SMP","text":"<p>Example of a Slurm script for geometry optimization using RI-approximation performed by SMP run:</p> Bash<pre><code>#!/bin/bash\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=24\n#SBATCH --job-name=test\n#SBATCH --mem-per-cpu=1GB\n#SBATCH -t 1:00:00\n#SBATCH --partition=common \n\nmodule load turbomole7.0-mpi\n\n# Directory where you run the script\nCALCULATIONDIR=`pwd`\n\n# Create scratch directory\nSCRATCH=/state/partition1/$SLURM_JOBID\nmkdir $SCRATCH\n\ncp * $SCRATCH\ncd $SCRATCH \n\nexport PARA_ARCH=SMP\nexport PATH=$TURBODIR/bin/`sysname`:$PATH \nexport PARNODES=$SLURM_NTASKS \nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n\n# Run calculations \njobex -ri -c 600 &gt; jobex.out 2&gt;jobex.err \nt2x -c &gt; final.xyz\n\n# Clean after yourself\nmv $SCRATCH/* $CALCULATIONDIR \nrm -rf $SCRATCH\n</code></pre>"},{"location":"software/turbomole/#memory","title":"Memory","text":"<p>For common calculations (e.g. optimization, frequency, etc.) it is enough 1 GB per 1 CPU. However, some calculations can require more memory (e.g. TD-DFT, large SCF calculations, etc.). Data from a <code>slurm-JOBID.stat</code> file can be useful to determine the amount of memory required for a computation. In the <code>slurm-JOBID.stat</code> file, the efficiency of memory utilization is shown.</p> <p>Bad example:</p> Text Only<pre><code>Memory Utilized: 3.08 GB \nMemory Efficiency: 11.83% of 26.00 GB\n</code></pre> <p>Good example:</p> Text Only<pre><code>Memory Utilized: 63.12 GB \nMemory Efficiency: 98.62% of 64.00 GB\n</code></pre>"},{"location":"software/turbomole/#time","title":"Time","text":"<p>Time limits depend on the time partition used taltech user-guides. If the calculation time exceeds the time limit requested in the Slurm script, then the job will be killed. Therefore, it is recommended to request a little more than is usually needed for the calculation.</p>"},{"location":"software/turbomole/#restarting-a-failedinterrupted-calculation","title":"Restarting a failed/interrupted calculation","text":"<p>All TURBOMOLE jobs are restart jobs by default.</p>"},{"location":"software/wam/","title":"WAM Cycle 6","text":"<p>Warning</p> <p>This page has not been completely updated for Rocky 8 yet!</p> <p>WAM is a third-generation wave model that describes the evolution of the wave spectrum by solving the wave energy transfer equation. WAM predicts wave direction spectra and properties and can be linked to a number of other models.</p> <p>A repository of the source code with modifications for Taltech HPC can be found here.</p>"},{"location":"software/wam/#how-to-cite","title":"How to cite","text":"<p>The WAM Model\u2014A Third Generation Ocean Wave Prediction Model DOI: https://doi.org/10.1175/1520-0485(1988)018&lt;1775:TWMTGO&gt;2.0.CO;2</p>"},{"location":"software/wam/#quickstart","title":"Quickstart","text":""},{"location":"software/wam/#short-jobs-one-core-jobs","title":"Short jobs &amp; one core jobs","text":"<ol> <li> <p>To run your first calculations, start a session on a node:</p> Bash<pre><code>srun -t 2:0:0 --pty bash\n</code></pre> </li> <li> <p>Enter the following commands to set up the environment and working directory:</p> Bash<pre><code>module load green/all\nmodule load WAM\n\nexport WORK=$HOME/newwamtest\n\nmkdir --parent ${WORK}/tempsg\n\ncd ${WORK}/tempsg\ncp ${WAMDIR}/const/TOPOCAT.DAT .\ncp ${WAMDIR}/const/Coarse_Grid/ARD/Preproc_User .\npreproc\n\ncp ${WAMDIR}/const/Coarse_Grid/ARD/WAM_User .\ncp ${WAMDIR}/const/WIND_INPUT.DAT .\n</code></pre> </li> <li> <p>Run WAM:</p> Bash<pre><code>mpirun wam\n\ncp ${WAMDIR}/const/Coarse_Grid/ARD/Grid_User .\npgrid\n\ncp ${WAMDIR}/const/Coarse_Grid/ARD/Time_User .\nptime\n\ncp ${WAMDIR}/const/Coarse_Grid/ARD/Spectra_User .\npspec\n\ncp ${WAMDIR}/const/Coarse_Grid/ARD/Time_User_S .\nptime_S\n\ncp ${WAMDIR}/const/Coarse_Grid/ARD/nlnetcdf NETCDF_User\nmpirun pnetcdf\n</code></pre> </li> <li> <p>Adapt the WORKDIR, LOGDIR, and output directories to your needs!</p> </li> <li> <p>If calculations are going normally, you should have the following files in your <code>$WORK</code> directory:</p> Text Only<pre><code>BLS19780907060000  Grid_Prot              OUT19780907060000  Time_Prot_S\nBLS19780908060000  Grid_User              OUT19780908060000  Time_User\nC0119780906060000  Grid_info_COARSE_GRID  Preproc_Prot       Time_User_S\nC0119780907060000  MAP19780906060000      Preproc_User       WAM_Prot\nC0119780908060000  MAP19780907060000      Spectra_Prot       WAM_User\nC0219780906060000  MAP19780908060000      Spectra_User       WAVE1978090606.nc\nC0219780907060000  NETCDF_User            TOPOCAT.DAT        WIND_INPUT.DAT\nC0219780908060000  OUT19780906060000      Time_Prot          pnetcdf_prot\n</code></pre> </li> <li> <p>To visualize results, you can open the <code>WAVE*.nc</code> file, for example, in Octave or Matlab.</p> Matlab<pre><code>pkg load netcdf\nnetcdf_open('WAVE1978090606.nc')\nncdisp('WAVE1978090606.nc')\nhmax = ncread(\"WAVE1978090606.nc\",'hmax_st')\n%plot field at timestep 12\npcolor(hmax(:,:,12))\n\nhs_swell = ncread(\"WAVE1978090606.nc\",'hs_swell')\n%plot timeseries at position 20 20\nplot(hs_swell(20,20,:))\n</code></pre> </li> </ol>"},{"location":"software/wam/#long-parallel-jobs","title":"Long &amp; parallel jobs","text":"<p>Longer running and parallel jobs are better submitted as batch jobs using an sbatch script wam.slurm:</p> Bash<pre><code>#!/bin/bash\n#SBATCH --job-name=WAM-testrun\n#SBATCH --mem-per-cpu=1GB\n#SBATCH --nodes=1\n#SBATCH --ntasks=2\n#SBATCH --cpus-per-task=1\n#SBATCH -t 0-01:0:00\n#SBATCH --partition=green-ib\n#SBATCH --no-requeue\n\nmodule load green\nmodule load WAM\n\nexport WORK=$HOME/newwamtest\n\nmkdir --parent ${WORK}/tempsg\nmkdir --parent ${WORK}/work\n\ncd ${WORK}/tempsg\ncp ${WAMDIR}/const/TOPOCAT.DAT .\ncp ${WAMDIR}/const/Coarse_Grid/ARD/Preproc_User .\npreproc\n\ncp ${WAMDIR}/const/Coarse_Grid/ARD/WAM_User .\ncp ${WAMDIR}/const/WIND_INPUT.DAT .\n\nmpirun wam\n\ncp ${WAMDIR}/const/Coarse_Grid/ARD/Grid_User .\npgrid\n\ncp ${WAMDIR}/const/Coarse_Grid/ARD/Time_User .\nptime\n\ncp ${WAMDIR}/const/Coarse_Grid/ARD/Spectra_User .\npspec\n\ncp ${WAMDIR}/const/Coarse_Grid/ARD/Time_User_S .\nptime_S\n\ncp ${WAMDIR}/const/Coarse_Grid/ARD/nlnetcdf NETCDF_User\nmpirun pnetcdf\n</code></pre>"},{"location":"software/wam/#wam-long-version","title":"WAM long version","text":""},{"location":"software/wam/#starting-calculations","title":"Starting calculations","text":"<p>If the job is small, it can be run as an interactive session:</p> Bash<pre><code>srun -t 2:0:0 --pty bash\n</code></pre> <p>If the calculation is long or needs several cores, it is better to gather all needed commands in one wam.slurm batch script and submit it with the command:</p> Bash<pre><code>sbatch wam.slurm\n</code></pre>"},{"location":"software/wam/#preparation","title":"Preparation","text":"<ol> <li> <p>Firstly, the user needs to load the proper environment with the commands:</p> Bash<pre><code>module load green\nmodule load WAM\n</code></pre> </li> <li> <p>After that, it is needed to determine the working directory and go into it:</p> Bash<pre><code>export WORK=$HOME/newwamtest\nmkdir --parent ${WORK}/tempsg\ncd ${WORK}/tempsg\n</code></pre> </li> <li> <p>Copy into the working directory all needed data, for example:</p> Bash<pre><code>cp ${WAMDIR}/const/TOPOCAT.DAT .\ncp ${WAMDIR}/const/Coarse_Grid/ARD/Preproc_User .\npreproc\n\ncp ${WAMDIR}/const/Coarse_Grid/ARD/WAM_User .\ncp ${WAMDIR}/const/WIND_INPUT.DAT .\n</code></pre> </li> </ol>"},{"location":"software/wam/#running-wam","title":"Running WAM","text":"<p>WAM calculations can be started with the command <code>WAM</code>.</p> Bash<pre><code>mpirun wam\n</code></pre> <p>To run calculations normally, such parameters as grid (<code>pgrid</code>), time (<code>ptime</code>), spectra (<code>pspec</code>), time step (<code>ptime_S</code>), and (<code>pnetcdf</code>) should be defined or copied from the example:</p> Bash<pre><code>cp ${WAMDIR}/const/Coarse_Grid/ARD/Grid_User .\npgrid\n\ncp ${WAMDIR}/const/Coarse_Grid/ARD/Time_User .\nptime\n\ncp ${WAMDIR}/const/Coarse_Grid/ARD/Spectra_User .\npspec\n\ncp ${WAMDIR}/const/Coarse_Grid/ARD/Time_User_S .\nptime_S\n\ncp ${WAMDIR}/const/Coarse_Grid/ARD/nlnetcdf NETCDF_User\nmpirun pnetcdf\n</code></pre> <p>If calculations are going normally, you should have the following files in your <code>$WORK</code> directory:</p> Text Only<pre><code>BLS19780907060000  Grid_Prot              OUT19780907060000  Time_Prot_S\nBLS19780908060000  Grid_User              OUT19780908060000  Time_User\nC0119780906060000  Grid_info_COARSE_GRID  Preproc_Prot       Time_User_S\nC0119780907060000  MAP19780906060000      Preproc_User       WAM_Prot\nC0119780908060000  MAP19780907060000      Spectra_Prot       WAM_User\nC0219780906060000  MAP19780908060000      Spectra_User       WAVE1978090606.nc\nC0219780907060000  NETCDF_User            TOPOCAT.DAT        WIND_INPUT.DAT\nC0219780908060000  OUT19780906060000      Time_Prot          pnetcdf_prot\n</code></pre>"},{"location":"software/wam/#visualization","title":"Visualization","text":"<p>To visualize results, you can open the <code>WAVE*.nc</code> file, for example, in Octave or Matlab:</p> Matlab<pre><code>pkg load netcdf\nnetcdf_open('WAVE1978090606.nc')\nncdisp('WAVE1978090606.nc')\nhmax = ncread(\"WAVE1978090606.nc\",'hmax_st')\n%plot field at timestep 12\npcolor(hmax(:,:,12))\n\nhs_swell = ncread(\"WAVE1978090606.nc\",'hs_swell')\n%plot timeseries at position 20 20\nplot(hs_swell(20,20,:))\n</code></pre>"},{"location":"software/xtb/","title":"xTB","text":""},{"location":"software/xtb/#xtb-short-introduction","title":"xTB Short Introduction","text":"<ol> <li> <p>Create the xtb.slurm batch script for parallel calculations:</p> Bash<pre><code>#!/bin/bash\n#SBATCH --job-name=xTB-test\n#SBATCH --mem=2GB\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=24\n#SBATCH -t 1-00:00:00\n#SBATCH --partition=common\n\nmodule load rocky8/all\nmodule load xtb-crest/6.7.0-crest3.0\n\n# Run calculations \nxtb struc.xyz --opt tight --cycles 50 --charge -0 --alpb toluene --gfn 2 -P 4 &gt; final.out\n</code></pre> </li> <li> <p>Copy the job input file struc.xyz.</p> </li> <li> <p>Submit the job on base:</p> Bash<pre><code>sbatch xtb.slurm\n</code></pre> </li> </ol>"},{"location":"software/xtb/#xtb-long-version","title":"xTB Long Version","text":"<p>Extended Tight Binding (xTB) is a program developed by Grimme's group for solving common chemical problems. The workhorses of xTB are the GFN methods, both semi-empirical and force-field. The program contains several implicit solvent models: GBSA, ALPB. xTB functionality covers single-point energy calculations, geometry optimization, frequency calculations, and reaction path methods. It also allows performing molecular dynamics, meta-dynamics, and ONIOM calculations. More about xTB on HPC can be found here.</p>"},{"location":"software/xtb/#environment","title":"Environment","text":"<p>The environment is set up with the commands:</p> Bash<pre><code>module load rocky8/all\nmodule load xtb-crest/6.7.0-crest3.0\n</code></pre>"},{"location":"software/xtb/#running-xtb-jobs","title":"Running xTB Jobs","text":"<p>The input file should be in <code>.xyz</code> format and is executed by the command <code>xtb</code>. This command is usually placed in a <code>slurm</code> script.</p> Bash<pre><code>xtb struc.xyz --opt tight --cycles 50 --charge -0 --alpb toluene --gfn 2 -P 4 &gt; final.out\n</code></pre> <p>In xTB, calculation options are specified as command line arguments. <code>-P</code> is the number of processors used, <code>--gfn 2</code> is the calculation method (here GFN2-xTB), <code>--alpb toluene</code> is the ALPB implicit solvation model for toluene. More about command line arguments.</p>"},{"location":"software/xtb/#time-memory","title":"Time &amp; Memory","text":"<p>Calculation time depends on the size of the molecule and the methods used, and can only be determined empirically.</p> <p>1 GB per 2 or even more cores should be sufficient. For more detailed information, look at the <code>slurm-XXX-.stat</code> file after a test run. The lines \"CPU Efficiency:\" and \"Memory Efficiency:\" will give an idea of how efficiently the resources were used.</p>"},{"location":"software/xtb/#how-to-cite","title":"How to Cite","text":"<p>The main publication for the xTB program - DOI: 10.1002/wcms.1493.</p>"},{"location":"visualization/VirtualGL/","title":"Remote visualization using VirtualGL","text":"<p>Warning</p> <p>This page has not been completely updated for Rocky 8 yet!</p> <p>The client (your desktop) computer needs X11 and the VirtualGL software:</p> <p>Linux:</p> <ul> <li>X11 is default; if you have a graphical desktop, you have X11.</li> <li>Download your VirtualGL package from VirtualGL.</li> </ul> <p>Windows:</p> <ul> <li>Cygwin</li> <li>Use the Cygwin installer to install Cygwin/X and VirtualGL.</li> </ul> <p>Mac:</p> <ul> <li>XQuartz needs to be installed.</li> <li>Download your VirtualGL package from VirtualGL.</li> </ul> <p>Any recent VGL client version should work (vis-node has 2.5.2). If there is no native package for your Linux distribution, you can download the .deb and unpack it using <code>dpkg -x virtualgl...deb vgl</code>. The programs you need are in <code>vgl/opt/VirtualGL/bin/</code>.</p> <p>Connect to the visualization node using <code>vglconnect</code> from the VirtualGL package:</p> Bash<pre><code>vglconnect -s uni-ID@viz.hpc.taltech.ee\n</code></pre> <p>Run the visualization application you want to use, e.g., ParaView or VisIt:</p> Bash<pre><code>vglrun paraview\n</code></pre>"},{"location":"visualization/visualization-chemistry/","title":"Visualization software for computational chemistry, biology and physics","text":"<p>Warning</p> <p>This page has not been completely updated for Rocky 8 yet!</p> <p>Visualization should mostly be done on viz (Molden, Avogadro, JMol, or VMD), but short-time geometry checks are also possible on base (Molden, Avogadro, or JMol).</p>"},{"location":"visualization/visualization-chemistry/#short-introduction","title":"Short introduction","text":"<ol> <li> <p>Access viz by remote access programs (more preferable) or by SSH protocol (less preferable):</p> Bash<pre><code>ssh -X -Y -J UNI-ID@base.hpc.taltech.ee UNI-ID@viz\n</code></pre> </li> <li> <p>Load environment (gray or gray/spack/):</p> Bash<pre><code>module load viz-spack\nmodule load jmol\n</code></pre> </li> <li> <p>Run visualization program (<code>jmol</code>, <code>molden</code>, <code>avogadro</code>, <code>vmd</code>, or <code>rasmol</code>):</p> Bash<pre><code>jmol job.xyz\n</code></pre> </li> </ol>"},{"location":"visualization/visualization-chemistry/#visualization-long-version","title":"Visualization long version","text":""},{"location":"visualization/visualization-chemistry/#molden","title":"Molden","text":"<p>Molden is a nice program for visualizing the results of quantum chemical calculations.</p> <p>Molden is quite universal:</p> <ul> <li>reads various input and output formats,</li> <li>can be used as a Z-matrix editor,</li> <li>shows optimization paths,</li> <li>animates reaction paths and molecular vibrations,</li> <li>displays molecular orbitals and electron densities.</li> </ul>"},{"location":"visualization/visualization-chemistry/#environment","title":"Environment","text":"<p>On viz the environment is set up by the commands:</p> Bash<pre><code>module load viz-spack\nmodule load molden\n</code></pre> <p>On base the environment is set up by the commands:</p> Bash<pre><code>module load green/all\nmodule load molden\n</code></pre>"},{"location":"visualization/visualization-chemistry/#running-molden","title":"Running Molden","text":"<p>Molden is executed by the command <code>molden</code> and reads <code>.xyz</code>, Gaussian and ORCA outputs, etc.</p> Bash<pre><code>molden job.out\n</code></pre> <p>Some useful links:</p> <ul> <li>Molden manual</li> <li>Building molecule video</li> <li>Calculation results visualization video (includes showing optimization path and molecular vibrations)</li> </ul> <p></p>"},{"location":"visualization/visualization-chemistry/#how-to-cite","title":"How to cite","text":"<p>Molden 6.7:</p> <ul> <li>DOI:10.1007/s10822-017-0042-5</li> <li>DOI:10.1023/A:1008193805436</li> </ul>"},{"location":"visualization/visualization-chemistry/#avogadro","title":"Avogadro","text":"<p>Avogadro is an advanced molecule editor and visualizer designed for computational chemistry, molecular modeling, bioinformatics, materials science, and related areas.</p> <p>Avogadro has many useful options:</p> <ul> <li>construction of molecules from fragments,</li> <li>Z-matrix generation,</li> <li>geometry optimization,</li> <li>measurements,</li> <li>performing a conformational search,</li> <li>reading various input and output formats,</li> <li>showing molecular orbitals and electron density,</li> <li>animation of reaction paths and molecular vibrations,</li> <li>construction of IR spectra.</li> </ul>"},{"location":"visualization/visualization-chemistry/#environment_1","title":"Environment","text":"<p>On viz there is a native install of Avogadro (no modules needed).</p> <p>On base the environment is set up by the commands:</p> Bash<pre><code>module load green/all\nmodule load Avogadro\n</code></pre>"},{"location":"visualization/visualization-chemistry/#running-avogadro","title":"Running Avogadro","text":"<p>Avogadro is executed by the command <code>avogadro</code> and reads various input and output formats.</p> Bash<pre><code>avogadro job.log\n</code></pre> <p>Some useful links:</p> <ul> <li>Avogadro manual</li> <li>Building molecule video I, video II, video III, changing molecule (including optimization)</li> <li>Orbitals visualization video</li> <li>Molecular vibrations &amp; IR spectra video</li> </ul> <p></p>"},{"location":"visualization/visualization-chemistry/#how-to-cite_1","title":"How to cite","text":"<ul> <li>Avogadro 1.2.0 available at http://avogadro.cc/</li> <li>DOI: 10.1186/1758-2946-4-17</li> </ul>"},{"location":"visualization/visualization-chemistry/#jmol","title":"JMol","text":"<p>JMol is a free, open-source viewer of molecular structures that supports a wide range of chemical file formats.</p> <p>JMol has the following possibilities:</p> <ul> <li>visualization of animation,</li> <li>visualization of vibration,</li> <li>visualization of surfaces,</li> <li>visualization of orbitals,</li> <li>schematic shapes for secondary structures in biomolecules,</li> <li>measurements.</li> </ul>"},{"location":"visualization/visualization-chemistry/#environment_2","title":"Environment","text":"<p>On viz the environment is set up by the commands:</p> Bash<pre><code>module load green-spack\nmodule load jmol\n</code></pre> <p>On base by the commands:</p> Bash<pre><code>module load green-spack\nmodule load jmol\n</code></pre>"},{"location":"visualization/visualization-chemistry/#running-jmol","title":"Running JMol","text":"<p>JMol is executed by the command <code>jmol</code> and reads <code>.xyz</code>, <code>.pdb</code>, <code>.mol</code> formats, etc.</p> Bash<pre><code>jmol job.pdb\n</code></pre> <p></p>"},{"location":"visualization/visualization-chemistry/#how-to-cite_2","title":"How to cite","text":"<p>Jmol 14.31.0: an open-source Java viewer for chemical structures in 3D. Available at http://www.jmol.org/.</p>"},{"location":"visualization/visualization-chemistry/#vmd","title":"VMD","text":"<p>Visual Molecular Dynamics (VMD) is a molecular modeling and visualization program designed for biological systems. It supports over 60 file formats and has user-extensible graphical and text-based interfaces, as well as built-in standard Tcl/Tk and Python scripting languages. VMD provides a wide range of methods for visualizing and coloring molecules or atom subsets and an extensive selection syntax for subsets of atoms and has no limits on the number of molecules, atoms, residues, or trajectory frames.</p> <p>VMD has the following features:</p> <ul> <li>animation of MD trajectories,</li> <li>analysis of MD trajectories,</li> <li>analysis of sequences and structures of proteins and nucleic acids,</li> <li>ability to export graphics to files that can be processed by ray tracing and image rendering packages,</li> <li>ability to write molecular analysis programs in the Tcl language.</li> </ul> <p>Some useful links:</p> <ul> <li>VMD manual</li> <li>Many tutorials</li> <li>Short introduction video</li> <li>Long introduction video</li> </ul>"},{"location":"visualization/visualization-chemistry/#environment_3","title":"Environment","text":"<p>The first time use, the user has to read the license here. The software can only be used if the license is accepted! If you agree to the license, do:</p> Bash<pre><code>touch ~/.licenses/vmd-accepted\n</code></pre> <p>After that, it is needed to unload the module and load it again by commands:</p> Bash<pre><code>module unload VMD/1.9.3-text\nmodule load VMD\n</code></pre> <p>If this is the first time you accept a license agreement, the following commands should be given:</p> Bash<pre><code>mkdir .licenses\ntouch ~/.licenses/vmd-accepted\nmodule unload VMD/1.9.3-text\nmodule load VMD\n</code></pre> <p>On viz the environment is set up by the commands:</p> Bash<pre><code>module load viz\nmodule load VMD\n</code></pre> <p>On base the environment is set up by the commands:</p> Bash<pre><code>module load green\nmodule load VMD\n</code></pre> <p>The user also needs to agree with the licenses, as described above.</p>"},{"location":"visualization/visualization-chemistry/#running-vmd","title":"Running VMD","text":"<p>VMD is executed by the command <code>vmd</code> and reads various input and output formats.</p> Bash<pre><code>vmd job.mol\n</code></pre> <p></p>"},{"location":"visualization/visualization-chemistry/#how-to-cite_3","title":"How to cite","text":"<ul> <li>VMD 1.9.4 available at http://www.ks.uiuc.edu/Research/vmd/</li> <li>DOI:10.1016/0263-7855(96)00018-5</li> </ul> <p>In addition, the following articles should be cited depending on the functionalities used:</p> <ul> <li>Interactive Molecular Dynamics - DOI:10.1145/364338.364398</li> <li>Multiple Alignment Plugin - DOI:10.1093/bioinformatics/bti825</li> <li>Tachyon ray tracing library - John Stone \"An Efficient Library for Parallel Ray Tracing and Animation\", Computer Science Department, University of Missouri-Rolla, 1998</li> <li>STRIDE Secondary Structure Prediction - DOI:10.1002/prot.340230412</li> <li>SURF solvent accessible surface calculator - DOI:10.1109/38.310720</li> <li>MSMS solvent excluded surface calculator - DOI:10.1145/220279.220324</li> <li>Speech and gesture recognition - DOI:10.1109/38.824531</li> </ul>"},{"location":"visualization/visualization-chemistry/#rasmol","title":"RasMol","text":"<p>RasMol is a molecular graphics program for visualization of proteins, nucleic acids, and small molecules. RasMol provides a variety of color schemes and molecule representations. In RasMol, different parts of the molecule may be represented and colored independently of the rest of the molecule or displayed in several representations simultaneously, and atoms may also be labeled with arbitrary text strings. In addition, RasMol can read a prepared list of commands from a 'script' file. Supported input file formats are <code>.pdb</code>, <code>.mol2</code>, <code>.mdl</code>, <code>.msc</code>, <code>.xyz</code>, <code>.xmol</code>, CHARMm and CIF formats files. Finally, the rendered image may be written in a variety of formats such as GIF, PPM, BMP, PICT, Sun rasterfile or as a MolScript input script or Kinemage.</p> <p>RasMol is available on viz just by the common <code>rasmol</code>.</p> Bash<pre><code>rasmol job.mol\n</code></pre> <p>Some useful links:</p> <ul> <li>RasMol manual</li> <li>tutorial for beginners part 1, part 2</li> </ul>"},{"location":"visualization/visualization-chemistry/#how-to-cite_4","title":"How to cite","text":"<p>Herbert J. Bernstein, 2009. RasMol, available at: http://www.rasmol.org/.</p>"},{"location":"visualization/visualization/","title":"Visualization","text":"<p>Tip</p> <p>The recommended way of doing visualizations is now using the desktop session on ondemand.hpc.taltech.ee.</p>"},{"location":"visualization/visualization/#ondemand-desktop-on-any-node-software-rendering","title":"OnDemand Desktop on any node (software rendering)","text":"<p>The default desktop environment is XFCE, which is configurable, lightweight, and fast.</p> <p>The menu only contains a couple of programs from the operating system.</p> <p>To start software, open an XTerminal and use the module system as you would from the command line and start the program from there.</p> <p>NB: Do not use quality settings \"Compression 0\" and/or \"Image Quality 9\", this will cause a zlib error message. The message box can be removed by reloading the browser tab:</p> <p></p> <p></p>"},{"location":"visualization/visualization/#accessing-the-visualization-system","title":"Accessing the visualization system","text":"<p>The visualization computer <code>viz.hpc.taltech.ee</code> can be accessed by SSH from within the university network, from FortiVPN, or using a jump host:</p> Bash<pre><code>ssh viz.hpc.taltech.ee\nssh viz -J base.hpc.taltech.ee\n</code></pre> <p>Access to viz is limited to using SSH keys, no password login. Therefore, the SSH key must be copied to base. More can be found here.</p> <p>The base home directories are shared with viz.</p>"},{"location":"visualization/visualization/#using-gui-software-remotely","title":"Using GUI software remotely","text":"<p>The visualization software can be executed directly on the visualization node of the cluster, thus removing the need to copy the data for the analysis. There are several possibilities to use graphical programs remotely on a Linux/Unix server:</p> <ul> <li>Remote X, forwarding through SSH</li> <li>X2GO</li> <li>Xpra</li> <li>VNC</li> <li>RDP (currently not installed)</li> <li>VirtualGL</li> </ul> <p>At least one of these methods should work for any user, which one depends on the configuration of the client computer (your desktop/laptop).</p> <p>These methods also have different requirements for what client software needs to be installed on your computer:</p> <ul> <li>SSH, e.g., PuTTY on Windows (essential)</li> <li>X-server (essential for X-forwarding, Xpra, VirtualGL; not needed for X2GO, VNC) (installed by default on Linux; for Windows Xming or VcXsrv) for Mac (XQuartz)</li> <li>Xpra</li> <li>TightVNCViewer</li> <li>VirtualGL</li> </ul> <p>SSH is essential on all platforms, an X-server and VirtualGL are highly recommended, and Xpra and VNC are recommended to have on the client computer.</p>"},{"location":"visualization/visualization/#window-manager-or-desktop-environment-for-remote-use","title":"Window manager or Desktop environment for remote use","text":"<p>The default window manager for VNC and X2GO is FVWM2, a configurable, lightweight, and fast window manager.</p> <p>You can get a context menu by clicking on the background (left, middle, and right give different menus). By the way, a nice X11 feature is easy copy-and-paste, marking with the left mouse button automatically puts the selection into a copy buffer and clicking the middle mouse button inserts it at the current mouse cursor position. No annoying ctrl+c, ctrl+v necessary.</p> <p>Within VNC or X2GO, you are running a complete desktop session. Typical modern desktop environments require a lot of memory just for the desktop environment! For this reason, only resource-efficient window managers like <code>jwm</code>, <code>fvwm2</code>, <code>awesome</code>, <code>lwm</code>, <code>fluxbox</code>, <code>blackbox</code>, <code>xmonad</code>, and <code>tritium</code> are installed.</p> <p>Software to be automatically started can be configured in <code>.xsession</code> (or <code>.vnc/xstartup</code> or <code>.xsessionrc-x2go</code>).</p>"},{"location":"visualization/visualization/#available-visualization-software-on-compute-nodes","title":"Available Visualization software on compute nodes","text":"<ul> <li>ParaView</li> <li>VisIt</li> </ul> <ul> <li>Py-MayaVi</li> </ul> <ul> <li>RasMol</li> <li>VESTA</li> </ul> <ul> <li>VMD</li> <li>Ovito</li> <li>Ospray (raytracer)</li> <li>PoVray (raytracer)</li> </ul>"},{"location":"visualization/visualization/#ondemand-desktop-on-gpu-nodes-hardware-rendering","title":"OnDemand Desktop on GPU nodes (hardware rendering)","text":"<p>Requires, of course, to be submitted to a GPU node and a GPU to be reserved. The nodes are configured in a way that requires EGL rendering and therefore may require other modules to be loaded (e.g., ParaView).</p> <p>Otherwise, the Desktop works as the regular (software rendering) one, see above.</p> <p>Please note that for most applications, software rendering is fast enough. Only heavy visualization, like volume visualization in ParaView, COVISE, VisIt, VMD, Star-CCM+, and Ansys, may require GPU rendering.</p>"},{"location":"visualization/visualization/#in-situ-visualization-in-preparation","title":"In-situ visualization (in preparation)","text":"<p>In-situ visualization creates the visualization during the simulation instead of during the post-processing phase. The simulation code needs to be connected to in-situ visualization libraries, e.g., Catalyst (ParaView), LibSim (VisIt), and Ascent.</p> <p>The following are installed on our cluster:</p> <ul> <li>Catalyst</li> <li>Ascent</li> <li>LibSim</li> <li>SENSEI</li> </ul> <p>Ascent on all nodes:</p> Bash<pre><code>module load rocky8-spack\nmodule load ascent\n</code></pre> <p>Catalyst on all nodes:</p> Bash<pre><code>module load rocky8-spack\nmodule load libcatalyst/2.0.0-gcc-10.3.0-openblas-bp26\n</code></pre> <p>Catalyst can be used within OpenFOAM and NEK5000 simulations.</p>"},{"location":"visualization/vnc/","title":"Remote visualization using VNC","text":"<p>Warning</p> <p>This page has not been completely updated for Rocky 8 yet!</p>"},{"location":"visualization/vnc/#short-guide","title":"Short guide","text":"<ol> <li> <p>Connect to viz:</p> Bash<pre><code>ssh uni-ID@viz.hpc.taltech.ee\n</code></pre> </li> <li> <p>Start VNC on viz with the command:</p> Bash<pre><code>vncserver -geometry 1265x980\n</code></pre> </li> <li> <p>Open a second connection to viz:</p> Bash<pre><code>ssh -L 59XX:localhost:50XX uni-ID@viz.hpc.taltech.ee\n</code></pre> <p>where <code>XX</code> is the display number that appears after giving the <code>vncserver</code> command. NB! <code>XX</code> is always a two-digit number (e.g., <code>01</code> for <code>:1</code>)</p> </li> <li> <p>On your desktop, start a vncviewer:</p> Bash<pre><code>vncviewer :XX\n</code></pre> <p>where <code>XX</code> is the display number.</p> </li> <li> <p>Stop the vncserver!!! on viz with the command:</p> Bash<pre><code>vncserver -kill :XX\n</code></pre> </li> </ol>"},{"location":"visualization/vnc/#get-started","title":"Get started","text":""},{"location":"visualization/vnc/#software-recommended-to-use","title":"Software recommended to use","text":"<p>Virtual Network Computing (VNC) is a graphical desktop-sharing system to remotely control another computer.</p> <p>The client (your desktop) computer needs a vncviewer:</p> <ul> <li>Linux: xtigervncviewer</li> <li>Windows: TigerVNCviewer: vncviewer64-1.12.0.exe</li> <li>Mac: VNC Viewer</li> </ul>"},{"location":"visualization/vnc/#first-time-use","title":"First time use","text":"<p>On the first start, VNC asks you to specify a password to connect to the server. Choose a secure one that does not match your HPC/UniID password because VNC connections are not encrypted!</p> <p></p>"},{"location":"visualization/vnc/#vnc-long-version","title":"VNC Long version","text":"<p>VNC should be run first on the viz node of HPC and then on the user's computer.</p> <ol> <li> <p>Connect to viz with the command:</p> Bash<pre><code>ssh uni-ID@viz.hpc.taltech.ee\n</code></pre> <p>If this command does not work, try to connect through the jump host:</p> Bash<pre><code>ssh -J uni-ID@base.hpc.taltech.ee uni-ID@viz \n</code></pre> <p>NB! Connection to viz can be done only with SSH keys. The SSH key generation guide is here.</p> <p>NB! To use viz, the SSH key must be added to the base node.</p> <p>On Mac and Linux, this can be done with the command:</p> Bash<pre><code>ssh-copy-id Uni-ID@base.hpc.taltech.ee\n</code></pre> <p>After about an hour, when the automatic script has synced the files, you can use viz.</p> </li> <li> <p>On viz, start the VNC server. Depending on which VNC client the user has, one of these commands should be given:</p> Bash<pre><code>vncserver -geometry 1265x980\n</code></pre> <p>and for Tiger VNC:</p> Bash<pre><code>tigervncserver -geometry 1280x1024\n</code></pre> <p>It is recommended to specify the window size as well with the <code>-geometry</code> flag, since changing the resolution of the remote desktop (= window size) at runtime can have undesired effects.</p> </li> <li> <p>The output in the terminal will show on which display VNC is running.</p> <p></p> <p>See the second line <code>desktop at :8</code>, where <code>:8</code> is the display number -- further <code>XX</code>.</p> </li> <li> <p>Open a second connection to viz (in a new terminal) and give the command:</p> Bash<pre><code>ssh -L 59XX:localhost:50XX uni-ID@viz.hpc.taltech.ee\n</code></pre> <p>where <code>XX</code> is the display number as two digits (e.g., <code>01</code> for <code>:1</code>).</p> <p>NB! If you were connected through the jump host, this command should be given:</p> Bash<pre><code>ssh -J Uni-ID@base.hpc.taltech.ee -L 59XX:127.0.0.1:59XX Uni-ID@viz\n</code></pre> </li> <li> <p>On your desktop, start a VNC viewer. If you do it from the terminal, give one of these commands depending on which VNC viewer you have:</p> Bash<pre><code>vncviewer :XX\n</code></pre> <p>or</p> Bash<pre><code>xtigervncviewer localhost:XX\n</code></pre> <p>where <code>XX</code> is the number from above. On Windows (depending on the VNC client), the address to connect to could be <code>localhost::50XX</code> (again, the <code>XX</code> stands for the display/port as specified before).</p> <p>If you use a graphical interface, specify localhost in the corresponding field (line at the top) and click the \"Continue\" button.</p> <p></p> <p>Type the password.</p> <p></p> <p>If you see a monochromatic field and cannot start a session, it means that you need to set up your VNC session: Setting up VNC config.</p> <p>If you see a terminal, then everything is done correctly and you can start working. Within the session window, you can start any program from the terminal or using the menus of the window manager.</p> <p>Viz has a module system. Most of the modules need to be loaded unless the manual says they are native.</p> <p>Before loading modules, the source must be specified:</p> Bash<pre><code>source /usr/share/lmod/6.6/init/bash\nmodule use /gpfs/mariana/modules/system\n</code></pre> <p>followed by two commands to load the modules. The first one loads viz-spack or viz module, depending on the program installation type, and the second command loads the program itself. For example:</p> Bash<pre><code>module load viz-spack\nmodule load jmol\n</code></pre> <p></p> <p>In the case of a native program, only the command that calls this program is needed.</p> Bash<pre><code>rasmol\n</code></pre> <p>or</p> Bash<pre><code>paraview\n</code></pre> </li> </ol>"},{"location":"visualization/vnc/#correct-termination","title":"Correct termination","text":"<p>It is very important to finish the session correctly! If you do not do it, the session continues to run even if you close the session on your computer.</p> <p>To stop the VNC session, give on viz one of these commands:</p> Bash<pre><code>vncserver -kill :XX\n</code></pre> <p>or</p> Bash<pre><code>tigervncserver -kill :XX\n</code></pre> <p>where <code>XX</code> is the display number.</p> <p>Running sessions can be checked with the command:</p> Bash<pre><code>vncserver -list\n</code></pre> <p></p>"},{"location":"visualization/vnc/#setting-up-vnc-session","title":"Setting up VNC session","text":"<p>It is impossible to work with VNC without setting it up. To do this, give the following commands from the home directory on base or viz:</p> Bash<pre><code>cat &lt;&lt;EOT &gt; .xsession\nxterm &amp;\nfvwm2\nEOT\n</code></pre> <p>This will configure the automatic startup of <code>xterm</code> and the <code>fvwm2</code> window manager. Alternatively, the user can use other window managers: more desktop-like -- <code>fluxbox</code>, <code>awesome</code>, or <code>jwm</code> or tiling -- <code>i3</code>, <code>xmonad</code>, or <code>tritium</code>. To do this, the corresponding line must be added to the <code>.xsession</code> file with the command:</p> Bash<pre><code>echo \"fluxbox\" &gt;&gt; .xsession\n</code></pre> <p>The same way <code>.vnc/xstartup</code> can be configured in case the user wants to apply special settings exactly to VNC visualization.</p>"},{"location":"visualization/x2go/","title":"Remote visualization using X2GO (easiest to get working)","text":"<p>Warning</p> <p>This page has not been completely updated for Rocky 8 yet!</p> <p>The client (your desktop) computer needs the X2GO-client.</p> <p>NB! X2GO uses SSH-key. The SSH-key generation guide is here.</p> <p>NB! To use viz, the SSH-key must be added to the base node.</p> <p>On Mac and Linux, this can be done with the command:</p> Bash<pre><code>ssh-copy-id Uni-ID@base.hpc.taltech.ee\n</code></pre> <p>After about an hour, when the automatic script has synced the files, you can use viz.</p>"},{"location":"visualization/x2go/#configuring-the-client","title":"Configuring the client","text":"<p>During the first use, X2GO-client needs to be configured as shown in the picture. To configure, select the \"Session\" tab in the upper left corner. Setting the SSH-key is only necessary if you use a non-standard name or not the default key.</p> <p></p> <p>Large memory-consuming desktop environments like MATE, KDE, and GNOME are not available. Use window managers like <code>blackbox</code>, <code>fluxbox</code>, <code>jwm</code>, <code>fvwm2</code>, <code>awesome</code>, <code>lwm</code>, and <code>fvwm-crystal</code> (last setting on the screen).</p> <p>If you select <code>Terminal</code> as the \"Session type\" (use \"fvwm2\" as the Command), you will get a \"rootless\" xterm, and you can use that to start other software which will appear as individual windows on your regular desktop (not within a remote desktop window).</p> <p>It is also recommended to configure the display settings, for example, as done in the example below or in some other suitable way, since changing the resolution of the remote desktop (= window size) at runtime is not possible (resizing the window would be the equivalent of stretching your physical monitor) or can have other undesired effects.</p> <p></p>"},{"location":"visualization/x2go/#configuring-the-server-side","title":"Configuring the server-side","text":"<p>A couple of config files need to be present:</p> <ul> <li><code>$HOME/.fvwm/.fvwm2rc</code> .fvwm2rc</li> <li><code>$HOME/.xsessionrc-x2go</code> .xsessionrc-x2go (can be a link to .xsessionrc, .xsession, or .vnc/xstartup)</li> </ul> <p>If the files are not present, just copy them from <code>/etc/skel/</code> or put:</p> <ul> <li>.fvwm2rc to <code>$HOME/.fvwm/.fvwm2rc</code></li> <li>.xsessionrc-x2go to <code>$HOME/.xsessionrc-x2go</code></li> </ul> <p>to copy/save the example configs.</p>"},{"location":"visualization/x2go/#x2go-usage","title":"X2GO usage","text":"<p><code>$HOME</code> on base coincides with <code>$HOME</code> on viz.</p> <p>When the session is configured, press <code>enter</code> to run the session. Press the left bottom of the mouse to call the menu and choose XTerm.</p> <p></p> <p>A terminal will appear where the user can call the desired visualization program. We do not maintain the list of software in the menus of window managers or desktop environments, which means even with a graphical frontend, you still need to use the command line to start your programs! You can configure the menus yourself, e.g., in the <code>$HOME/.fvwm/.fvwm2rc</code> file for the fvwm window manager.</p> <p>Viz has a module system. Most of the modules need to be loaded unless the manual says they are native.</p> <p>Before loading modules, the source must be specified:</p> Bash<pre><code>source /usr/share/lmod/6.6/init/bash\nmodule use /gpfs/mariana/modules/system\n</code></pre> <p>followed by module load commands, for example:</p> Bash<pre><code>module load viz-spack\nmodule load jmol\n</code></pre> <p></p> <p>In the case of a native program, only the command that calls this program is needed.</p> Bash<pre><code>rasmol\n</code></pre> <p>or</p> Bash<pre><code>paraview\n</code></pre> <p>NB! ParaView and maybe other software using GLX need to be started using VirtualGL: <code>vglrun paraview</code></p> <p></p>"},{"location":"visualization/x2go/#terminating-x2go","title":"Terminating X2GO","text":"<p>It is extremely important to end the session properly! To do this:</p> <ol> <li>Type <code>exit</code> in your terminal.</li> <li>Click the left mouse button, call the menu, and choose <code>Exit fvwm</code>.</li> </ol> <p></p>"},{"location":"visualization/xpra/","title":"Remote visualization using Xpra","text":"<p>Warning</p> <p>This page has not been completely updated for Rocky 8 yet!</p> <p>The client (your desktop) computer needs X11 and Xpra:</p> <p>Linux:</p> <ul> <li>X11 is default; if you have a graphical desktop, you have X11.</li> <li>Xpra should be available in the package manager.</li> </ul> <p>Windows:</p> <ul> <li>VcXsrv, Xming, or Cygwin/X</li> <li>Xpra client</li> </ul> <p>Mac:</p> <ul> <li>XQuartz needs to be installed (xquartz.org)</li> <li>Xpra client</li> </ul> <p>Unlike VNC, these applications are \"rootless.\" They appear as individual windows inside your window manager rather than being contained within a single window.</p> Bash<pre><code>xpra start ssh://uni-ID@viz.hpc.taltech.ee/ --start-child=xterm\n</code></pre> <p>Specifying an SSH key to use and a jump host:</p> Bash<pre><code>xpra start ssh://viz/ --ssh=\"ssh -J base.hpc.taltech.ee\" --start-child=xterm\n</code></pre> <p>Re-attach from a different computer:</p> Bash<pre><code>xpra attach ssh:viz.hpc.taltech.ee\n</code></pre> <p>To stop Xpra, run on the server:</p> Bash<pre><code>xpra stop\n</code></pre>"}]}